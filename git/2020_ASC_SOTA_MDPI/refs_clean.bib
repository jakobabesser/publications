% Encoding: ISO-8859-1
@article{Abdoli:2009:1DConvEnvSounds:ARXIV,
abstract = {In this paper, we present an end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network (CNN) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal's fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the UrbanSound8k dataset and the experimental results have shown that it achieves 89{\%} of mean accuracy. Therefore, the propose approach outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input. Furthermore, the proposed approach has a small number of parameters compared to other architectures found in the literature, which reduces the amount of data required for training.},
archivePrefix = {arXiv},
arxivId = {1904.08990},
author = {Abdoli, Sajjad and Cardinal, Patrick and Koerich, Alessandro Lameiras},
eprint = {1904.08990},
file = {:Users/jakobabeer/Dropbox/{\_}LESEN/1904.08990v1.pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {convolutional neural network,deep,environmental sound classification,gammatone filterbank,learning,machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {apr},
pages = {1--24},
title = {{End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network}},
url = {http://arxiv.org/abs/1904.08990},
year = {2019}
}
@inproceedings{Abesser:2017:BassTranscription:AES,
address = {Erlangen, Germany},
author = {Abe{\ss}er, Jakob and Balke, Stefan and Frieler, Klaus and Pfleiderer, Martin and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the AES Conference on Semantic Audio},
doi = {10.1016/S0022-328X(00)00444-7},
file = {:Users/jakobabeer/Desktop/journal refs/Abesser{\_}2017{\_}AES.pdf:pdf},
title = {{Deep Learning for Jazz Walking Bass Transcription}},
year = {2017}
}
@inproceedings{Abesser:2018:BassSaliency:ISMIR,
address = {Paris, France},
author = {Abe{\ss}er, Jakob and Balke, Stefan and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Desktop/journal refs/Abesser{\_}2018{\_}ISMIR.pdf:pdf},
pages = {306--312},
title = {{Improving Bass Saliency Estimation Using Label}},
url = {http://ismir2018.ircam.fr/doc/pdfs/143{\_}Paper.pdf},
year = {2018}
}
@inproceedings{Abesser:2015:IntonationModulation:ISMIR,
address = {M{\'{a}}laga, Spain},
author = {Abe{\ss}er, Jakob and Cano, Estefan{\'{i}}a and Frieler, Klaus and Pfleiderer, Martin and Zaddach, Wolf-Georg},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Abe{\ss}er et al. - Unknown - SCORE-INFORMED ANALYSIS OF INTONATION AND PITCH MODULATION IN JAZZ SOLOS.pdf:pdf},
keywords = {melody{\_}contour{\_}analysis},
mendeley-tags = {melody{\_}contour{\_}analysis},
pages = {823--829},
title = {{Score-informed analysis of intonation and pitch modulation in jazz solos}},
year = {2015}
}
@article{Abesser:2017:ScoreInformedJazz:IEEE_TASLP,
author = {Abe{\ss}er, Jakob and Frieler, Klaus and Pfleiderer, Martin and Zaddach, Wolf-georg},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Abe{\ss}er et al. - 2012 - Score-Informed Analysis of Tuning , Intonation , Pitch Modulation , and Dynamics in Jazz Solos.pdf:pdf},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {168--177},
title = {{Score-Informed Analysis of Tuning , Intonation , Pitch Modulation , and Dynamics in Jazz Solos}},
volume = {25},
year = {2017}
}
@inproceedings{Abesser:2019:Stadtlaerm:DCASE,
address = {New York, NY, USA},
author = {Abe{\ss}er, Jakob and G{\"{o}}tze, Marco and Clau{\ss}, Tobias and Zapf, Dominik and K{\"{u}}hn, Christian and Lukashevich, Hanna and K{\"{u}}hnlenz, Stephanie and Mimilakis, Stylianos},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}DCASE.pdf:pdf},
keywords = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
title = {{Urban Noise Monitoring in the Stadtl{\"{a}}rm Project - A Field Report}},
year = {2019}
}
@inproceedings{Abesser:2018:Stadtlaerm:FICLOUD,
abstract = {Smart city applications for acoustic monitoring become essential to cope with the overall increasing noise pollution in urban environments. This paper gives an overview over a distributed sensor network for noise monitoring in the German city of Jena. Several acoustic sensor units allow for classifying among various acoustic scenes and events using Convolutional Neural Networks (CNN) and measuring different noise level parameters. Connected by a communication system based on MQTT (Message Queue Telemetry Transport), these sensors communicate measurement data to a central server for data postprocessing and storage. Finally, a web-based application allows for various real-time visualizations of noise exposure distributed over the city.},
address = {Barcelona, Spain},
author = {Abe{\ss}er, Jakob and Gr{\"{a}}fe, Robert and K{\"{u}}hn, Christian and Clau{\ss}, Tobias and Lukashevich, Hanna and G{\"{o}}tze, Marco and K{\"{u}}hnlenz, Stephanie},
booktitle = {Proceedings of the 6th IEEE International Conference on Future Internet of Things and Cloud (FiCLOUD)},
doi = {10.1109/FiCloud.2018.00053},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2018{\_}FICLOUD.pdf:pdf},
isbn = {9781538675038},
keywords = {TA L{\"{a}}rm,acoustic scene classification,event detection,machine{\_}learning,noise level measurement,sensor network,smart city},
mendeley-tags = {machine{\_}learning},
pages = {318--324},
title = {{A Distributed Sensor Network for Monitoring Noise Level and Noise Sources in Urban Environments}},
year = {2018}
}
@article{Abesser:2019:UrbanNoise:ERCIM,
author = {Abe{\ss}er, Jakob and Kepplinger, Sara},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}ERCIM.pdf:pdf},
journal = {ERCIM News 119},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Smart Solutions to Cope with Urban Noise Pollution}}
}
@inproceedings{Abesser:2010:BassPlayingStyles:ICASSP,
address = {Dallas, TX, USA},
author = {Abe{\ss}er, Jakob and Lukashevich, Hanna and Schuller, Gerald},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2010.5495945},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2010{\_}ICASSP.pdf:pdf},
keywords = {bass{\_}transcription},
mendeley-tags = {bass{\_}transcription},
pages = {2290--2293},
title = {{Feature-Based Extraction of Plucking and Expression Styles of the Electric Bass Guitar}},
year = {2010}
}
@inproceedings{Abesser:2019:F0Contours:ICASSP,
address = {Brighton, UK},
author = {Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}ICASSP.pdf:pdf},
keywords = {idmt,melody{\_}contour{\_}analysis},
mendeley-tags = {idmt,melody{\_}contour{\_}analysis},
title = {{Fundamental Frequency Contour Classification: A Comparison Between Hand-Crafted and CNN-Based Features}},
year = {2019}
}
@inproceedings{Abesser:2019:ICASSP,
author = {Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}ICASSP.pdf:pdf},
title = {{Fundamental Frequency Contour Classification: A Comparison between Hand-Crafted and CNN-Based Features}},
year = {2019}
}

@InProceedings{Abesser:2017:ASC:DCASE,
  author =    {Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Gr{\"{a}}fe, Robert and Lukashevich, Hanna},
  title =     {{Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =      {2017},
  address =   {Munich, Germany},
  month =     {16 - 17 November},
  abstract =  {Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban en-vironments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-the-art CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configura-tion leads to a classification accuracy of 75{\%} for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.},
  file =      {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Abesser{\_}165.pdf:pdf}
}
@article{Abesser:2017:BassGuitar:IEEE_TASLP,
author = {Abe{\ss}er, Jakob and Schuller, Gerald},
doi = {10.1109/TASLP.2017.2702384},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2017{\_}IEEE{\_}Bass{\_}Guitar.pdf:pdf},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {acoustic signal processing,audio signal processing},
number = {9},
pages = {1741--1750},
title = {{Instrument-Centered Music Transcription of Solo Bass Guitar Recordings}},
volume = {25},
year = {2017}
}
@inproceedings{Abidin:2017:LBP:ICASSP,
address = {New Orleans, LA, USA},
author = {Abidin, Shamsiah and Togneri, Roberto and Sohel, Ferdous},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Downloads/07952231.pdf:pdf},
isbn = {9781509041176},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {626--630},
title = {{Enhanced LBP Texture Features from Time Frequency Representations for Acoustic Scene Classification}},
year = {2017}
}

@InProceedings{Adavanne:2017:AEDWeaklyLabels:DCASE,
  author =        {Adavanne, Sharath and Virtanen, Tuomas},
  title =         {{Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  abstract =      {This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log mel-band energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3{\%} for weak labels on the unseen test split.},
  archiveprefix = {arXiv},
  arxivid =       {1710.02998},
  eprint =        {1710.02998},
  file =          {:Users/jakobabeer/Desktop/Adavanne{\_}DCASE2017{\_}weaklabels.pdf:pdf},
  url =           {http://arxiv.org/abs/1710.02998}
}
@article{Ahn2013,
author = {von Ahn, Luis},
doi = {10.1098/rsta.2012.0383},
file = {::},
issn = {1364-503X, 1471-2962},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {artificial intelligence},
number = {1987},
title = {{Augmented intelligence: the Web and human intelligence}},
url = {http://rsta.royalsocietypublishing.org.libproxy1.nus.edu.sg/content/371/1987/20120383{\%}5Cnhttp://rsta.royalsocietypublishing.org.libproxy1.nus.edu.sg/content/371/1987/20120383.full.pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23419852},
volume = {371},
year = {2013}
}
@inproceedings{Almeida:1984:VarFreqSynHarmonicCoding:ICASSP,
abstract = {The Harmonic Coding concept has already shown its potential for efficiently coding speech. Previous implementations have usec a frame rate of one every 16 ms. This was mainly due to the fact that, with longer frames, even a nonstationary spectral model (of low order) cannot reproduce the zones of fast-varying pitch with the desirable quality. However, the high framing rate is a limitation, since it implies that fewer bits will be available for encoding each frame. A solution for this problem has been devised: the signal is synthesized in the time domain, as a superimposition of "harmonics" whose instantaneous frequency varies continuously along an interpolation curve, within each frame. In this way, fast pitch variations can be tracked with no difficulty. Experimental results are presented, confirming these facts. The integration of this synthesis scheme in a speech coder is discussed.},
author = {Almeida, L. and Silva, F.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/icassp.1984.1172489},
file = {::},
month = {mar},
pages = {437--440},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Variable-frequency synthesis: An improved harmonic coding scheme}},
year = {1984}
}
@inproceedings{Amiriparian:2018:GANASC:EUSIPCO,
abstract = {Unsupervised representation learning shows high promise for generating robust features for acoustic scene analysis. In this regard, we propose and investigate a novel combination of features learnt using both a deep convolutional generative adversarial network (DCGAN) and a recurrent sequence to sequence autoencoder (S2SAE). Each of the representation learning algorithms is trained individually on spectral features extracted from audio instances. The learnt representations are: (i) the activations of the discriminator in case of the DCGAN, and (ii) the activations of a fully connected layer between the decoder and encoder units in case of the S2SAE. We then train two multilayer perceptron neural networks on the DCGAN and S2SAE feature vectors to predict the class labels. The individual predicted labels are combined in a weighted decision-level fusion to achieve the final prediction. The system is evaluated on the development partition of the acoustic scene classification data set of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). In comparison to the baseline, the accuracy is increased from 74.8 {\%} to 86.4 {\%} using only the DCGAN, to 88.5 {\%} on the development set using only the S2SAE, and to 91.1 {\%} after fusion of the individual predictions.},
address = {Rome, Italy},
author = {Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Gerczuk, Maurice and Pugachevskiy, Sergey and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
doi = {10.23919/EUSIPCO.2018.8553225},
file = {:Users/jakobabeer/Downloads/08553225.pdf:pdf},
isbn = {9789082797015},
issn = {22195491},
keywords = {Acoustic scene classification,Generative adversarial networks,Sequence to sequence autoencoders,Unsupervised feature learning},
pages = {977--981},
title = {{A Fusion of Deep Convolutional Generative Adversarial Networks and Sequence to Sequence Autoencoders for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Arandjelovic2017,
abstract = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
author = {Arandjelovic, Relja and Zisserman, Andrew},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.73},
file = {::},
isbn = {978-1-5386-1032-9},
issn = {15505499},
month = {oct},
pages = {609--617},
publisher = {IEEE},
title = {{Look, Listen and Learn}},
url = {http://ieeexplore.ieee.org/document/8237335/},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{Arora:2017:TransferLearning:IEEE_WMSP,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
address = {Luton, UK},
annote = {abr: test},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {Proceedings of the IEEE International Workshop on Multimedia Signal Processing (MMSP)},
doi = {10.1109/MMSP.2017.8122258},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario.pdf:pdf},
isbn = {978-1-5090-3649-3},
keywords = {machine{\_}listening,transfer{\_}learning},
mendeley-tags = {machine{\_}listening,transfer{\_}learning},
pages = {1--6},
title = {{A Study on Transfer Learning for Acoustic Event Detection in a Real Life Scenario}},
url = {http://ieeexplore.ieee.org/document/8122258/},
year = {2017}
}
@inproceedings{Arora2017,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)},
doi = {10.1109/MMSP.2017.8122258},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario.pdf:pdf},
isbn = {978-1-5090-3649-3},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{A study on transfer learning for acoustic event detection in a real life scenario}},
url = {http://ieeexplore.ieee.org/document/8122258/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Atal:1982:LPCspeech:ICASSP,
abstract = {The excitation for LPC speech synthesis usually consists of two separate signals - a delta-function pulse once every pitch penod for voiced speech and white noise for unvoiced speech. This manner of representing excitation requires that speech segments be classified accurately into voiced and unvoiced categories and the pitch period of voiced segments be known. It is now well recognised that such a rigid idealization of the vocal excitation is often responsible for the unnatural qualizy a.ssock{\#}ed with synthesized speech. This paper describes a new approach to the excitation problem that does not require a priori knowledge of either the voiced-unvoiced decision or the pitch period. All classes of sounds are generated by exciting the LPC filter with a sequence of pulses; the amplitudes and locations of the pulses are determined using a non-iterative analysis-by-synthesis procedure. This procedure minimizes a perceptual-distance metric representing subjectively-important differences between the waveforms of the original and the synthetic speech signals. The disiance metric takes account of the finite-frequency resolution as well as the ditTerential sensitivity of the human ear to errors in the formant and inter-formant regions of the speech spectrum.},
author = {Atal, Bishnu S. and Remde, Joel R.},
booktitle = {International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/icassp.1982.1171649},
file = {::},
keywords = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
mendeley-tags = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
month = {mar},
pages = {614--617},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{A new model of LPC excitation for producing natural-sounding speech at low bit rates}},
year = {1982}
}
@article{Aytar:2016:SoundNet:NIPS,
abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
archivePrefix = {arXiv},
arxivId = {1610.09001},
author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
eprint = {1610.09001},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Aytar, Vondrick, Torralba - 2016 - SoundNet Learning Sound Representations from Unlabeled Video.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {892--900},
title = {{SoundNet: Learning Sound Representations from Unlabeled Video}},
year = {2016}
}
@techreport{Bock,
abstract = {We propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolu-tional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy.},
author = {B{\"{o}}ck, Sebastian and Davies, Matthew E P and Knees, Peter},
file = {::},
title = {{MULTI-TASK LEARNING OF TEMPO AND BEAT: LEARNING ONE TO IMPROVE THE OTHER}}
}
@inproceedings{Boeck:2014:BeatTracking:ISMIR,
abstract = {In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possi- ble beat positions. It chooses the model with the most ap- propriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27{\%} over existing state- of-the-art methods. Under certain conditions the system is able to match even human tapping performance.},
address = {Taipei, Taiwan},
author = {B{\"{o}}ck, S and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{o}}ck, Krebs, Widmer - 2014 - A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music Styles.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {603--608},
title = {{A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music Styles}},
url = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T108{\_}367{\_}Paper.pdf},
year = {2014}
}
@inproceedings{Boeck:2016:BeatTracking:ISMIR,
abstract = {In this paper we present a novel method for jointly extract-ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro-grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down-beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.},
address = {New York, NY, USA},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International So- ciety for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Downloads/Boeck{\_}etal{\_}ISMIR{\_}2016.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {255--261},
title = {{Joint Beat and Downbeat Tracking with Recurrent Neural Networks}},
year = {2016}
}
@inproceedings{Boeck:2016:JointBeatDownbeat:ISMIR,
abstract = {In this paper we present a novel method for jointly extract-ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro-grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down-beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.},
address = {New York, NY, USA},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/Joint Beat and downbeat tracking with RNN.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {255--261},
title = {{Joint Beat and Downbeat Tracking with Recurrent Neural Networks}},
year = {2016}
}
@article{Bach:2015:LRP:PLOS,
abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest.We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1371/journal.pone.0130140},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/73bbd4448083b01b5a9389b3c37f5425aac0.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--46},
pmid = {26161953},
title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
volume = {10},
year = {2015}
}

@InProceedings{Bae:2016:LSTMCNN:DCASE,
  author =    {Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo},
  title =     {{Acoustic Scene Classification using Parallel Combination of LSTM and CNN}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =      {2016},
  address =   {Budapest, Hungary},
  month =     {3 September},
  file =      {:Users/jakobabeer/Downloads/Bae-DCASE2016workshop.pdf:pdf}
}
@article{Bai:2018:ConvRecurrent:ARXIV,
abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
archivePrefix = {arXiv},
arxivId = {1803.01271},
author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
eprint = {1803.01271},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling(2).pdf:pdf},
month = {mar},
title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
url = {http://arxiv.org/abs/1803.01271},
year = {2018}
}
@inproceedings{Balke:2018:MelodyTranscription:ICASSP,
address = {New Orleans, USA},
author = {Balke, Stefan and Dittmar, Christian and Abe{\ss}er, Jakob and Meinard, M},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Downloads/2017{\_}BalkeDAM{\_}SoloVoiceEnhancement{\_}ICASSP.pdf:pdf},
pages = {196--200},
title = {{Data-Driven Solo Voice Enhancement for Jazz Music Retrieval}},
year = {2018}
}
@article{Balke:2019:SoftAttention:ISMIR,
abstract = {Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.},
archivePrefix = {arXiv},
arxivId = {1906.10996},
author = {Balke, Stefan and Dorfer, Matthias and Carvalho, Luis and Arzt, Andreas and Widmer, Gerhard},
eprint = {1906.10996},
file = {:Users/jakobabeer/Downloads/1906.10996.pdf:pdf},
journal = {arXiv},
title = {{Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music Retrieval}},
url = {http://arxiv.org/abs/1906.10996},
year = {2019}
}
@article{Barbancho:2012:GuitarTranscription:IEEE_TASLP,
abstract = {In this paper, a system for the extraction of the tablature of guitar musical pieces using only the audio waveform is presented. The analysis of the inharmonicity relations between the fundamentals and the partials of the notes played is the main process that allows to estimate both the notes played and the string/fret combination that was used to produce that sound. A procedure to analyze chords will also be described. This proce- dure will also make use of the inharmonicity analysis to find the simultaneous string/fret combinations used to play each chord. The proposed method is suitable for any guitar type: classical, acoustic and electric guitars. The system performance has been evaluated on a series of guitar samples from the RWC instruments database and our own recordings.},
author = {Barbancho, Isabel and Tard{\'{o}}n, Lorenzo J. and Sammartino, Simone and Barbancho, Ana M.},
doi = {10.1109/TASL.2012.2191281},
file = {:Users/jakobabeer/Downloads/Barbancho et al.{\_}2012{\_}IEEE Transactions on Audio, Speech, and Language Processing{\_}Inharmonicity-Based Method for the Automatic Generatio Kopie.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Fret,guitar,inharmonicity,music analysis,music{\_}transcription,pitch estimation,string,string{\_}estimation,tablature},
mendeley-tags = {music{\_}transcription,string{\_}estimation},
number = {6},
pages = {1857--1868},
title = {{Inharmonicity-based method for the automatic generation of guitar tablature}},
volume = {20},
year = {2012}
}
@article{Barbedo:2011:InstrumentRecognition:IEEE_TASLP,
author = {Barbedo, Arnal Garcia Jayme and Tzanetakis, George},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Garcia, Barbedo, Tzanetakis - 2011 - Individual Partials.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {instrument recognition,partial{\_}tracking},
mendeley-tags = {instrument recognition,partial{\_}tracking},
number = {1},
pages = {111--122},
title = {{Musical Instrument Classification Using Individual Partials}},
volume = {19},
year = {2011}
}
@inproceedings{Bartkowiak:2011:NonTimeProgressiveSM:AES,
abstract = {In this paper we propose a new sinusoidal model tracking algorithm that implements a non-time-progressive way of data processing. Sinusoidal partial parameters are estimated in the consecutive frames; however, the order of establishing individual connections between partials is determined by a greedy rule within the whole signal or within a specific time window. In this way, the strongest connections may be determined early, and subsequent predictions of each trajectory evolution are based on a more reliable partial evolution history, compared to a traditional progressive scheme. As a consequence, the proposed non-progressive tracking algorithm offers a statistically significant improvement of obtained trajectories in terms of better classic pattern recognition measures.},
address = {New York, NY, USA},
author = {Bartkowiak, Maciej and {\.{Z}}ernicki, Tomasz},
booktitle = {Proceedings of the 131st Audio Engineering Society Convention},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {1--8},
title = {{A Non-Time-Progressive Partial Tracking Algorithm for Sinusoidal Modeling}},
year = {2011}
}
@inproceedings{Basbug:2019:SpatialPyramidPoolingASC:ICSC,
address = {Newport, CA, USA, 30 January - 1 February},
author = {Basbug, Ahmet Melih and Sert, Mustafa},
booktitle = {Proceedings of the 13th IEEE International Conference on Semantic Computing (ICSC),},
doi = {10.1109/ICOSC.2019.8665547},
file = {:Users/jakobabeer/Downloads/08665547.pdf:pdf},
isbn = {9781538667835},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {128--131},
title = {{Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks}},
year = {2019}
}
@article{Bear:2019:JointASCAED:INTERSPEECH,
abstract = {Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two separate tasks in the field of computational sound scene analysis. In this work, we present a new dataset with both sound scene and sound event labels and use this to demonstrate a novel method for jointly classifying sound scenes and recognizing sound events. We show that by taking a joint approach, learning is more efficient and whilst improvements are still needed for sound event detection, SED results are robust in a dataset where the sample distribution is skewed towards sound scenes.},
archivePrefix = {arXiv},
arxivId = {1904.10408},
author = {Bear, Helen L. and Nolasco, In{\^{e}}s and Benetos, Emmanouil},
doi = {10.21437/Interspeech.2019-2169},
eprint = {1904.10408},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Bear, Nolasco, Benetos - 2019 - Towards joint sound scene and polyphonic sound event recognition.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Acoustic scene classification,CRNN,Computational sound scene analysis,Sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
pages = {4594--4598},
title = {{Towards joint sound scene and polyphonic sound event recognition}},
volume = {2019-Septe},
year = {2019}
}
@article{Bello:2018:SONYC:CACM,
abstract = {We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs. SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment. SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.},
archivePrefix = {arXiv},
arxivId = {1805.00889},
author = {Bello, Juan Pablo and Silva, Claudio and Nov, Oded and DuBois, R. Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish},
eprint = {1805.00889},
file = {:Users/jakobabeer/Downloads/bello{\_}sonyc{\_}cacm{\_}2018.pdf:pdf},
isbn = {1234567245},
journal = {Communications of the ACM (CACM)},
keywords = {acm reference format,citizen science,cyber-physical systems,machine listening,noise pollution,sensor networks,smart cities,visualization},
number = {2},
title = {{SONYC: A System for the Monitoring, Analysis and Mitigation of Urban Noise Pollution}},
url = {http://arxiv.org/abs/1805.00889},
volume = {62},
year = {2018}
}
@article{Benetos:2012:ASC:DAFX,
abstract = {In this paper, we propose a method for modeling and classifying acoustic scenes using temporally-constrained shift-invariant probabilistic latent component analysis (SIPLCA). SIPLCA can be used for extracting time-frequency patches from spectrograms in an unsupervised manner. Component-wise hidden Markov models are incorporated to the SIPLCA formulation for enforcing temporal constraints on the activation of each acoustic component. The time-frequency patches are converted to cepstral coefficients in order to provide a compact representation of acoustic events within a scene. Experiments are made using a corpus of train station recordings, classified into 6 scene classes. Results show that the proposed model is able to model salient events within a scene and outperforms the non-negative matrix factorization algorithm for the same task. In addition, it is demonstrated that the use of temporal constraints can lead to improved performance.},
author = {Benetos, Emmanouil and Lagrange, Mathieu and Dixon, Simon},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/dafx12{\_}submission{\_}30.pdf:pdf},
journal = {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {1--7},
title = {{Characterisation of Acoustic Scenes using a Temporally-Constrained Shift-Invariant Model}},
year = {2012}
}
@inproceedings{Bisot:2015:ASC:EUSIPCO,
abstract = {Acoustic scene classification is a difficult problem mostly due to the high density of events concurrently occurring in audio scenes. In order to capture the occurrences of these events we propose to use the Subband Power Distribution (SPD) as a feature. We extract it by computing the histogram of amplitude values in each frequency band of a spectrogram image. The SPD allows us to model the density of events in each frequency band. Our method is evaluated on a large acoustic scene dataset using support vector machines. We outperform the previous methods when using the SPD in conjunction with the histogram of gradients. To reach further improvement, we also consider the use of an approximation of the earth mover's distance kernel to compare histograms in a more suitable way. Using the so-called Sinkhorn kernel improves the results on most of the feature configurations. Best performances reach a 92.8{\%} F1 score.},
address = {Nice, France},
author = {Bisot, Victor and Essid, Slim and Richard, Gael},
booktitle = {Proceedings of the 23rd European Signal Processing Conference (EUSIPCO)},
doi = {10.1109/EUSIPCO.2015.7362477},
file = {:Users/jakobabeer/Downloads/07362477.pdf:pdf},
isbn = {9780992862633},
keywords = {Acoustic scene classification,Sinkhorn distance,acoustic{\_}scene{\_}classification,machine{\_}listening,subband power distribution image,support vector machine},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {719--723},
title = {{HOG and Subband Power Distribution Image Features for Acoustic Scene Classification}},
year = {2015}
}
@article{Bisot:2017:ASC:TASLP,
abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
author = {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Ga{\"{e}}l},
doi = {10.1109/TASLP.2017.2690570},
file = {:Users/jakobabeer/Downloads/main{\_}bisot2016.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Acoustic scene classification,feature learning,matrix factorization},
number = {6},
pages = {1216--1229},
title = {{Feature Learning with Matrix Factorization Applied to Acoustic Scene Classification}},
volume = {25},
year = {2017}
}

@InProceedings{Bisot:2017:NFASC:DCASE,
  author =        {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael},
  title =         {{Nonnegative Feature Learning Methods for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Bisot{\_}194.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@article{Bittner:2018:MultitaskLearning:ARXIV,
archivePrefix = {arXiv},
arxivId = {1809.00381},
author = {Bittner, Rachel M and McFee, Brian and Bello, Juan Pablo},
eprint = {1809.00381},
file = {:Users/jakobabeer/Downloads/1809.00381.pdf:pdf},
journal = {CoRR},
title = {{Multitask Learning for Fundamental Frequency Estimation in Music}},
volume = {abs/1809.0},
year = {2018}
}
@inproceedings{Bittner:2017:DeepSaliency:ISMIR,
address = {Suzhou, China},
author = {Bittner, Rachel M. and McFee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P.},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference ({\{}ISMIR{\}})},
file = {:Users/jakobabeer/Downloads/ismir2017{\_}salience.pdf:pdf},
pages = {63--70},
title = {{Deep Salience Representations for {\{}F0{\}} Estimation in Polyphonic Music}},
year = {2017}
}
@inproceedings{Bittner:2014:MedleyDB:ISMIR,
address = {Taipei, Taiwan},
author = {Bittner, Rachel M and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Desktop/journal refs/6.pdf:pdf},
pages = {155--160},
title = {{MedleyDB: A Multitrack Dataset for Annotation-Intensive {\{}MIR{\}} Research}},
year = {2014}
}
@article{Boddapati:2017:ASC:PCS,
abstract = {Automatic classification of environmental sounds, such as dog barking and glass breaking, is becoming increasingly interesting, especially for mobile devices. Most mobile devices contain both cameras and microphones, and companies that develop mobile devices would like to provide functionality for classifying both videos/images and sounds. In order to reduce the development costs one would like to use the same technology for both of these classification tasks. One way of achieving this is to represent environmental sounds as images, and use an image classification neural network when classifying images as well as sounds. In this paper we consider the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds. We evaluate the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition (AlexNet and GoogLeNet). Our experiments show that we obtain good classification accuracy for the three datasets.},
author = {Boddapati, Venkatesh and Petef, Andrej and Rasmusson, Jim and Lundberg, Lars},
doi = {10.1016/j.procs.2017.08.250},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Boddapati et al. - 2017 - Classifying environmental sounds using image recognition networks.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolutional Neural Networks,Deep Learning,Environmental Sound Classification,GPU Processing,Image Classification},
pages = {2048--2056},
publisher = {Elsevier B.V.},
title = {{Classifying environmental sounds using image recognition networks}},
url = {http://dx.doi.org/10.1016/j.procs.2017.08.250},
volume = {112},
year = {2017}
}
@inproceedings{Bonada:2008:WideBandModel:DAFX,
abstract = {In this paper we propose a method to estimate and transform harmonic components in wide-band conditions, out of a single period of the analyzed signal. This method allows estimating harmonic parameters with higher temporal resolution than typical Short Time Fourier Transform (STFT) based methods. We also discuss transformations and synthesis strategies in such context, focusing on the human voice.},
author = {Bonada, Jordi},
booktitle = {Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
title = {{Wide-Band Harmonic Sinusoidal Modelling}},
year = {2008}
}
@incollection{Bonada:2011:SpectralProcessing:DAFX_book,
author = {Bonada, Jordi and Serra, Xavier and Amatriain, Xavier and Loscos, Alex},
booktitle = {DAFX: Digital Audio Effects},
chapter = {Spectral P},
edition = {Second Edi},
editor = {{Udo Z{\"{o}}lzer}},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {394--445},
title = {{Spectral Processing}},
year = {2011}
}
@article{Cances:2019:PostProcessing:ARXIV,
abstract = {Sound event detection (SED) aims at identifying audio events (audio tagging task) in recordings and then locating them temporally (localization task). This last task ends with the segmentation of the frame-level class predictions, that determines the onsets and offsets of the audio events. Yet, this step is often overlooked in scientific publications. In this paper, we focus on the post-processing algorithms used to identify the audio event boundaries. Different post-processing steps are investigated, through smoothing, thresholding, and optimization. In particular, we evaluate different approaches for temporal segmentation, namely statistic-based and parametric methods. Experiments are carried out on the DCASE 2018 challenge task 4 data. We compared post-processing algorithms on the temporal prediction curves of two models: one based on the challenge's baseline and a Multiple Instance Learning (MIL) model. Results show the crucial impact of the post-processing methods on the final detection score. Statistic-based methods yield a 22.9{\%} event-based F-score on the evaluation set with our MIL model. Moreover, the best results were obtained using class-dependent parametric methods with 32.0{\%} F-score.},
archivePrefix = {arXiv},
arxivId = {1906.06909},
author = {Cances, Leo and Guyot, Patrice and Pellegrini, Thomas},
eprint = {1906.06909},
file = {:Users/jakobabeer/Downloads/1906.06909.pdf:pdf},
title = {{Evaluation of post-processing algorithms for polyphonic sound event detection}},
url = {http://arxiv.org/abs/1906.06909},
year = {2019}
}
@article{Cances:2019:SED:DCASE,
author = {Cances, L{\'{e}}o and Pellegrini, Thomas and Guyot, Patrice},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Cances{\_}69.pdf:pdf},
pages = {2--5},
title = {{Multi-Task Learning and Post Processing Optimization for Sound Event Detection}},
year = {2019}
}
@inproceedings{Cano:2009:MelodyLineSeparation:DAFX,
abstract = {We propose a system which separates saxophone melodies from composite recordings of saxophone, piano, and/or orchestra. The system is intended to produce an accompaniment sans saxophone suitable for rehearsal and practice purposes. A Melody Line Detection (MLD) algorithm is proposed as the starting point for a source separation implementation which incorporates known information about typical saxophone melody lines, acoustic characteristics and range of the saxophone in order to prevent and correct detection errors. By extracting reliable information about the soloist melody line, the system separates piano or orchestra accompaniments from the solo part. The system was tested with commercial recordings and a performance of 79.7{\%} of accurate detections was achieved. The accompaniment tracks obtained after source separation successfully remove most of the saxophone sound while preserving the original nature of the accompaniment track.},
author = {Cano, Estefan{\'{i}}a and Cheng, Corey},
file = {::},
title = {{Melody Line Detection and Source Separation in Classical Saxophone Recordings}}
}
@inproceedings{Cano:2019:LBD:ISMIR,
address = {Delft, The Netherlands},
author = {Cano, Estefania and Escamilla, Antonio and Grollmisch, Sascha and Kehling, Christian and Gil, Gustavo Adolfo L{\'{o}}pez and {\'{A}}ngel, Fernando Mora},
booktitle = {Late Breaking Demo at the International Society for Music Information Retrieval (ISMIR)},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{ACMus - Advancing Computational Musicology: Semi-Supervised and Unsupervised Segmentation and Annotation of Musical Collections}},
year = {2019}
}
@inproceedings{Cano:2019:SelectiveHearing:MMSP,
address = {Kuala Lumpur, Malaysia},
author = {Cano, Estefan{\'{i}}a and Lukashevich, Hanna},
booktitle = {Proceedings of the IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)},
file = {::},
isbn = {9781728118178},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Selective Hearing : A Machine Listening Perspective}},
year = {2019}
}
@techreport{Cao:2019:EventLocalizationDetection:DCASE,
author = {Cao, Yin and Iqbal, Turab and Kong, Qiuqiang and Galindo, Miguel B. and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Detection and Classification of Acoustic Scenes and Events 2019},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Cao{\_}74.pdf:pdf},
keywords = {event{\_}detection,event{\_}localization,machine{\_}listening},
mendeley-tags = {event{\_}detection,event{\_}localization,machine{\_}listening},
title = {{Two-Stage Sound Event Localization and Detection using Intensity Vector and Generalized Cross-Correlation}},
year = {2019}
}
@inproceedings{Cartwright:2019:Tricycle:WASPAA,
address = {New Paltz, NY, USA},
author = {Cartwright, Mark and Cramer, Jason and Salamon, Justin and Bello, Juan Pablo},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {::},
keywords = {self{\_}supervised{\_}learning},
mendeley-tags = {self{\_}supervised{\_}learning},
title = {{Tricycle: Audio Representation Learning from Sensor Network Data using Self-Supervision}},
year = {2019}
}
@phdthesis{Caruana:1997:MultitaskLearning:THESIS,
author = {Caruana, Rich},
doi = {10.1007/978-1-4899-7687-1_100322},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/CMU-CS-97-203.pdf:pdf},
school = {Carnegie Mellon University},
title = {{Multitask Learning}},
type = {PhD thesis},
year = {1997}
}
@inproceedings{Chen:2019:ASC:DCASE,
author = {Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Zhang{\_}34.pdf:pdf},
title = {{Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling}},
year = {2019}
}
@inproceedings{Chen:2018:Scalogram:INTERSPEECH,
abstract = {Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5{\%} in the evaluation step in DCASE2016.},
address = {Hyderabad, India},
author = {Chen, Hangting and Zhang, Pengyuan and Bai, Haichuan and Yuan, Qingsheng and Bao, Xiuguo and Yan, Yonghong},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
doi = {10.21437/Interspeech.2018-1524},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1524.pdf:pdf},
issn = {19909772},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {3304--3308},
title = {{Deep convolutional neural network with scalogram for audio scene modeling}},
year = {2018}
}
@inproceedings{Chen:2019:ASCFilters:ICASSP,
address = {Brighton, UK},
author = {Chen, Hangting and Zhang, Pengyuan and Yan, Yonghong},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0000835.pdf:pdf},
isbn = {9781538646588},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {835--839},
title = {{An Audio Scene Classification Framework with Embedded Filters and a DCT-Based Temporal Module}},
year = {2019}
}
@inproceedings{Cheng:2018:BeatTracking:EUSIPCO,
address = {Rome, Italy},
author = {Cheng, Tian and Fukayama, Satoru and Goto, Masataka},
booktitle = {Proceedings of the 28th European Signal Processing Conference (EUSIPCO)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/Convolving gaussian Kernals for RNN based beat tracking.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1905--1909},
title = {{Convolving Gaussian Kernels for RNN-Based Beat Tracking}},
year = {2018}
}
@article{Cheveigne:2002:YinPitchTracking:JASA,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
month = {apr},
number = {4},
pages = {1917--1930},
title = {{YIN, a fundamental frequency estimator for speech and music}},
url = {http://asa.scitation.org/doi/10.1121/1.1458024},
volume = {111},
year = {2002}
}
@inproceedings{Cho:DCASE:LargeMarginCNN:DCASE,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1910.06784},
author = {Cho, Janghoon and Yun, Sungrack and Park, Hyoungwoo and Eum, Jungyun and Hwang, Kyuwoong},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
doi = {10.33682/8xh4-jm46},
eprint = {1910.06784},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2019 - Acoustic Scene Classification Based on a Large-Margin Factorized CNN.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {45--49},
title = {{Acoustic Scene Classification Based on a Large-Margin Factorized CNN}},
year = {2019}
}
@article{Choi:2019:DrumTranscription:ISMIR,
archivePrefix = {arXiv},
arxivId = {1906.03697v1},
author = {Choi, Keunwoo and Cho, Kyunghyun},
eprint = {1906.03697v1},
file = {:Users/jakobabeer/Downloads/1906.03697.pdf:pdf},
journal = {arXiv},
keywords = {drum{\_}transcription,unsupervised{\_}learning},
mendeley-tags = {drum{\_}transcription,unsupervised{\_}learning},
title = {{Deep unsupervised drum transcription}},
year = {2019}
}
@inproceedings{Chou:2019:Attention:ICASSP,
address = {Brighton, UK},
author = {Chou, Szu-Yu and Cheng, Kai-Hsiang and Jang, Jyh-Shing Roger and Yang, Yi-Hsuan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682558},
file = {:Users/jakobabeer/Downloads/08682558.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {26--30},
title = {{Learning to Match Transient Sound Events Using Attentional Similarity for Few-shot Sound Recognition}},
url = {https://ieeexplore.ieee.org/document/8682558/},
year = {2019}
}
@article{Cong2018,
abstract = {In this paper, a supervised approach based on Convolutional Neural Networks (CNN) for polyphonic piano transcription is presented. The system consists of pitch detection model, onset/offset detection model, and note search model. The pitch detection model is a single-channel CNN predicting the probabilities of pitches contained in one frame of the audio. The onset/offset model based on dual-channel CNN is used for estimating the probabilities of each pitch's onset or offset in a frame. The note search model is rule-based; it integrates the outputs of the pitch model and onset/offset model to determine the final onset, offset and pitch of notes in audio. Two experiments with different dataset conditions are accomplished to compare with state-of-the-art approaches on the same datasets. Experimental results reveal that the proposed approach preforms better in both frame-and note-based metrics.},
author = {Cong, Fu'Ze and Liu, Shuchang and Guo, Li and Wiggins, Geraint A.},
doi = {10.1109/ICASSP.2018.8461794},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
title = {{A Parallel Fusion Approach to Piano Music Transcription Based on Convolutional Neural Network}},
year = {2018}
}
@inproceedings{Cramer2019,
address = {Brighton, United Kingdom},
author = {Cramer, Jason and Wu, Ho-hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682475},
file = {::},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {3852--3856},
publisher = {IEEE},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}},
url = {https://ieeexplore.ieee.org/document/8682475/},
year = {2019}
}
@inproceedings{Cuccovillo:2019:AESForensics,
author = {Cuccovillo, Luca and Aichroth, Patrick},
booktitle = {AES International Conference on Audio Forensics},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Inverse Decoding of PCM {\{}{\$}A{\$}-law{\}} and {\{}{\$}\mu{\$}-law{\}}}},
year = {2019}
}
@inproceedings{Dang:2018:ASCMulti:ICCE,
address = {Hue City, Vietnam},
author = {Dang, An and Vu, Toan H. and Wang, Jia-Ching},
booktitle = {Proceedings of the IEEE International Conference on Consumer Electronics (ICCE)},
doi = {10.1109/ICCE.2018.8326315},
file = {:Users/jakobabeer/Downloads/08326315.pdf:pdf},
isbn = {9781538630259},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
publisher = {IEEE},
title = {{Acoustic Scene Classification using Convolutional Neural Networks and Multi-Scale Multi-Feature Extraction}},
year = {2018}
}
@inproceedings{Davies:2019:BeatTracking:EUSIPCO,
address = {A Coru{\~{n}}a, Spain},
author = {Davies, Matthew E. P. and B{\"{o}}ck, Sebastia},
booktitle = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/tcn{\_}beat{\_}tracking.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
title = {{Temporal convolutional networks for musical audio beat tracking}},
year = {2019}
}
@article{Davies2009,
abstract = {A fundamental research topic in music information retrieval is the automatic extraction of beat locations frommusic signals. In this paper we address the under-explored topic of beat tracking evaluation. We present a review of existing evaluation models and, given their strengths and weaknesses, we propose a new method based on a novel visualisation for beat tracking performance, the beat error histogram. To investigate the properties of evaluation methods we undertake a large scale beat tracking experiment. We conduct experiments using a new annotated test database which we make available to the research community. We demonstrate that the choice of evaluation method can have a significant impact on the relative performance of different beat tracking algorithms. On this basis we make a set of recommendations for comparative beat tracking experiments.},
author = {Davies, Matthew E. P. and Degara, Norberto and Plumbley, Mark D},
file = {::},
journal = {Audio},
number = {October},
pages = {17},
title = {{Evaluation Methods for Musical Audio Beat Tracking Algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.6936{\&}rep=rep1{\&}type=pdf},
year = {2009}
}
@techreport{Davies2009a,
abstract = {A fundamental research topic in music information retrieval is the automatic extraction of beat locations from music signals. In this paper we address the under-explored topic of beat tracking evaluation. We present a review of existing evaluation models and, given their strengths and weaknesses, we propose a new method based on a novel visualisation for beat tracking performance, the beat error histogram. To investigate the properties of evaluation methods we undertake a large scale beat tracking experiment. We conduct experiments using a new annotated test database which we make available to the research community. We demonstrate that the choice of evaluation method can have a significant impact on the relative performance of different beat tracking algorithms. On this basis we make a set of recommendations for comparative beat tracking experiments.},
author = {Davies, Matthew E P and Degara, Norberto and Plumbley, Mark D},
file = {::},
title = {{Evaluation Methods for Musical Audio Beat Tracking Algorithms}},
year = {2009}
}
@article{Defferrard:2016:FMA:ARXIV,
abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma},
archivePrefix = {arXiv},
arxivId = {1612.01840},
author = {Defferrard, Micha{\"{e}}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
eprint = {1612.01840},
file = {:Users/jakobabeer/Downloads/1612.01840.pdf:pdf},
journal = {arXiv},
title = {{FMA: A Dataset For Music Analysis}},
url = {http://arxiv.org/abs/1612.01840},
year = {2016}
}
@article{Delphin-Poulat:2019:SED:DCASE,
author = {Delphin-Poulat, Lionel and Plapous, Cyril},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Delphin{\_}15.pdf:pdf},
pages = {2018--2020},
title = {{Mean Teacher with Data Augmentation for DCASE 2019 Task 4}},
year = {2019}
}
@inproceedings{Depalle:1993:PartialTrackingHMM:ICASSP,
address = {Minneapolis, MN},
author = {Depalle, Philippe and Garcia, Guillermo and Rodet, Xavier},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {225--228 vol.1},
title = {{Tracking of partials for additive sound synthesis using hidden Markov models}},
year = {1993}
}
@article{DiGiorgi2013,
abstract = {Chords and keys are among the most exhaustive descriptors of songs. In this study we focus on chord and key sequence recognition from an audio signal, in the context of pop and rock music. The system exploits a set of novel probabilistic models that describe the relationship between different aspects of music and their temporal evolution. These models are based on a set of parameters with a musical meaning. The models include two diatonic key modes, Dorian and Mixolydian, besides major and minor modes previously considered in the literature. These four key modes are the most used in western pop and rock music. In order to provide a compact representation of the chord and key sequences, three novel time-varying harmony- based features are here introduced. Given the importance of emotion characterization in music, the three features are here related to the mood perceived in songs. The method outperforms the state-of-the-art in both chord and key recognition tasks. In order to better train our parameters, we create annotations of chords and keys for a new dataset of 62 songs from the first five Robbie Williams' albums.},
author = {{Di Giorgi}, Bruno and Zanoni, Massimiliano and Sarti, Augusto and Tubaro, Stefano},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Di Giorgi et al. - 2013 - Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony(2).pdf:pdf},
isbn = {9783800735433},
journal = {Proceedings of the 8th International Workshop on Multidimensional Systems},
pages = {145--150},
title = {{Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony}},
url = {http://ieeexplore.ieee.org/document/6623838/},
year = {2013}
}
@inproceedings{Diment2013,
abstract = {instrument recognition. The conventional supervised approaches normally rely on annotated data to train the classifier. This implies performing costly manual annotations of the training data. The SSL methods enable utilising the additional unannotated data, which is significantly easier to obtain, allowing the overall development cost maintained at the same level while notably improving the performance. The implemented classifier incorporates the Gaussian mixture model-based SSL scheme utilising the iterative EM-based algorithm, as well as the extensions facilitating a simpler convergence criteria. The evaluation is performed on a set of nine instruments while training on a dataset, in which the relative size of the labelled data is as little as 15{\%}. It yields a noteworthy absolute performance gain of 16{\%} compared to the performance of the initial supervised models.},
address = {Marrakech, Morocco},
annote = {* GMM
* fully supervised for showing upper limit
* first estimate model parameters with labeled data
* then apply to unlabeled with uncertainty weighting scheme
* performance gain with unlabeled data},
author = {Diment, Aleksandr and Heittola, Toni and Virtanen, Tuomas},
booktitle = {Proceedings of the 21st European Signal Processing Conference (EUSIPCO 2013)},
file = {::},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
publisher = {IEEE},
title = {{Semi-supervised learning for musical instrument recognition}},
year = {2013}
}
@article{Dinkel2019,
abstract = {Depression detection research has increased over the last few decades as this disease is becoming a socially-centered problem. One major bottleneck for developing automatic depression detection methods lies on the limited data availability. Recently, pretrained text-embeddings have seen success in sparse data scenarios, while pretrained audio embeddings are rarely investigated. This paper proposes DEPA, a self-supervised, Word2Vec like pretrained depression audio embedding method for depression detection. An encoder-decoder network is used to extract DEPA on sparse-data in-domain (DAIC) and large-data out-domain (switchboard, Alzheimer's) datasets. With DEPA as the audio embedding, performance significantly outperforms traditional audio features regarding both classification and regression metrics. Moreover, we show that large-data out-domain pretraining is beneficial to depression detection performance.},
archivePrefix = {arXiv},
arxivId = {1910.13028},
author = {Dinkel, Heinrich and Zhang, Pingyue and Wu, Mengyue and Yu, Kai},
eprint = {1910.13028},
file = {::},
title = {{Depa: Self-supervised audio embedding for depression detection}},
url = {http://arxiv.org/abs/1910.13028},
year = {2019}
}
@inproceedings{Doras:2019:UNet:MMRP,
address = {Milano, Italy},
author = {Doras, Guillaume and Esling, Philippe and Peeters, Geoffroy},
booktitle = {Proceedings of the International Workshop on Multilayer Music Representation and Processing (MMRP)},
doi = {10.1109/MMRP.2019.00020},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Doras, Esling, Peeters - 2019 - On the use of U-Net for dominant melody estimation in polyphonic music.pdf:pdf},
isbn = {9781728116495},
pages = {66--70},
publisher = {IEEE},
title = {{On the use of U-Net for dominant melody estimation in polyphonic music}},
year = {2019}
}
@inproceedings{Dorfer:2018:NoisyLabelsSelfVerification:DCASE,
address = {Surrey, UK},
author = {Dorfer, Matthias and Widmer, Gerhard},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Dorfer, Widmer - 2018 - Training General-Purpose Audio Tagging Networks with Noisy Labels and Iterative Self-Verification.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
title = {{Training General-Purpose Audio Tagging Networks with Noisy Labels and Iterative Self-Verification}},
year = {2018}
}
@inproceedings{Dressler:2011:PitchEstimation:AES,
author = {Dressler, Karin},
booktitle = {Information Retrieval},
file = {::},
keywords = {pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
pages = {1--10},
title = {{Pitch Estimation by the Pair-Wise Evaluation of Spectral Peaks}},
year = {2011}
}
@inproceedings{Driedger:2019:BeatTapping:ISMIR,
address = {Delft, The Netherlands},
author = {Driedger, Jonathan and Schreiber, Hendrik and de Has, W. Bas and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
title = {{Towards Automatically Correcting Tapped Beat Annotations for Music Recordings}},
year = {2019}
}
@inproceedings{Drossos:2019:DomainAdaptation:WASPAA,
abstract = {A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of $\Delta$-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\%} to 45{\%}, using the TUT Acoustic Scenes dataset.},
address = {New Paltz, NY, USA},
author = {Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {::},
keywords = {Acoustic scene classification,Wasserstein distance,acoustic{\_}scene{\_}classificaiton,adversarial training,domain{\_}adaptation,machine{\_}listening,unsupervised domain adaptation},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,domain{\_}adaptation,machine{\_}listening},
pages = {259--263},
publisher = {IEEE},
title = {{Unsupervised Adversarial Domain Adaptation based on the Wasserstein Distance for Acoustic Scene Classification}},
year = {2019}
}
@article{Drossos:2020:SED:ARXIV,
abstract = {State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85{\%} and average training time per epoch by 78{\%}, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6{\%} and 3.8{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {2002.00476},
author = {Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas},
eprint = {2002.00476},
file = {::},
title = {{Sound Event Detection with Depthwise Separable and Dilated Convolutions}},
url = {http://arxiv.org/abs/2002.00476},
year = {2020}
}
@inproceedings{Durand:2016:DownbeatTracking:ICASSP,
abstract = {We define a novel system for the automatic estimation of downbeat positions from audio music signals. New rhythm and melodic fea- tures are introduced and feature adapted convolutional neural net- works are used to take advantage of their specificity. Indeed, invari- ance to melody transposition, chroma data augmentation and length- specific rhythmic patterns prove to be useful to learn downbeat like- lihood. After the data is segmented in tatums, complementary fea- tures related to melody, rhythm and harmony are extracted and the likelihood of a tatum being at a downbeat position is computed with the aforementioned neural networks. The downbeat sequence is then extracted with a flexible temporal hidden Markov model. We then show the efficiency and robustness of our approach with a compara- tive evaluation conducted on 9 datasets.},
address = {Shanghai, China},
author = {Durand, Simon and Bello, Juan P. and David, Bertrand and Richard, Gael},
booktitle = {Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2016.7471684},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Durand et al. - 2016 - Feature adapted convolutional neural networks for downbeat tracking.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
keywords = {Convolutional Neural Networks,Downbeat Tracking,Music Information Retrieval,Music Signal Processing,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {296--300},
title = {{Feature adapted convolutional neural networks for downbeat tracking}},
year = {2016}
}
@article{Durand:2017:DownbeatTracking:IEEE_TASLP,
abstract = {In this paper, we present a novel state of the art system for automatic downbeat tracking from music signals. The audio signal is first segmented in frames which are synchronized at the tatum level of the music. We then extract different kind of features based on harmony, melody, rhythm and bass content to feed convolutional neural networks that are adapted to take advantage of each feature characteristics. This ensemble of neural networks is combined to obtain one downbeat likelihood per tatum. The downbeat sequence is finally decoded with a flexible and efficient temporal model which takes advantage of the metrical continuity of a song. We then perform an evaluation of our system on a large base of 9 datasets, compare its performance to 4 other published algorithms and obtain a significant increase of 16.8 percent points compared to the second best system, for altogether a moderate cost in test and training. The influence of each step of the method is studied to show its strengths and shortcomings.},
author = {Durand, Simon and Bello, Juan Pablo and David, Bertrand and Richard, Gael},
doi = {10.1109/TASLP.2016.2623565},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Durand et al. - 2017 - Robust downbeat tracking using an ensemble of convolutional networks.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Convolutional neural networks,beat{\_}tracking,downbeat tracking,music information retrieval,music signal processing},
mendeley-tags = {beat{\_}tracking},
number = {1},
pages = {72--85},
title = {{Robust Downbeat Tracking Using an Ensemble of Convolutional Networks}},
volume = {25},
year = {2017}
}
@inproceedings{Eck:2007:BeatTracking:ICASSP,
abstract = {We introduce a novel method for estimating beat from digital audio. We compute autocorrelation such that the distribution of energy in phase space is preserved in a so-called autocorrelation phase matrix (APM). We estimate beats by computing individual APMs over short overlapping segments of an onset trace derived from the audio. Then an adaptation of Viterbi decoding is used to search the APMs for metrical combinations of points that change smoothly over time in the lag/phase plane. Because small temporal perturbations are seen as local movements on the APM, the Viterbi search can be bounded using a small 2D Gaussian window. The resulting algorithm jointly estimates tempo, meter and beat. As is always the case with Viterbi decoding, an online version is possible although best performance is achieved offline. We report results on an annotated dataset of 60-second musical segments},
address = {Honolulu, Hawaii, USA},
author = {Eck, Douglas},
booktitle = {Proceedings of the 32nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2007.367319},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/Beat Tracking using an Autocorrelation Phase Matrix.pdf:pdf},
isbn = {1424407281},
issn = {15206149},
keywords = {Autocorrelation phase matrix,Beat induction,Correlation,Viterbi decoding,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1313--1316},
title = {{Beat tracking using an autocorrelation phase matrix}},
volume = {4},
year = {2007}
}
@inproceedings{Lin:2019:SingingVoiceSeparation:ICASSP,
address = {Brighton, UK},
author = {{Edward Lin}, Kin Wah and Goto, Masataka},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682958},
file = {:Users/jakobabeer/Downloads/08682958.pdf:pdf},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {251--255},
title = {{Zero-mean Convolutional Network with Data Augmentation for Sound Level Invariant Singing Voice Separation}},
url = {https://ieeexplore.ieee.org/document/8682958/},
year = {2019}
}

@InProceedings{Elizalde:2016:ASC:DCASE,
  author =        {Elizalde, Benjamin and Kumar, Anurag and Shah, Ankit and Badlani, Rohan and Vincent, Emmanuel and Raj, Bhiksha and Lane, Ian},
  title =         {{Experiments on the DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  abstract =      {In this paper we present our work on Task 1 Acoustic Scene Classi- fication and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9{\%} compared to the baseline of 72.6{\%} and for Task 3 we achieved a Segment-Based Error Rate of 0.76 compared to the baseline of 0.91.},
  archiveprefix = {arXiv},
  arxivid =       {1607.06706},
  eprint =        {1607.06706},
  file =          {:Users/jakobabeer/Downloads/Elizalde-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
  url =           {http://arxiv.org/abs/1607.06706}
}
@inproceedings{Elowsson:2019:PitchChromaCNN:ISMIR,
author = {Elowsson, Anders and Friberg, Anders},
booktitle = {ISMIR 2019},
file = {:Users/jakobabeer/Downloads/1906.07145.pdf:pdf},
title = {{Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN}},
year = {2019}
}
@article{Essid:2006:InstrumentRecognition:IEEE_TASLP,
abstract = {Journal Helia started its "mission" nearly 36 years ago, very unpreten- tiously, as an information bulletin FAO Research Network on Sunflower, with the founders' intention to allow a quick and easy exchange of knowledge gained from experimental field trials of improving and breeding important oilseed crop species, such as sunflower. With the time passing, the number of scien- tists gathered around the project development and promotion of sunflower growing under the FAO Research Network on Sunflower has risen slowly but steadily, while the projects have become more extensive, complex and serious. The abundance of scientific research results, realized in the framework of a research network, determined the direction of the evolution journal Helia in scientific journal with internationally recognized quality, peer-reviewed papers and its relatively high ranking in the scientific society. Since the beginning of publishing in Serbia (1990th), the Journal pub- lished by the Institute of Field and Vegetable Crops, as the publisher and edito- rial office headquarters, was under the auspices of F.A.O. and ISA until the 2006th when the editorial office was transferred to the Serbian Academy of Sci- ences and Arts; Branch in Novi Sad, which also assumed the role of the main publisher, while the Institute remained co-publisher.Since 1990 a total of 24 volumes with 47 regular and two extraordinary numbers have been published in the scientific journal HELIA. That is a pretty impressive library with 6900 pages of printed material in 746 scientific papers in English. So far 2125 authors and co-authors of scientific papers from 43 countries from all continents have participated in publishing scientific papers in the journal All submitted manuscripts are subjected to anonymous international review (so-called "single-blind peer review", where the authors of the papers do not know who the reviewers are, but the reviewers know who the authors of the papers are) and published in the journal only after receiving a positive review by two independent reviewers and the final opinion of the editor. Regarding the impact factor of the ISJ Helia in the last 10 years, accord- ing to citation indicators of some papers published in the journal, it can be concluded that it has had relatively high levels over the past 10 years, with a trend of significant increase in 2011 and 2012. Focusing on that parameter, and relatively high two-and five-year impact factors in 2011 and 2012, we can be very satisfied about these trends, which have led to our journal being ranked near relatively influential journals on the global level.},
author = {Essid, Slim and Richard, Ga{\"{e}}l and David, Bertrand},
file = {::},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {1401--1412},
title = {{Pairwise Classification Strategies}},
volume = {14},
year = {2006}
}
@article{Esterer:2019:LinearProgrammingParTrack:ARXIV,
abstract = {A new approach to the tracking of sinusoidal chirps using linear programming is proposed. It is demonstrated that the classical algorithm of McAulay and Quatieri is greedy and exhibits exponential complexity for long searches, while approaches based on the Viterbi algorithm exhibit factorial complexity. A linear programming (LP) formulation to find the best {\$}L{\$} paths in a lattice is described and its complexity is shown to be less than previous approaches. Finally it is demonstrated that the new LP formulation outperforms the classical algorithm in the tracking of sinusoidal chirps in high levels of noise.},
archivePrefix = {arXiv},
arxivId = {1901.05044},
author = {Esterer, Nicholas and Depalle, Philippe},
eprint = {1901.05044},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Esterer, Depalle - 2019 - A linear programming approach to the tracking of partials(3).pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {jan},
title = {{A Linear Programming Approach to the Tracking of Partials}},
url = {http://arxiv.org/abs/1901.05044},
year = {2019}
}
@phdthesis{Fayek:2019:ProgressiveLearning:PHD,
author = {Fayek, Haytham M.},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Fayek - 2019 - via Progressive Learning(2).pdf:pdf},
keywords = {progressive{\_}learning},
mendeley-tags = {progressive{\_}learning},
school = {Petronas University of Technology},
title = {{Continual Deep Learning via Progressive Learning}},
type = {PhD Thesis},
year = {2019}
}
@inproceedings{Fiocchi:2018:BeatTracking:EUSIPCO,
address = {Rome, Italy},
author = {Fiocchi, Davide and Buccoli, Michele and Zanoni, Massimiliano and Antonacci, Fabio and Sarti, Augusto},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Fiocchi et al. - 2018 - Beat Tracking using Recurrent Neural Network a Transfer Learning Approach.pdf:pdf},
isbn = {9789082797015},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1915--1919},
publisher = {EURASIP},
title = {{Beat Tracking using Recurrent Neural Network: a Transfer Learning Approach}},
url = {https://www.politesi.polimi.it/bitstream/10589/139073/1/tesi.pdf},
year = {2018}
}
@inproceedings{Fitz:1995:BandwidthEnhancedSM:ICMC,
abstract = {We present a system for sound modeling and synthesis that preserves the elegance and malleability of a sinusoidal model, while accommodating sounds with noisy (non-sinusoidal) components. We use an enhanced McAulay-Quatieri (MQ) style analysis that extracts bandwidth information in addition to the sinusoidal parameters for each partial.},
address = {Banff, Canada},
author = {Fitz, Kelly and Haken, Lippold},
booktitle = {Proceedings of the International Computer Music Conference},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {154--157},
title = {{Bandwidth Enhanced Sinusoidal Modeling in Lemur}},
year = {1995}
}
@inproceedings{Fitz:1992:MQLimitedOscillators:ICMC,
abstract = {The McAulay-Quatieri (MQ) analysis is a robust, general sinusoidal analysis technique. Unlike many other analysis techniques, it can be used to analyze sounds without a stable harmonic structure (i. e., polyphonic or non-harmonic sounds and instrument tones with extreme vibrato). The MQ technique can provide the time-varying spectral information needed to control a real-time additive synthesis engine. Unfortunately, the original MQ technique generates an arbitrary number of sinusoidal tracks, while a real-time system has a limited number of oscillators. This paper presents some improvements to the basic MQ analysis technique which, in addition to improving the quality of the ensuing syntheses, and making the sinusoidal model more robust, make the MQ analysis data more suitable for controlling a real-time sinusoidal synthesis engine with a fixed number of oscillators. 1. The Choice of the McAulay-Quatieri Model In our research, we have sought a model for sound that would accommodate synthesis with time scale modifications. The goal has been to find a model that would produce time-scaled syntheses of sampled audio signals that are otherwise perceptually equivalent to the original signals. We chose a sinusoidal model advanced by McAulay and Quatieri (McAulay and Quatieri 1985) as the basis for our research. The MQ sinusoidal model allows independent time-and frequency-scale modification. Many implementations of sinusoidal modeling derive sinusoidal components from the Short-Time Fourier Transform (STFT) and are of limited use for time-and frequency-scale modification because of artifacts caused by phase uncertainty and discotinuities. (A more detailed description of the STFT may be found in signal processing texts.) Pitch tracking analysis methods have been used in the past to solve some of these problems, but their use is restricted to the class of monophonic, strongly harmonic signals (Grey 1975, Haken 1989). McAulay and Quatieri (McAulay and Quatieri 1985) propose a sinusoidal analysis technique for speech processing. The premise of the MQ technique is that a sound can be represented by a collection of sinusoidal components (called tracks), each with time-varying amplitude and frequency. To construct these tracks, STFT's are performed on a signal at regular intervals, called frames. Amplitude peaks in the resulting frequency spectra are identified, and parabolic interpolation is used to obtain a close approximation of the exact spectral peak frequencies. These peaks are the most prominent frequencies in the sound at that instant. The peaks in adjacent frames are compared and peaks of similar frequencies are matched. A continuous chain of these matched peaks is a track. A peak that is not matched represents the birth or death of a track. MQ synthesis uses cubic phase interpolation to reduce phase uncertainty and eliminate phase discontinuities. 2. Lemur Extensions Lemur is a Macintosh implementation of the MQ technique with some extensions. It is based on the program MQAN, written by Rob Maher and James Beauchamp at the Computer Music Project at the University of Illinois (Maher 1989), which implemented the original MQ analysis/synthesis technique on a UNIX system. Lemur provides some extensions to the basic MQ technique.},
address = {San Jose, USA},
author = {Fitz, Kelly and Walker, William and Haken, Lippold},
booktitle = {Proceedings of the International Computer Music Conference},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Fitz, Walker, Haken - 1992 - Extending the McAulay-Quatieri Analysis for Synthesis with a Limited Number of Oscillators.pdf:pdf},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {381--382},
title = {{Extending the McAulay-Quatieri Analysis for Synthesis with a Limited Number of Oscillators}},
year = {1992}
}
@article{Flanagan:1980:SpeechParametricCoding:ASOC,
abstract = {We suggest that a class of speech coders can be designed from criteria that are perception-specific. Such a class represents a middle ground between "waveform" coders and speech-specific "source" coders. We argue that, within perceptually acceptable limits, a bandpass portion of a speech signal can be represented by its short-time spectral amplitude [S [and its phase derivative q. Both parameters are evaluated at the center frequency to. of the passband whose spectral width is W. rad/s. We show that the short-time spectral amplitude is identical to the Hilbert envelope A. of the passband waveform. As a by-product, we show that the bandwidth occupied by both A. 2 and A. 2 q. is the same as that for their parent bandpass signal, but their spectra are translated to the range 0 to W.. No comparable statements can be adduced for A. and q., but on practical grounds, we conjecture that useful low-pass limits of less than W./2 can be applied to both. Finally, we outline techniques to utilize the passband parameters A., q., A.  and A.  qn in digital voice coding},
author = {Flanagan, James L.},
doi = {10.1121/1.384752},
file = {::},
journal = {The Journal of the Acoustical Society of America},
keywords = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
mendeley-tags = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
month = {oct},
number = {2},
pages = {412--419},
publisher = {Acoustical Society of America (ASA)},
title = {{Parametric coding of speech spectra}},
volume = {68},
year = {1980}
}
@article{Flanagan:1966:PhaseVocoder:BSTJ,
author = {Flanagan, James L.. and Golden, Roger M.},
file = {::},
journal = {The Bell System Technical Journal},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {1493--1509},
title = {{Phase Vocoder}},
volume = {45},
year = {1966}
}
@book{Fletcher:1991:PhysicsMusicalInstruments:BOOK,
author = {Fletcher, Neville H. and Rossing, Thomas D.},
publisher = {Springer},
title = {{The Physics of Musical Instruments}},
year = {1991}
}

@InProceedings{Fonseca:2017:ASC:DACSE,
  author =        {Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier},
  title =         {{Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Fonseca{\_}181.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Fuentes:2019:BeatTracking:ICASSP,
address = {Brighton, UK},
author = {Fuentes, Magdalena and McFee, Brian and Crayencour, H{\'{e}}l{\`{e}}ne C. and Essid, Slim and Bello, Juan Pablo},
booktitle = {Proceedings of the 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682870},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/A music Structure Informaed Downbeat Tracking system using Skip chain conditional randowm fields and deep learning.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {Convolutional-Recurrent Neural Networks,Deep Learning,Downbeat Tracking,Music Structure,Skip-Chain Conditional Random Fields,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {481--485},
title = {{A Music Structure Informed Downbeat Tracking System Using Skip-chain Conditional Random Fields and Deep Learning}},
year = {2019}
}

@InProceedings{Gajarsky2018,
  author =        {Gajarsky, Tomas and Purwins, Hendrik},
  title =         {{An Xception Residual Recurrent Neural Network for Audio Event Detection and Tagging}},
  booktitle =     {Proceedings of the Sound and Music Computing Conference (SMC)},
  year =          {2018},
  pages =         {210--216},
  file =          {:Users/jakobabeer/Downloads/smc{\_}2018{\_}028.pdf:pdf},
  keywords =      {machine{\_}listening},
  mendeley-tags = {machine{\_}listening}
}
@inproceedings{Gararsky:2018:Xception:SMC,
author = {Gajarsky, Tomas and Purwins, Hendrik},
booktitle = {Proceedings of the Sound and Music Computing Conference (SMC)},
file = {:Users/jakobabeer/Downloads/smc{\_}2018{\_}028.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {210--216},
title = {{An Xception Residual Recurrent Neural Network for Audio Event Detection and Tagging}},
year = {2018}
}
@inproceedings{Gemmeke:2017:Audioset:ICASSP,
address = {New Orleans, LA, USA},
author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45857.pdf:pdf},
pages = {776--780},
title = {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
year = {2017}
}
@article{Gfeller:2019:SPLICE:ARXIV,
abstract = {We propose a model to estimate the fundamental frequency in monophonic audio, often referred to as pitch estimation. We acknowledge the fact that obtaining ground truth annotations at the required temporal and frequency resolution is a particularly daunting task. Therefore, we propose to adopt a self-supervised learning technique, which is able to estimate (relative) pitch without any form of supervision. The key observation is that pitch shift maps to a simple translation when the audio signal is analysed through the lens of the constant-Q transform (CQT). We design a self-supervised task by feeding two shifted slices of the CQT to the same convolutional encoder, and require that the difference in the outputs is proportional to the corresponding difference in pitch. In addition, we introduce a small model head on top of the encoder, which is able to determine the confidence of the pitch estimate, so as to distinguish between voiced and unvoiced audio. Our results show that the proposed method is able to estimate pitch at a level of accuracy comparable to fully supervised models, both on clean and noisy audio samples, yet it does not require access to large labeled datasets},
archivePrefix = {arXiv},
arxivId = {1910.11664},
author = {Gfeller, Beat and Frank, Christian and Roblek, Dominik and Sharifi, Matt and Tagliasacchi, Marco and Velimirovi{\'{c}}, Mihajlo},
eprint = {1910.11664},
file = {::},
journal = {ArXiv e-prints},
pages = {1--9},
title = {{SPICE: Self-supervised Pitch Estimation}},
url = {http://arxiv.org/abs/1910.11664},
year = {2019}
}
@book{Ghahramani:2004:UL:BOOK,
abstract = {Abstract We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised ... $\backslash$n},
author = {Ghahramani, Zoubin},
file = {:Users/jakobabeer/Downloads/ul.pdf:pdf},
title = {{Unsupervised Learning}},
year = {2004}
}
@article{Gharib:2019:ASCCompetition:MLSP,
archivePrefix = {arXiv},
arxivId = {1808.02357},
author = {Gharib, Shayan and Derrar, Honain and Niizumi, Daisuke and Senttula, Tuukka and Tommola, Janne and Heittola, Toni and Virtanen, Tuomas and Huttunen, Heikki},
doi = {10.1109/MLSP.2018.8517000},
eprint = {1808.02357},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Gharib et al. - 2018 - Acoustic Scene Classification A Competition Review.pdf:pdf},
isbn = {9781538654774},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Acoustic Scene Classification,DCASE,Data Augmentation,Kaggle,acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification: A Competition Review}},
year = {2018}
}
@inproceedings{Gharib:2018:DomainAdaptationASC:DCASE,
address = {Surrey, UK},
author = {Gharib, Shayan and Drossos, Konstantinos and Emre, Cakir and Serdyuk, Dmitriy and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Gharib et al. - 2018 - Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
mendeley-tags = {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
title = {{Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Goodfellow:2014:GAN:NIPS,
abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and {Yoshua Bengio}},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/5423-generative-adversarial-nets.pdf:pdf},
issn = {10495258},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
year = {2014}
}
@article{Gordon:2018:MorphNet:CVPR,
abstract = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
archivePrefix = {arXiv},
arxivId = {1711.06798},
author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien Ju and Choi, Edward},
doi = {10.1109/CVPR.2018.00171},
eprint = {1711.06798},
file = {::},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {1586--1595},
title = {{MorphNet: Fast {\&} Simple Resource-Constrained Structure Learning of Deep Networks}},
year = {2018}
}
@article{Goto:2004:RealtimeMelodyBassTranscription:SP,
abstract = {In this paper, we describe the concept of music scene description and address the problem of detecting melody and bass lines in real-world audio signals containing the sounds of various instruments. Most previous pitch-estimation methods have had difficulty dealing with such complex music signals because these methods were designed to deal with mixtures of only a few sounds. To enable estimation of the fundamental frequency (F0) of the melody and bass lines, we propose a predominant-F0 estimation method called PreFEst that does not rely on the unreliable fundamental compo- nent and obtains the most predominant F0 supported by harmonics within an intentionally limited frequency range. This method estimates the relative dominance of every possible F0 (represented as a probability density function of the F0) by using MAP (maximum a posteriori probability) estimation and considers the F0!s temporal continuity by using a multiple-agent architecture. Experimental results with a set of ten music excerpts from compact-disc recordings showed that a real-time system implementing this method was able to detect melody and bass lines about 80{\%} of the time these existed. !},
author = {Goto, Masataka},
file = {:Users/jakobabeer/Downloads/Goto{\_}2004{\_}Speech Communication{\_}A Real-Time Music-Scene-Description System - Predominant-F0 Estimation for Detecting Melody and Bass Line Kopie.pdf:pdf},
journal = {Speech Communication},
keywords = {bass{\_}transcription,melody{\_}transcription},
mendeley-tags = {bass{\_}transcription,melody{\_}transcription},
pages = {311--329},
title = {{A real-time music scene description system: predominant-F0 estimation for detecting melody and bass lines in real-world audio signals}},
volume = {43},
year = {2004}
}
@article{Goto,
author = {Goto, Masataka},
file = {::},
journal = {Ismir2002.Ismir.Net},
pages = {1--2},
title = {{ISMIR2002RWCMDBgoto.pdf}},
url = {http://ismir2002.ismir.net/proceedings/03-SP04-1.pdf}
}
@article{Gouyon2006,
abstract = {We report on the tempo induction contest organized during the International Conference on Music Information Retrieval (ISMIR 2004) held at the University Pompeu Fabra in Barcelona, Spain, in October 2004. The goal of this contest was to evaluate some state-of-the-art algorithms in the task of inducing the basic tempo (as a scalar, in beats per minute) from musical audio signals. To our knowledge, this is the first published large scale cross-validation of audio tempo induction algorithms. Participants were invited to submit algorithms to the contest organizer, in one of several allowed formats. No training data was provided. A total of 12 entries (representing the work of seven research teams) were evaluated, 11 of which are reported in this document. Results on the test set of 3199 instances were returned to the participants before they were made public. Anssi Klapuri's algorithm won the contest. This evaluation shows that tempo induction algorithms can reach over 80{\%} accuracy for music with a constant tempo, if we do not insist on finding a specific metrical level. After the competition, the algorithms and results were analyzed in order to discover general lessons for the future development of tempo induction systems. One conclusion is that robust tempo induction entails the processing of frame features rather than that of onset lists. Further, we propose a new "redundant" approach to tempo induction, inspired by knowledge of human perceptual mechanisms, which combines multiple simpler methods using a voting mechanism. Machine emulation of human tempo induction is still an open issue. Many avenues for future work in audio tempo tracking are highlighted, as for instance the definition of the best rhythmic features and the most appropriate periodicity detection method. In order to stimulate further research, the contest results, annotations, evaluation software and part of the data are available at http://ismir2004.ismir.net/ISMIR{\_}Contest.html},
author = {Gouyon, Fabien and Klapuri, Anssi and Dixon, Simon and Alonso, Miguel and Tzanetakis, George and Uhle, Christian and Cano, Pedro},
doi = {10.1109/TSA.2005.858509},
file = {::},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Benchmark,Evaluation,Tempo induction},
number = {5},
pages = {1832--1844},
title = {{An experimental comparison of audio tempo induction algorithms}},
volume = {14},
year = {2006}
}
@article{Grasis:2014:MultipleExpertInstrumentRecognition:LNCS,
author = {Grasis, Mikus and Abe{\ss}er, Jakob and Dittmar, Christian and Lukashevich, Hanna},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Grasis et al. - 2014 - A multiple-expert framework for instrument recognition.pdf:pdf},
isbn = {978-3-319-12976-1},
journal = {Sound, music, and motion. 10th International Symposium, CMMR 2013 : Marseille, France, October 15 - 18, 2013; Revised selected papers, Lecture Notes in Computer Science},
keywords = {Classifier ensemble,Decision fusion,Instrument recognition,Overtones,Partial tracking,Partial-wise features,instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
pages = {619--634},
title = {{A Multiple-Expert Framework for Instrument Recognition}},
volume = {8905},
year = {2014}
}

@InProceedings{Green:2017:SpatialFeaturesASC:DCASE,
  author =        {Green, Marc C. and Murphy, Damian},
  title =         {{Acoustic Scene Classification using Spatial Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  abstract =      {Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or bin-aural datasets. This paper introduces EigenScape -a new dataset of 4th-order Ambisonic acoustic scene recordings -and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient -Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Direc-tional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The dif-ferences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Green{\_}126.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Grollmisch:2019:ISA:EUSIPCO,
address = {A Coruna, Spain},
author = {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
booktitle = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Grollmisch{\_}2019{\_}EUSIPCO.pdf:pdf},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis (ISA)}},
year = {2019}
}
@article{Grollmisch2019,
abstract = {The ongoing process of automation in production lines increases the requirements for robust and reliable quality control. Acoustic quality control can play a major part in advanced quality control systems since several types of faults such as changes in machine conditions can be heard by experienced machine operators but can hardly be detected otherwise. To this day, acoustic detection systems using airborne sounds struggle due to the highly complex noise scenarios inside factories. Machine learning systems are theoretically able to cope with these conditions. However, recent advancements in the field of Industrial Sound Analysis (ISA) are sparse compared to related research fields like Music Information Retrieval (MIR) or Acoustic Event Detection (AED). One main reason is the lack of freely available datasets since most of the data is very sensitive for companies. Therefore, three novel datasets for ISA with different application fields were recorded and published along with this paper: detection of the operational state of an electric engine, detection of the surface of rolling metal balls, and detection of different bulk materials. For each dataset, neural network based baseline systems were evaluated. The results show that such systems obtain high classification accuracies over all datasets in many of the subtasks which demonstrates the feasibility of audio-based analysis of industrial analysis scenarios. However, the baseline systems remain highly sensitive to changes in the recording setup, which leaves a lot of room for improvement. The main goal of this paper is to stimulate further research in the field of ISA.},
author = {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Grollmisch et al. - 2019 - Sounding Industry Challenges and Datasets for Industrial Sound Analysis.pdf:pdf},
journal = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
keywords = {ima,own},
mendeley-tags = {ima,own},
title = {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis}},
year = {2019}
}
@inproceedings{Grollmisch:2019:S2S:SMC,
address = {M{\'{a}}laga, Spain},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a},
booktitle = {Proceedings of the Sound {\&} Music Computing Conference (SMC)},
file = {:Users/jakobabeer/Downloads/SMC{\_}2019{\_}S2S{\_}Demo.pdf:pdf},
keywords = {idmt},
mendeley-tags = {idmt},
pages = {3},
title = {{Automatic Chord Recognition in Music Education Applications}}
}
@inproceedings{Grollmisch:2019:EnsembleSize:CMMR,
address = {Marseille, France},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Mora-{\'{A}}ngel, Fernando and Gil, Gustavo L{\'{o}}pez},
booktitle = {Proceedings of the 14th International Symposium of Computer Music Multidisciplinary Research (CMMR)},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Ensemble size classification in Colombian Andean string music recordings}},
year = {2019}
}
@inproceedings{Gururani:2019:IREC:ISMIR,
abstract = {While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or `attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results.},
address = {Delft, The Netherlands},
archivePrefix = {arXiv},
arxivId = {1907.04294},
author = {Gururani, Siddharth and Sharma, Mohit and Lerch, Alexander},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference},
eprint = {1907.04294},
file = {:Users/jakobabeer/Downloads/Gururani-et-al.-2019-An-Attention-Mechanism-for-Music-Instrument-Recogn.pdf:pdf},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
title = {{An Attention Mechanism for Musical Instrument Recognition}},
url = {http://arxiv.org/abs/1907.04294},
year = {2019}
}
@article{Haeusser2017,
abstract = {In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unlabeled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. Associations are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.},
archivePrefix = {arXiv},
arxivId = {1706.00909},
author = {Haeusser, Philip and Mordvintsev, Alexander and Cremers, Daniel},
doi = {10.1109/CVPR.2017.74},
eprint = {1706.00909},
file = {::},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
pages = {626--635},
title = {{Learning by association a versatile semi-supervised training method for neural networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Han2017,
abstract = {Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the class-wise average followed by normalization, and the other perform temporally local class-wise max-pooling on the output probability prior to averaging and normalization steps to minimize the effect of averaging process suppresses the activation of sporadically appearing instruments. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Our analysis on the instrument-wise performance found that the onset type is a critical factor for recall and precision of each instrument. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.619 for micro and 0.513 for macro, respectively, achieving 23.1{\%} and 18.8{\%} in performance improvement compared with the state-of-the-art algorithm.},
archivePrefix = {arXiv},
arxivId = {1605.09507},
author = {Han, Yoonchang and Kim, Jaehun and Lee, Kyogu},
doi = {10.1109/TASLP.2016.2632307},
eprint = {1605.09507},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Convolutional neural networks,deep learning,instrument recognition,multi-layer neural network,music information retrieval},
number = {1},
pages = {208--221},
title = {{Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music}},
volume = {25},
year = {2017}
}
@inproceedings{Han:2017:BinauralASC:DCASE,
author = {Han, Yoonchang and Park, Jeongsoo and Lee, Kyogu},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Han{\_}206.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification}},
year = {2017}
}
@article{Hasani2019,
author = {Hasani, Moein},
file = {::},
keywords = {batch normalization,convolutional neural networks},
title = {{IEEE Copyright Notice {\textcopyright} 2019 IEEE . Personal use of this material is permitted . Permission from IEEE must be obtained for all other uses , in any current or future media , including reprinting / republishing this material for advertising or promotional p}},
year = {2019}
}
@inproceedings{Hawley:2019:Compressor:AES,
author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis},
booktitle = {Proceedings of the 147th Audio Engineering Society Convention},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{SignalTrain: Profiling Audio Compressors with Deep Neural Networks}},
year = {2019}
}
@article{Hawthorne2017,
abstract = {We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100{\%} relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.},
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
eprint = {1710.11153},
file = {::},
title = {{Onsets and Frames: Dual-Objective Piano Transcription}},
url = {http://arxiv.org/abs/1710.11153},
year = {2017}
}
@inproceedings{Hawthorne:2018:PianoTranscription:ISMIR,
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11153v1},
author = {Hawthorne, Curtis and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Roberts, Adam},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {arXiv:1710.11153v1},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Hawthorne et al. - Unknown - ONSETS AND FRAMES DUAL-OBJECTIVE PIANO TRANSCRIPTION Curtis.pdf:pdf},
title = {{Onsets and frames: Dual-objective piano transcription}},
year = {2018}
}
@article{Hawthorne2018,
abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\~{}}0.1 ms to {\~{}}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\~{}}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
archivePrefix = {arXiv},
arxivId = {1810.12247},
author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
eprint = {1810.12247},
file = {::},
pages = {1--12},
title = {{Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset}},
url = {http://arxiv.org/abs/1810.12247},
year = {2018}
}
@inproceedings{Hedelin:1981:ToneOrientedVoiceExcitedVocoder:ICASSP,
abstract = {An LPC base-band vocoder is developed. The novel feature concerns the coding of the base-band. A model is set up for the base-band as a set of modulated tones. Algorithms are presented for the extraction of amplitude and phase/frequency of the tones. Implementation aspects as well as simulation results are discussed. Total bit rates in the order of 3,2-4.8 kbits are possible where approximately one half of the bits represents the base-band coding. Experiments have shown the coder to be robust to background noi as.},
author = {Hedelin, Per},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
title = {{A Tone-Oriented Voice-Excited Vocoder}},
year = {1981}
}
@article{Hershey:2017:CNN:ICASSP,
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.09430v2},
author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P.W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
doi = {10.1109/ICASSP.2017.7952132},
eprint = {arXiv:1609.09430v2},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Hershey et al. - 2017 - CNN architectures for large-scale audio classification.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification},
pages = {131--135},
title = {{CNN architectures for large-scale audio classification}},
year = {2017}
}
@inproceedings{Hershey2017,
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.09430v2},
author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7952132},
eprint = {arXiv:1609.09430v2},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Hershey et al. - 2017 - CNN architectures for large-scale audio classification.pdf:pdf},
isbn = {978-1-5090-4117-6},
issn = {15206149},
keywords = {Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification},
month = {mar},
pages = {131--135},
publisher = {IEEE},
title = {{CNN architectures for large-scale audio classification}},
url = {http://ieeexplore.ieee.org/document/7952132/},
year = {2017}
}
@inproceedings{Hestermann:2019:SelectiveHearing:ICSA,
author = {Hestermann, Simon and Lukashevich, Hanna and Sladeczek, Christoph},
booktitle = {Proceedings of the 5th International Conference on Spatial Audio (ICSA)},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Deep Neural Network Approaches for Selective Hearing based on Spatial Data Simulation}},
year = {2019}
}
@article{Holzapfel2012,
abstract = {In this paper, we propose a method that can identify challenging music samples for beat tracking without ground truth. Our method, motivated by the machine learning method selective sampling, is based on the measurement of mutual agreement between beat sequences. In calculating this mutual agreement we show the critical inuence of different evaluation measures. Using our approach we demonstrate how to compile a new evaluation dataset comprised of difcult excerpts for beat tracking and examine this difculty in the context of perceptual and musical properties. Based on tag analysis we indicate the musical properties where future advances in beat tracking research would be most protable and where beat tracking is too difcult to be attempted. Finally, we demonstrate how our mutual agreement method can be used to improve beat tracking accuracy on large music collections.},
author = {Holzapfel, Andr{\'{e}} and Davies, Matthew E P and Zapata, Jos{\'{e}} R. and Oliveira, Jo{\~{a}}o Lobato and Gouyon, Fabien},
doi = {10.1109/TASL.2012.2205244},
file = {::},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Beat tracking,evaluation,ground truth annotation,selective sampling},
month = {nov},
number = {9},
pages = {2539--2548},
title = {{Selective Sampling for Beat Tracking Evaluation}},
url = {http://ieeexplore.ieee.org/document/6220849/},
volume = {20},
year = {2012}
}
@inproceedings{Hou:2019:SoundEvent:ICASSP,
author = {Hou, Yuanbo and Kong, Qiuqiang and Li, Shengchen and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/icassp.2019.8683627},
file = {:Users/jakobabeer/Downloads/08683627.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {46--50},
title = {{Sound Event Detection with Sequentially Labelled Data Based on Connectionist Temporal Classification and Unsupervised Clustering}},
year = {2019}
}
@article{Hoyer:2004:NMF:JMLR,
author = {Hoyer, Patrik O},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {idmt},
mendeley-tags = {idmt},
pages = {1457--1469},
title = {{Non-negative Matrix Factorization with Sparseness Constraints}},
volume = {5},
year = {2004}
}
@inproceedings{Huang:2019:ASCEnsemble:DCASE,
address = {New York, NY, USA},
author = {Huang, Jonathan and Lu, Hong and Lopez-Meyer, Paulo and Maruri, Hector A. Cordourier and Ontiveros, Juan A. del Hoyo},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2019 - Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {94--98},
title = {{Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging}},
year = {2019}
}
@inproceedings{Humphrey:2018:InstrumentRecognition:ISMIR,
abstract = {Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music informa-tion retrieval. While there has been significant progress in developing predictive models for this and related classifi-cation tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset con-tains 20,000 examples of Creative Commons-licensed mu-sic available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the data-set was sampled and annotated, and compare its character-istics to similar, previous data-sets. Finally, we present ex-perimental results and baseline model performance to mo-tivate future work.},
address = {Paris, France},
author = {Humphrey, Eric J and Durand, Simon and Mcfee, Brian},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Downloads/ismir2018{\_}openmic.pdf:pdf},
pages = {438--444},
title = {{OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition}},
url = {http://bmcfee.github.io/papers/ismir2018{\_}openmic.pdf},
year = {2018}
}
@inproceedings{Hung:2019:InstrumentRecognition:ICASSP,
address = {Brighton, UK},
author = {Hung, Yun-Ning and Chen, Yi-An and Yang, Yi-Hsuan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683426},
file = {:Users/jakobabeer/Downloads/08683426.pdf:pdf},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
month = {may},
pages = {381--385},
title = {{Multitask Learning for Frame-level Instrument Recognition}},
url = {https://ieeexplore.ieee.org/document/8683426/},
year = {2019}
}
@inproceedings{HungYang:2018:FrameIRTimbrePitch:ISMIR,
abstract = {Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at $\backslash$url{\{}https://biboamy.github.io/instrument-recognition/{\}}.},
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {1806.09587},
author = {Hung, Yun-Ning and Yang, Yi-Hsuan},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {1806.09587},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Hung, Yang - 2018 - Frame-Level Instrument Recognition by Timbre and Pitch.pdf:pdf},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
pages = {135--142},
title = {{Frame-level Instrument Recognition by Timbre and Pitch}},
url = {http://arxiv.org/abs/1806.09587},
year = {2018}
}
@inproceedings{Imoto:2019:EventDetection:ICASSP,
abstract = {The types of sound events that occur in a situation are limited, and some sound events are likely to co-occur; for instance, ``dishes'' and ``glass jingling.'' In this paper, we introduce a technique of sound event detection utilizing graph Laplacian regularization taking the sound event co-occurrence into account. To consider the co-occurrence of sound events in a sound event detection system, we first represent sound event occurrences as a graph whose nodes indicate the frequency of event occurrence and whose edges indicate the co-occurrence of sound events. We then utilize this graph structure for sound event modeling, which is optimized under an objective function with a regularization term considering the graph structure. Experimental results obtained using TUT Acoustic Scenes 2016 development and 2017 development datasets indicate that the proposed method improves the detection performance of sound events by 7.9 percentage points compared to that of the conventional CNN-BiGRU-based method in terms of the segment-based F1-score. Moreover, the results show that the proposed method can detect co-occurring sound events more accurately than the conventional method.},
address = {Brighton, UK},
author = {Imoto, Keisuke and Kyochi, Seisuke},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683708},
file = {:Users/jakobabeer/Downloads/08683708.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {1--5},
title = {{Sound Event Detection Using Graph Laplacian Regularization Based on Event Co-occurrence}},
url = {https://ieeexplore.ieee.org/document/8683708/},
year = {2019}
}
@inproceedings{Imoto:2017:ASC:EUSIPCO,
address = {Kos Island, Greece},
author = {Imoto, Keisuke and Ono, Nobutaka},
booktitle = {Proceedings of the 25th European Signal Processing Conference (EUSIPCO)},
doi = {10.23919/EUSIPCO.2017.8081616},
file = {:Users/jakobabeer/Downloads/08081616.pdf:pdf},
isbn = {9780992862671},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {2279--2283},
title = {{Acoustic Scene Classification Based on Generative Model of Acoustic Spatial Words for Distributed Microphone Array}},
year = {2017}
}
@inproceedings{Ioffe:2015:BatchNorm:ICML,
address = {Lille, France},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
doi = {10.1080/17512786.2015.1058180},
file = {:Users/jakobabeer/Downloads/ioffe15.pdf:pdf},
issn = {17512794},
pages = {448--456},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{Iversen2008,
abstract = {The ability to perceive a musical beat (and move in synchrony with it) seems widespread, but we currently lack normative data on the distribution of this ability in musically untrained individuals. To aid in the survey of beat processing abilities in the general population, as well as to attempt to identify and differentiate impairments in beat processing, we have developed a psychophysical test called the Beat Alignment Test (BAT). The BAT is intended to complement existing tests of rhythm processing by directly examining beat perception in isolation from beat synchronization. The goals of the BAT are 1) to study the distribution of beat-based processing abilities in the normal population and 2) to provide a way to search for rhythm deaf individuals, who have trouble with beat processing in music though they are not tone deaf. The BAT is easily implemented and it is our hope that it is widely adopted. Data from a pilot study of 30 individuals is presented.},
author = {Iversen, John R. and Patel, Aniruddh D.},
file = {::},
isbn = {9784990420802},
journal = {Proceedings of the 10th International Conference on Music Perception and Cognition},
keywords = {[Electronic Manuscript]},
number = {Icmpc 10},
pages = {465--468},
title = {{The Beat Alignment Test (BAT): Surveying beat processing abilities in the general population}},
url = {http://www.nsi.edu/users/iversen/pubs/Iversen{\_}Patel{\_}2008{\_}ICMPC10{\_}BAT{\_}paper.pdf},
year = {2008}
}
@article{Jankowski2020,
abstract = {Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available.},
archivePrefix = {arXiv},
arxivId = {2001.04619},
author = {Jankowski, Charles and Mruthyunjaya, Vishwas and Lin, Ruixi},
eprint = {2001.04619},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/2001.04619.pdf:pdf},
title = {{Improved Robust ASR for Social Robots in Public Spaces}},
url = {http://arxiv.org/abs/2001.04619},
year = {2020}
}
@inproceedings{Jansen:2018:UnsupervisedLearning:ICASSP,
address = {Calgary, AB, Canada},
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P W and Hershey, Shawn and Liu, Jiayang and Moore, R Channing and Saurous, Rif A},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jansen et al. - Unknown - Unsupervised learning of semantic audio representations.pdf:pdf},
pages = {126--130},
publisher = {EE},
title = {{Unsupervised Learning of Semantic Audio Representations}},
year = {2018}
}
@inproceedings{Jati:2019:EventDetection:ICASSP,
address = {Brighton, UK},
author = {Jati, Arindam and Kumar, Naveen and Chen, Ruxin and Georgiou, Panayiotis},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682341},
file = {:Users/jakobabeer/Downloads/08682341.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {6--10},
title = {{Hierarchy-aware Loss Function on a Tree Structured Label Space for Audio Event Detection}},
url = {https://ieeexplore.ieee.org/document/8682341/},
year = {2019}
}
@inproceedings{Jati:2020:ASC:ICASSP,
abstract = {Devices capable of detecting and categorizing acoustic scenes have numerous applications such as providing context-aware user experiences. In this paper, we address the task of characterizing acoustic scenes in a workplace setting from audio recordings collected with wearable microphones. The acoustic scenes, tracked with Bluetooth transceivers, vary dynamically with time from the egocentric perspective of a mobile user. Our dataset contains experience sampled long audio recordings collected from clinical providers in a hospital, who wore the audio badges during multiple work shifts. To handle the long egocentric recordings, we propose a Time Delay Neural Network{\~{}}(TDNN)-based segment-level modeling. The experiments show that TDNN outperforms other models in the acoustic scene classification task. We investigate the effect of primary speaker's speech in determining acoustic scenes from audio badges, and provide a comparison between performance of different models. Moreover, we explore the relationship between the sequence of acoustic scenes experienced by the users and the nature of their jobs, and find that the scene sequence predicted by our model tend to possess similar relationship. The initial promising results reveal numerous research directions for acoustic scene classification via wearable devices as well as egocentric analysis of dynamic acoustic scenes encountered by the users.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1911.03843},
author = {Jati, Arindam and Nadarajan, Amrutha and Mundnich, Karel and Narayanan, Shrikanth},
booktitle = {submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {1911.03843},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jati et al. - 2020 - Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting(2).pdf:pdf},
keywords = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
title = {{Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting}},
url = {http://arxiv.org/abs/1911.03843},
year = {2020}
}
@article{Jenckel2018,
author = {Jenckel, Martin and Parkala, Sourabh Sarvotham and Bukhari, Syed Saqib and Dengel, Andreas},
doi = {10.5220/0006592703880393},
file = {::},
isbn = {9789897582769},
journal = {Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
keywords = {Document Analysis,Fuzzy Ground Truth,LSTM,OCR},
number = {Icpram},
pages = {388--393},
title = {{Impact of training LSTM-RNN with fuzzy ground truth}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Jeong:2018:AudioTagging:DCASE,
abstract = {In this paper; we describe the techniques and models applied to our submission for DCASE 2018 task 2: General-purpose audio tagging ofFreesound content with AudioSet labels. We mainly fo- cus on how to train deep learning models efficiently against strong augmentation and label noise. First; we conducted a single-block DenseNet architecture and multi-head softmax classifier for effi- cient learning with mixup augmentation. For the label noise; we applied the batch-wise loss masking to eliminate the loss of out- liers in a mini-batch. We also tried an ensemble of various models; trained by using different sampling rate or audio representation.},
address = {Surrey, UK},
author = {Jeong, Il-Young and Lim, Hyungui},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jeong, Lim - 2018 - Audio Tagging System Using Densely Connected Convolutional Networks(2).pdf:pdf},
keywords = {audio{\_}tagging,machine{\_}listening},
mendeley-tags = {audio{\_}tagging,machine{\_}listening},
title = {{Audio Tagging System Using Densely Connected Convolutional Networks}},
year = {2018}
}
@techreport{Jiang,
abstract = {While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., ma-jor/minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training. In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary. Large-vocabulary chord transcription is a difficult task, as the number of chord qualities is large, and the distribution of training chord classes is extremely biased. For example, the Billboard dataset [2], a human-annotated dataset, contains 230 different chord qualities, or equivalently, 2,749 distinct chord classes 1. While the first 10{\%} chord qualities cover 93.86{\%} of the data, the last 50{\%} chord qualities only cover 0.35{\%} of the data altogether 2. Such a long-tailed chord distribution makes it extremely hard to model rare chord qualities. To bypass the problem, former systems typically adopt two kinds of strategies: chord quality simplification and 1 We here assume that each chord quality can be combined with all possible 12 roots except for the N chord. 2 In calculation, the chord quality counts are weighted by their durations .},
author = {Jiang, Junyan and Chen, Ke and Li, Wei and Xia, Gus},
file = {::},
title = {{LARGE-VOCABULARY CHORD TRANSCRIPTION VIA CHORD STRUCTURE DECOMPOSITION}}
}

@InProceedings{Jimenez:2017:ShiftInvariantASC:DCASE,
  author =        {Jim{\'{e}}nez, Abelino and Elizalde, Benjam{\'{i}}n and Raj, Bhiksha},
  title =         {{DCASE 2017 Task 1: Acoustic Scene Classification using Shift-Invariant Kernels and Random Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Jimenez{\_}195.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Jung:2018:FeatureEnsembleASC:DCASE,
address = {Surrey, UK},
author = {Jung, Jee-weon and Heo, Hee-soo and Shim, Hye-jin and Yu, Ha-jin},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jung et al. - 2018 - DNN based Multi-Level Feature Ensemble for Acoustic Scene Detection.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{DNN based Multi-Level Feature Ensemble for Acoustic Scene Detection}},
year = {2018}
}
@article{Kalchbrenner:2016:TCN:CORR,
abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
archivePrefix = {arXiv},
arxivId = {1610.10099},
author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and van den Oord, Aaron and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1610.10099},
file = {:Users/jakobabeer/Downloads/1610.10099.pdf:pdf},
journal = {CoRR},
title = {{Neural Machine Translation in Linear Time}},
url = {http://arxiv.org/abs/1610.10099},
volume = {abs/1610.1},
year = {2016}
}
@inproceedings{Kapka:2019:CRNNEnsemble:DCASE,
abstract = {In this technical report, we describe our method for DCASE2019 task 3: Sound Event Localization and Detection. We use four CRNN SELDnet-like single output models which run in a consecutive manner to recover all possible information of occurring events. We decompose the SELD task into estimating number of active sources, estimating direction of arrival of a single source, estimating direction of arrival of the second source where the direction of the first one is known and a multi-label classification task. We use custom consecutive ensemble to predict events' onset, offset, direction of arrival and class. The proposed approach is evaluated on the development set of TAU Spatial Sound Events 2019-Ambisonic.},
author = {Kapka, S{\l}awomir and Lewandowski, Mateusz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {CRNN,Index Terms-DCASE 2019,Sound Event Localization and Detection,ambisonics},
title = {{Sound Source Detection, Localization and Classification using Consecutive Ensemble of CRNN Models}},
url = {http://dcase.community/documents/challenge2019/technical{\_}reports/DCASE2019{\_}Kapka{\_}26.pdf},
year = {2019}
}
@inproceedings{Kareer2018,
abstract = {Knowing the number of sources present in a mixture is useful for many computer audition problems such as polyphonic music transcription, source separation, and speech enhancement. Most existing algorithms for these applications require the user to provide this number thereby limiting the possibility of complete automatization. In this paper we explore a few probabilistic and machine learning approaches for an autonomous source number estimation. We then propose an implementation of a multi-class classification method using convolutional neural networks for musical polyphony estimation. In addition, we use these results to improve the performance of an instrument classifier based on the same dataset. Our final classification results for both the networks, prove that this method is a promising starting point for further advancements in unsupervised source counting and separation algorithms for music and speech.},
address = {Milan, Italy},
author = {Kareer, Saarish and Basu, Sattwik},
booktitle = {Audio Engineering Society Convention 144},
file = {::},
title = {{Musical Polyphony Estimation}},
year = {2018}
}
@inproceedings{Kehling:2014:GuitarTranscription:DAFX,
address = {Erlangen, Germany},
author = {Kehling, Christian and Abe{\ss}er, Jakob and Dittmar, Christian and Schuller, Gerald},
booktitle = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Kehling{\_}2014{\_}DAFX.pdf:pdf},
keywords = {guitar{\_}transcription,m2d},
mendeley-tags = {guitar{\_}transcription,m2d},
pages = {1--8},
title = {{Automatic Tablature Transcription of Electric Guitar Recordings by Estimation of Score- and Instrument-Related Parameters}},
year = {2014}
}
@inproceedings{Kelz:2019:PianoTranscription:ICASSP,
abstract = {We investigate a late-fusion approach to piano transcription , combined with a strong temporal prior in the form of a handcrafted Hidden Markov Model (HMM). The network architecture under consideration is compact in terms of its number of parameters and easy to train with gradient descent and momentum. The network outputs are fused over time in the final stage to obtain note segmentations, with an HMM whose transition-and observation probabilities are chosen based on a model of attack decay sustain release (ADSR) envelope. The note segments are then subject to a final binary decision rule to reject too weak note segment hypotheses. 1. METHODS We would like to transcribe a polyphonic audio recording of a piano into a symbolic score. For each note sounding, we expect to obtain a tuple (s, e, n, v), denoting start, end, MIDI note number, and optionally, volume. 1.1 Deep convolutional neural network We employ a model with multiple outputs, predicting different note phases. A conceptual drawing is shown in Figure 1. The network input x t  R cb is a spectro-gram snippet, extending c context frames in time, and b bins in frequency. b is the number of bins resulting from passing a linear STFT through a filterbank with semi-logarithmically spaced, triangular filters, resulting in a resolution of approximately two bins per semitone. We choose c = 11, b = 144. The temporal resolution of the model is 50 [frames/s]. The target matrix y t  {\{}0, 1{\}} 883 decomposes into vectors y on t , y f rm t , and y off t respectively, denoting the onset , the intermediate note phases, and the offset for each note for the center frame within the context window c.},
address = {Brighton, UK},
author = {Kelz, Rainer and Bock, Sebastian and Widmer, Gerhard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683582},
file = {:Users/jakobabeer/Dropbox/ICASSP{\_}2019/to read/08683582.pdf:pdf},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {246--250},
publisher = {IEEE},
title = {{Deep Polyphonic ADSR Piano Note Transcription}},
url = {https://ieeexplore.ieee.org/document/8683582/},
year = {2019}
}
@article{Kelz2019,
abstract = {We investigate a late-fusion approach to piano transcription , combined with a strong temporal prior in the form of a handcrafted Hidden Markov Model (HMM). The network architecture under consideration is compact in terms of its number of parameters and easy to train with gradient descent and momentum. The network outputs are fused over time in the final stage to obtain note segmentations, with an HMM whose transition-and observation probabilities are chosen based on a model of attack decay sustain release (ADSR) envelope. The note segments are then subject to a final binary decision rule to reject too weak note segment hypotheses. 1. METHODS We would like to transcribe a polyphonic audio recording of a piano into a symbolic score. For each note sounding, we expect to obtain a tuple (s, e, n, v), denoting start, end, MIDI note number, and optionally, volume. 1.1 Deep convolutional neural network We employ a model with multiple outputs, predicting different note phases. A conceptual drawing is shown in Figure 1. The network input x t  R cb is a spectro-gram snippet, extending c context frames in time, and b bins in frequency. b is the number of bins resulting from passing a linear STFT through a filterbank with semi-logarithmically spaced, triangular filters, resulting in a resolution of approximately two bins per semitone. We choose c = 11, b = 144. The temporal resolution of the model is 50 [frames/s]. The target matrix y t  {\{}0, 1{\}} 883 decomposes into vectors y on t , y f rm t , and y off t respectively, denoting the onset , the intermediate note phases, and the offset for each note for the center frame within the context window c.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.09165v1},
author = {Kelz, Rainer and Bock, Sebastian and Widmer, Gerhard},
doi = {10.1109/icassp.2019.8683582},
eprint = {arXiv:1906.09165v1},
file = {::},
pages = {246--250},
title = {{Deep Polyphonic ADSR Piano Note Transcription}},
year = {2019}
}
@article{Kelz2016,
abstract = {In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset -- without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.},
archivePrefix = {arXiv},
arxivId = {1612.05153},
author = {Kelz, Rainer and Dorfer, Matthias and Korzeniowski, Filip and B{\"{o}}ck, Sebastian and Arzt, Andreas and Widmer, Gerhard},
eprint = {1612.05153},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kelz et al. - 2016 - On the Potential of Simple Framewise Approaches to Piano Transcription.pdf:pdf},
month = {dec},
title = {{On the Potential of Simple Framewise Approaches to Piano Transcription}},
url = {http://arxiv.org/abs/1612.05153},
year = {2016}
}
@article{Kim2019,
abstract = {We describe a novel weakly labeled Audio Event Classification approach based on a self-supervised attention model. The weakly labeled framework is used to eliminate the need for expensive data labeling procedure and self-supervised attention is deployed to help a model distinguish between relevant and irrelevant parts of a weakly labeled audio clip in a more effective manner compared to prior attention models. We also propose a highly effective strongly supervised attention model when strong labels are available. This model also serves as an upper bound for the self-supervised model. The performances of the model with self-supervised attention training are comparable to the strongly supervised one which is trained using strong labels. We show that our self-supervised attention method is especially beneficial for short audio events. We achieve 8.8{\%} and 17.6{\%} relative mean average precision improvements over the current state-of-the-art systems for SL-DCASE-17 and balanced AudioSet.},
archivePrefix = {arXiv},
arxivId = {1908.02876},
author = {Kim, Bongjun and Ghaffarzadegan, Shabnam},
doi = {10.23919/EUSIPCO.2019.8902567},
eprint = {1908.02876},
file = {::},
isbn = {9789082797039},
issn = {22195491},
title = {{Self-supervised Attention Model for Weakly Labeled Audio Event Classification}},
url = {http://arxiv.org/abs/1908.02876},
year = {2019}
}
@article{Kim:2019:Learning:NCA,
abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
author = {Kim, Jaehun and Urbano, Juli{\'{a}}n and Liem, Cynthia C.S. and Hanjalic, Alan},
doi = {10.1007/s00521-019-04076-1},
file = {:Users/jakobabeer/Downloads/Kim2019{\_}Article{\_}OneDeepMusicRepresentationToRu.pdf:pdf},
isbn = {0123456789},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Multitask learning,Music Information Retrieval,Representation learning},
title = {{One deep music representation to rule them all? A comparative analysis of different representation learning strategies}},
volume = {1},
year = {2019}
}
@article{Kim2018,
abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
archivePrefix = {arXiv},
arxivId = {1802.04051},
author = {Kim, Jaehun and Urbano, Juli{\'{a}}n and Liem, Cynthia C. S. and Hanjalic, Alan},
eprint = {1802.04051},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2018 - One Deep Music Representation to Rule Them All A comparative analysis of different representation learning strate(2).pdf:pdf},
keywords = {accepted to,learning for music and,multi-task learning,music information retrieval,neural computing and applications,representation learning,special issue on deep,this work has been},
month = {feb},
title = {{One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies}},
url = {http://arxiv.org/abs/1802.04051},
year = {2018}
}
@article{Kim:2019:AdversarialLearning:ARXIV,
abstract = {Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements on transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.},
archivePrefix = {arXiv},
arxivId = {1906.08512},
author = {Kim, Jong Wook and Bello, Juan Pablo},
eprint = {1906.08512},
file = {:Users/jakobabeer/Downloads/1906.08512.pdf:pdf},
journal = {arXiv},
title = {{Adversarial Learning for Improved Onsets and Frames Music Transcription}},
url = {http://arxiv.org/abs/1906.08512},
year = {2019}
}
@inproceedings{Kim:2018:PitchTracking:ICASSP,
abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
address = {New Orleans, USA},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8461329},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2018 - Crepe A Convolutional Representation for Pitch Estimation.pdf:pdf},
keywords = {Convolutional neural network,Pitch estimation,pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
pages = {161--165},
title = {{Crepe: A Convolutional Representation for Pitch Estimation}},
year = {2018}
}
@inproceedings{Koch:2015:SiameseOneShot:JLMR,
address = {Lille, France},
author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Koch - 2011 - Siamese Neural Networks for One-shot Image Recognition.pdf:pdf},
keywords = {siamese{\_}networks},
mendeley-tags = {siamese{\_}networks},
title = {{Siamese Neural Networks for One-shot Image Recognition}},
year = {2015}
}
@inproceedings{Kong:2019:SceneGeneration:ICASSP,
address = {Brighton, UK},
author = {Kong, Qiuqiang and Xu, Yong and Iqbal, Turab and Cao, Yin and Wang, Wenwu and Plumbley, Mark D},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781538646588},
pages = {925--929},
title = {{Acoustic Scene Generation with Conditional SampleRNN}},
year = {2019}
}
@article{Kong:2019:AttentionWeaklyLabel:IEEE_TASLP,
abstract = {Audio tagging is the task of predicting the presence or absence of sound classes within an audio clip. Previous work in audio classification focused on relatively small datasets limited to recognising a small number of sound classes. We investigate audio tagging on AudioSet, which is a dataset consisting of over 2 million audio clips and 527 classes. AudioSet is weakly labelled, in that only the presence or absence of sound classes is known for each clip, while the onset and offset times are unknown. To address the weakly-labelled audio classification problem, we propose attention neural networks as a way to attend the the most salient parts of an audio clip. We bridge the connection between attention neural networks and multiple instance learning (MIL) methods, and propose decision-level and feature-level attention neural networks for audio tagging. We investigate attention neural networks modelled by different functions, depths and widths. Experiments on AudioSet show that the feature-level attention neural network achieves a state-of-the-art mean average precision (mAP) of 0.369, outperforming the best multiple instance learning (MIL) method of 0.317 and Google's deep neural network baseline of 0.314. In addition, we discover that the audio tagging performance on AudioSet embedding features has a weak correlation with the number of training examples and the quality of labels of each sound class.},
author = {Kong, Qiuqiang and Yu, Changsong and Xu, Yong and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.},
doi = {10.1109/taslp.2019.2930913},
file = {::},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {11},
pages = {1--1},
publisher = {IEEE},
title = {{Weakly Labelled AudioSet Tagging with Attention Neural Networks}},
volume = {27},
year = {2019}
}
@article{Koops:2019:harmony_subjectivity:journal,
abstract = {Reference annotation datasets containing harmony annotations are at the core of a wide range of studies in music information retrieval (MIR) and related fields. The majority of these datasets contain single reference annotations describing the harmony of each piece. Nevertheless, studies showing differences among annotators in many other MIR tasks make the notion of a single ground-truth' reference annotation a tenuous one. In this paper, we introduce and analyse the Chordify Annotator Subjectivity Dataset (CASD) containing chord labels for 50 songs from 4 expert annotators in order to gain a better understanding of the differences between annotators in their chord label choice. Our analysis reveals that annotators use distinct chord-label vocabularies, with low chord-label overlap across all annotators. Between annotators, we find only 73 percent overlap on average for the traditional majorminor vocabulary and 54 percent overlap for the most complex chord labels. A factor analysis reveals the relative importance of triads, sevenths, inversions and other musical factors for each annotator on their choice of chord labels and reported difficulty of the songs. Our results further substantiate the existence of a harmonic subjectivity ceiling': an upper bound for evaluations in computational harmony research. Current state-of-the-art chord-estimation systems perform beyond this subjectivity ceiling by about 10 percent. This suggests that current ACE algorithms are powerful enough to tune themselves to particular annotators' idiosyncrasies. Overall, our results show that annotator subjectivity is an important factor in harmonic transcriptions, which should inform future studies into harmony perception and computational models of harmony.},
author = {Koops, Hendrik Vincent and de Haas, W. Bas and Burgoyne, John Ashley and Bransen, Jeroen and Kent-Muller, Anna and Volk, Anja},
doi = {10.1080/09298215.2019.1613436},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {Annotator subjectivity,annotator subjectivity,harmony,inter-rater,inter-rater agreement},
month = {may},
number = {0},
pages = {1--21},
publisher = {Taylor {\&} Francis},
title = {{Annotator subjectivity in harmony annotations of popular music}},
url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2019.1613436},
volume = {0},
year = {2019}
}
@article{Korbar2018,
abstract = {There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9{\%} in action recognition accuracy on UCF101 and a boost of +17.7{\%} on HMDB51.},
archivePrefix = {arXiv},
arxivId = {1807.00230},
author = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
eprint = {1807.00230},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {7763--7774},
title = {{Cooperative learning of audio and video models from self-supervised synchronization}},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{Kosmider:2019:DeviceCalibration:DCASE,
author = {Kosmider, Michal},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
title = {{Calibrating Neural Networks for Secondary Recording Devices}},
year = {2019}
}
@inproceedings{Kothinti:2019:SoundEvent:ICASSP,
address = {Brighton, UK},
author = {Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682772},
file = {:Users/jakobabeer/Downloads/08682772.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {36--40},
title = {{Joint Acoustic and Class Inference for Weakly Supervised Sound Event Detection}},
url = {https://ieeexplore.ieee.org/document/8682772/},
year = {2019}
}
@inproceedings{Koutini:2019:ReceptiveField:DCASE,
address = {New York, NY, USA},
author = {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Koutini, Eghbal-zadeh, Widmer - 2019 - Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {124--128},
title = {{Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Koutini:2019:ASC:DCASE,
author = {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard and Kepler, Johannes},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Koutini{\_}99.pdf:pdf},
pages = {1--5},
title = {{CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs}},
year = {2019}
}
@inproceedings{Krebs:2016:DownbeatTracking:ISMIR,
abstract = {In this paper, we propose a system that extracts the down-beat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results.},
address = {New York, NY, USA},
author = {Krebs, Florian and B{\"{o}}ck, Sebastian and Dorfer, Matthias and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/Downbeat Tracking using Beat synchronous Feature and RNN.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {129--135},
title = {{Downbeat Tracking Using Beat-Synchronous Features and Recurrent Neural Networks}},
year = {2016}
}
@article{Krebs:2013:Beat:ISMIR,
abstract = {Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pat- tern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observa- tion model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic pat- terns and evaluating beat and downbeat tracking, 697 ball- room dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces oc- tave errors (detection of half or double tempo) and sub- stantially improves downbeat tracking.},
author = {Krebs, Florian and B{\"{o}}ck, Sebastian and Widmer, Gerhard},
file = {::},
journal = {Ismir},
pages = {227--232},
title = {{Rhythmic Pattern Modeling for Beat and Downbeat Tracking in Musical Audio.}},
year = {2013}
}
@inproceedings{Krebs:2015:TempoMeterTracking:ISMIR,
abstract = {Dynamic Bayesian networks (e.g., Hidden Markov Mod-els) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhyth-mic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic pa-rameters from a piece of music. While this allows the mutual dependencies between these parameters to be ex-ploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We in-corporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets.},
address = {M{\'{a}}laga, Spain},
author = {Krebs, Florian and Sebastian, B and Widmer, Gerhard},
booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/git/2020{\_}ICASSP{\_}BeatTracking/5d6f937436a61f000198168d/material/An Efficient state space model for joint tempo and Mether Tracking.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {72--78},
title = {{An Efficient State-Space Model for Joint Tempo and Meter Tracking}},
year = {2015}
}
@article{Kruspe:2019:ARXIV,
abstract = {Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single class. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated via softmax. We solve this problem by introducing a "null class" centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched test set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples' distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.},
archivePrefix = {arXiv},
arxivId = {1906.00820},
author = {Kruspe, Anna},
eprint = {1906.00820},
file = {:Users/jakobabeer/Downloads/1906.00820.pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {machine{\_}learning},
mendeley-tags = {machine{\_}learning},
month = {jun},
title = {{One-Way Prototypical Networks}},
url = {http://arxiv.org/abs/1906.00820},
year = {2019}
}
@article{Kum:2017:MelodyCNN:PREPRINTS,
author = {Kum, Sangeun and Nam, Juhan},
doi = {10.20944/preprints201711.0027.v1},
file = {:Users/jakobabeer/Downloads/preprints201711.0027.v1.pdf:pdf},
journal = {Preprints},
title = {{Classification-Based Singing Melody Extraction Using Deep Convolutional Neural Networks}},
year = {2017}
}
@inproceedings{Kum:2016:Melody:ISMIR,
address = {New York City, USA},
author = {Kum, Sangeun and Oh, Changheun and Nam, Juhan},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, (ISMIR)},
file = {:Users/jakobabeer/Downloads/KumOhNam-ismir2016.pdf:pdf},
pages = {819--825},
title = {{Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks}},
year = {2016}
}
@inproceedings{Kumar:2018:ASC:ICASSP,
abstract = {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
annote = {- for weakly labeled data
- CNN
- audio of variable length
- usable for transfer learning for domain and task adaptation
- problem on scene classification:
-- few datasets
-- labeling expensive/difficult
-- begin and end of event subjective
- audioset with weak labels (DCASE2017)
- "Soundnet" transfer visual model to audio data
- train on "Audioset"(weakly labeled example from Youtube with 527 classes)
- tested on ESC-50 (same task but different domain) and DCASE2016 (task adaption)
- SLAT -{\textgreater} strong label assumption training (class correct for all patches)
- logmel spec as input, 44.1kHz, 128 mel bands, winsize 23ms, overlap 11.5
- global pooling at the end -{\textgreater} fully conv for variable length of input
- ESC-50: outperform sota by 9.3{\%}
- DCASE2016: absolute improvement of 4.1{\%}
- Audioset: sota results},
author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8462200},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Khadkevich, Fugen - 2018 - Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events an(3).pdf:pdf},
isbn = {978-1-5386-4658-8},
issn = {15206149},
keywords = {Audio Event Classification,Learning Representations,Transfer Learning,Weak Label Learning,acmus,dnn better},
mendeley-tags = {acmus,dnn better},
month = {apr},
pages = {326--330},
publisher = {IEEE},
title = {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
url = {https://ieeexplore.ieee.org/document/8462200/},
year = {2018}
}
@inproceedings{Kumar2017,
abstract = {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
annote = {- for weakly labeled data
- CNN
- audio of variable length
- usable for transfer learning for domain and task adaptation
- problem on scene classification:
-- few datasets
-- labeling expensive/difficult
-- begin and end of event subjective
- audioset with weak labels (DCASE2017)
- "Soundnet" transfer visual model to audio data
- train on "Audioset"(weakly labeled example from Youtube with 527 classes)
- tested on ESC-50 (same task but different domain) and DCASE2016 (task adaption)
- SLAT -{\textgreater} strong label assumption training (class correct for all patches)
- logmel spec as input, 44.1kHz, 128 mel bands, winsize 23ms, overlap 11.5
- global pooling at the end -{\textgreater} fully conv for variable length of input
- ESC-50: outperform sota by 9.3{\%}
- DCASE2016: absolute improvement of 4.1{\%}
- Audioset: sota results},
author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8462200},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Khadkevich, Fugen - 2018 - Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events an(3).pdf:pdf},
isbn = {978-1-5386-4658-8},
issn = {15206149},
keywords = {Audio Event Classification,Learning Representations,Transfer Learning,Weak Label Learning,acmus,dnn better},
mendeley-tags = {acmus,dnn better},
month = {apr},
pages = {326--330},
publisher = {IEEE},
title = {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
url = {https://ieeexplore.ieee.org/document/8462200/},
year = {2018}
}
@inproceedings{Lagrange:2003:PartialTrackingLinearPred:DAFX,
abstract = {In this paper, we introduce a new partial tracking method suitable for the sinusoidal modeling of mixtures of instrumental sounds with pseudo-stationary frequencies. This method, based on the linear prediction of the frequency evolutions of the partials, enables us to track these partials more accurately at the analysis stage, even in complex sound mixtures. This allows our spectral model to better handle polyphonic sounds.},
address = {London, UK},
author = {Lagrange, Mathieu and Marchand, Sylvain and Raspaud, Martin and Rault, Jean-Bernard},
booktitle = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {141--146},
title = {{Enhanced Partial Tracking Using Linear Prediction}},
url = {https://hal.archives-ouvertes.fr/hal-00308184},
year = {2003}
}
@inproceedings{Lagrange:2005:TrackingSMPolyphonic:ICASSP,
abstract = {This paper proposes to further improve the tracking of partials in a polyphonic context. Spectral characteristics of the controlling parameters (amplitude and frequency) are taken into account to ensure that these parameters evolve slowly with time. The resulting algorithm better tracks closely-spaced sinusoids and is able to avoid most of the spectral data belonging to noise. As a consequence , the proposed algorithm extracts a more meaningful sinu-soidal representation from polyphonic recordings.},
address = {Philadelphia},
author = {Lagrange, Mathieu and Marchand, Sylvain and Rault, Jean-Bernard},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {229--232, vol.3},
title = {{Tracking Partials for the Sinusoidal Modeling of Polyphonic Sounds}},
year = {2005}
}
@inproceedings{Lassseck:2018:Bird:DCASE,
abstract = {This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 $\backslash$$\backslash${\%} on the public challenge leaderboard.},
address = {Surrey, UK},
author = {Lasseck, Mario},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE)},
file = {::},
keywords = {Bird Detection,Data Augmentation,Deep Convolutional Neural Networks,Deep Learning,bird{\_}recognition,machine{\_}listening},
mendeley-tags = {bird{\_}recognition,machine{\_}listening},
pages = {143--147},
title = {{Acoustic bird detection with deep convolutional neural networks}},
year = {2018}
}
@techreport{Lee,
abstract = {Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and query-by-singer in both the in-domain and cross-domain tasks.},
author = {Lee, Kyungyun and Nam, Juhan},
file = {::},
title = {{LEARNING A JOINT EMBEDDING SPACE OF MONOPHONIC AND MIXED MUSIC SIGNALS FOR SINGING VOICE}},
url = {http://github.com/kyungyunlee/mono2mixed-singer}
}
@article{Lee2019,
abstract = {While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6{\%} through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.},
archivePrefix = {arXiv},
arxivId = {1910.12587},
author = {Lee, Tyler and Gong, Ting and Padhy, Suchismita and Rouditchenko, Andrew and Ndirango, Anthony},
eprint = {1910.12587},
file = {::},
pages = {1--10},
title = {{Label-efficient audio classification through multitask learning and self-supervision}},
url = {http://arxiv.org/abs/1910.12587},
year = {2019}
}
@inproceedings{Lehner:2019:ASCReject:DCASE,
address = {New York, NY, USA},
author = {Lehner, Bernhard and Koutini, Khaled and Schwarzlm{\"{u}}ller, Christopher and Gallien, Thomas and Widmer, Gerhard},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events},
file = {:Users/jakobabeer/Downloads/DCASE20191.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
title = {{Acoustic Scene Classification with Reject Option based on Resnets}},
year = {2019}
}
@article{Li2019,
abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches to address the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multimodal tasks. We benchmark two existing MIR tasks (multipitch analysis and score-informed source separation) on the dataset and compare them with other existing music audio datasets. In addition, we consider two novel multimodal MIR tasks (visually informed multipitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measurements and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.},
archivePrefix = {arXiv},
arxivId = {1612.08727},
author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
doi = {10.1109/TMM.2018.2856090},
eprint = {1612.08727},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
title = {{Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications}},
year = {2019}
}
@article{Li2019a,
abstract = {Convolutional neural networks are widely adopted in Acoustic Scene Classification (ASC) tasks, but they generally carry a heavy computational burden. In this work, we propose a lightweight yet high-performing baseline network inspired by MobileNetV2, which replaces square convolutional kernels with unidirectional ones to extract features alternately in temporal and frequency dimensions. Furthermore, we explore a dynamic architecture space built on the basis of the proposed baseline with the recent Neural Architecture Search (NAS) paradigm, which first trains a supernet that incorporates all candidate networks and then applies a well-known evolutionary algorithm NSGA-II to discover more efficient networks with higher accuracy and lower computational cost. Experimental results demonstrate that our searched network is competent in ASC tasks, which achieves 90.3{\%} F1-score on the DCASE2018 task 5 evaluation set, marking a new state-of-the-art performance while saving 25{\%} of FLOPs compared to our baseline network.},
archivePrefix = {arXiv},
arxivId = {1912.12825},
author = {Li, Jixiang and Liang, Chuming and Zhang, Bo and Wang, Zhao and Xiang, Fei and Chu, Xiangxiang},
eprint = {1912.12825},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1912.12825.pdf:pdf},
title = {{Neural Architecture Search on Acoustic Scene Classification}},
url = {http://arxiv.org/abs/1912.12825},
year = {2019}
}
@article{Liu:2019:EnvironmentSurveillance:ARXIV,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167},
author = {Li, Yuan and Cheng, Zhongwei and Liu, Jie and Yassin, Bourhan and Nan, Zhe and Luo, Jiebo},
eprint = {arXiv:1502.03167},
file = {:Users/jakobabeer/Downloads/1908.07517.pdf:pdf},
isbn = {9781510810587},
keywords = {acoustic surveillance,audio classi,cation,neural networks},
title = {{AI for Earth: Rainforest Conservation by Acoustic Surveillance}},
year = {2019}
}
@inproceedings{Li:2018:ASC:ICALIP,
abstract = {Although acoustic scene classification has been received great attention from researchers in the field of audio signal processing, it is still a challenging and unsolved task to date. In this paper, we present our work of acoustic scene classification for the challenge of the Detection and Classification of Acoustic Scenes and Events 2017, i.e., DCASE2017 challenge, using a feature of Deep Audio Feature (DAF) for acoustic scene representation and a classifier of Bidirectional Long Short Term Memory (BLSTM) network for acoustic scene classification. We first use a deep neural network to generate the DAF from Mel frequency cepstral coefficients, and then adopt a network of BLSTM fed by the DAF for acoustic scene classification. When evaluated on the official datasets of the DCASE2017 challenge, the proposed system outperforms the baseline system in terms of classification accuracy.},
address = {Copenhagen, Denmark},
author = {Li, Yanxiong and Li, Xianku and Zhang, Yuhan and Wang, Wucheng and Liu, Mingle and Feng, Xiaohui},
booktitle = {Proceedings of the 6th International Conference on Audio, Language and Image Processing (ICALIP)},
doi = {10.1109/ICALIP.2018.8455765},
file = {:Users/jakobabeer/Downloads/08455765.pdf:pdf},
isbn = {9781538651957},
keywords = {acoustic scene classification,acoustic{\_}scene{\_}classification,bidirectional long short term memory network,deep audio feature,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {371--374},
title = {{Acoustic Scene Classification Using Deep Audio Feature and BLSTM Network}},
year = {2018}
}
@inproceedings{Li:2019:MultilevelAttention:ICMEW,
abstract = {Acoustic scene classification (ASC) refers to the classification of audio into one of predefined classes that characterize the environment. People are used to combine log-mel filterbank features with convolutional neural network (CNN) to build ASC system. In this paper, we explore the use of deep scattering spectrum (DSS) features combined with a multi-level attention model based on CNN for ASC tasks. First, the time scatter and frequency scatter coefficients of DSS with different resolutions are explored as ASC features. Second, we incorporate a multi-level attention model into CNN to build the classification system. We then evaluate the proposed approach on the IEEE challenge of detection and classification of acoustic scenes and events 2018 (DCASE 2018) dataset. Results show that the DSS features provide between a 11{\%}-14{\%} relative improvement in accuracy over log-mel features, within a state-of-the-art framework. The application of multilevel attention model on CNN can improve the accuracy by nearly 5{\%}. The highest accuracy of our proposed system is 78.3{\%} on the development set.},
address = {Shanghai, China},
author = {Li, Zhitong and Hou, Yuanbo and Xie, Xiang and Li, Shengchen and Zhang, Liqiang and Du, Shixuan and Liu, Wei},
booktitle = {Proceedings of the IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
doi = {10.1109/ICMEW.2019.00074},
file = {:Users/jakobabeer/Downloads/08794892.pdf:pdf},
isbn = {9781538692141},
keywords = {Acoustic scene classification,DCASE 2018,Deep scattering spectrum,Multi-level attention mechanism,acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {396--401},
title = {{Multi-Level Attention Model with Deep Scattering Spectrum for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Liao2013,
abstract = {YouTube is a highly visited video sharing website where over one billion people watch six billion hours of video every month. Improving accessibility to these videos for the hearing impaired and for search and indexing purposes is an excellent application of automatic speech recognition. However, YouTube videos are extremely challenging for automatic speech recognition systems. Standard adapted Gaussian Mixture Model (GMM) based acoustic models can have word error rates above 50{\%}, making this one of the most difficult reported tasks. Since 2009, YouTube has provided automatic generation of closed captions for videos detected to have English speech; the service now supports ten different languages. This paper describes recent improvements to the original system, in particular the use of owner-uploaded video transcripts to generate additional semi-supervised training data and deep neural networks acoustic models with large state inventories. Applying an island of confidence filtering heuristic to select useful training segments, and increasing the model size by using 44,526 context dependent states with a low-rank final layer weight matrix approximation, improved performance by about 13{\%} relative compared to previously reported sequence trained DNN results for this task.},
author = {Liao, Hank and McDermott, Erik and Senior, Andrew},
booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding},
doi = {10.1109/ASRU.2013.6707758},
file = {::},
isbn = {978-1-4799-2756-2},
keywords = {Large vocabulary speech recognition,audio indexing,deep learning,deep neural networks,semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {dec},
pages = {368--373},
publisher = {IEEE},
title = {{Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription}},
url = {http://ieeexplore.ieee.org/document/6707758/},
year = {2013}
}
@article{Lin:2019:SED:DCASE,
author = {Lin, Liwei and Wang, Xiangdong},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Lin{\_}25.pdf:pdf},
pages = {1--5},
title = {{Guided Learning Convolution System for DCASE 2019 Task 4}},
year = {2019}
}
@article{London:2019:BeatTempo:MP,
author = {London, Justin and Burger, Birgitta and Thompson, Marc and Hildreth, Molly and Wilson, Johanna and Schally, Nick and Toiviainen, Petri},
file = {:Users/jakobabeer/Downloads/London{\_}et{\_}al{\_}MP2019.pdf:pdf},
journal = {Music Perception},
keywords = {absolute ver-,after only a few,ambiguous,beat salience,ever,keen sense of whether,notes or drum,perceptual sharpening,rhythm,strokes we have a,sus relative tempo judgment,tempo,the},
number = {1},
pages = {26--41},
title = {{Motown, Disco, and Drumming: An Exploration of the Relationship between Beat Salience, Melodic Structure, and Perceived Tempo}},
volume = {37},
year = {2019}
}
@article{Lostanlen:2018:PCEN:SPL,
abstract = {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named per-channel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This letter investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Second, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Third, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise, PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
author = {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
doi = {10.1109/LSP.2018.2878620},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Lostanlen et al. - 2019 - Per-channel energy normalization Why and how(2).pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Acoustic noise,acoustic sensors,acoustic signal detection,machine{\_}listening,signal classification,spectrogram},
mendeley-tags = {machine{\_}listening},
number = {1},
pages = {39--43},
title = {{Per-channel energy normalization: Why and how}},
volume = {26},
year = {2019}
}
@book{Lostanlen:2019:EventDetection:PLOS,
abstract = {Bioacoustic sensors, sometimes known as autonomous recording units (ARUs), can record sounds of wildlife over long periods of time in scalable and minimally invasive ways. Deriving per-species abundance estimates from these sensors requires detection, classification, and quantification of animal vocalizations as individual acoustic events. Yet, variability in ambient noise, both over time and across sensors, hinders the reliability of current automated systems for sound event detection (SED), such as convolutional neural networks (CNN) in the time-frequency domain. In this article, we develop, benchmark, and combine several machine listening techniques to improve the generalizability of SED models across heterogeneous acoustic environments. As a case study, we consider the problem of detecting avian flight calls from a ten-hour recording of nocturnal bird migration, recorded by a network of six ARUs in the presence of heterogeneous background noise. Starting from a CNN yielding state-of-the-art accuracy on this task, we introduce two noise adaptation techniques, respectively integrating short-term (60-millisecond) and long-term (30-minute) context. First, we apply per-channel energy normalization (PCEN) in the time-frequency domain, which applies short-term automatic gain control to every subband in the mel-frequency spectrogram. Secondly, we replace the last dense layer in the network by a context-adaptive neural network (CA-NN) layer, i.e. an affine layer whose weights are dynamically adapted at prediction time by an auxiliary network taking long-term summary statistics of spectrotemporal features as input. We show that both techniques are helpful and complementary. [...] We release a pre-trained version of our best performing system under the name of BirdVoxDetect, a ready-to-use detector of avian flight calls in field recordings.},
archivePrefix = {arXiv},
arxivId = {1905.08352},
author = {Lostanlen, Vincent and Salamon, Justin and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
doi = {10.1371/journal.pone.0214168},
eprint = {1905.08352},
file = {:Users/jakobabeer/Downloads/journal.pone.0214168.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
pages = {1--31},
title = {{Robust sound event detection in bioacoustic sensor networks}},
url = {http://arxiv.org/abs/1905.08352},
year = {2019}
}
@article{Lucic:2019:HighFidelityImageGeneration:ARXIV,
abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform state-of-the-art (SOTA) on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the art conditional model BigGAN on ImageNet using only 10{\%} of the labels and outperform it using 20{\%} of the labels.},
archivePrefix = {arXiv},
arxivId = {1903.02271},
author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
eprint = {1903.02271},
file = {:Users/jakobabeer/Dropbox/{\_}LESEN/1903.02271.pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {generative{\_}adversarial{\_}network},
mendeley-tags = {generative{\_}adversarial{\_}network},
month = {mar},
title = {{High-Fidelity Image Generation With Fewer Labels}},
url = {http://arxiv.org/abs/1903.02271},
year = {2019}
}
@article{Luo:2018:ConvTasNet:ARXIV,
abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.},
archivePrefix = {arXiv},
arxivId = {1809.07454},
author = {Luo, Yi and Mesgarani, Nima},
doi = {10.1109/TASLP.2019.2915167},
eprint = {1809.07454},
file = {:Users/jakobabeer/Downloads/1809.07454v3.pdf:pdf},
journal = {arXiv},
keywords = {speaker{\_}separation},
mendeley-tags = {speaker{\_}separation},
pages = {1--12},
title = {{Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation}},
url = {http://arxiv.org/abs/1809.07454{\%}0Ahttp://dx.doi.org/10.1109/TASLP.2019.2915167},
year = {2018}
}
@article{Luo:2019:TimbrePitchDisentanglement:ARXIV,
abstract = {In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model efficacy by latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high accuracy when tested on our synthesized sounds, which verifies the model performance of controllable realistic timbre and pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single autoencoder architecture, which is evaluated by measuring the shift in posterior of instrument classification. Our in depth evaluation confirms the model ability to successfully disentangle timbre and pitch.},
archivePrefix = {arXiv},
arxivId = {1906.08152},
author = {Luo, Yin-Jyun and Agres, Kat and Herremans, Dorien},
eprint = {1906.08152},
file = {:Users/jakobabeer/Downloads/1906.08152.pdf:pdf},
title = {{Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders}},
url = {http://arxiv.org/abs/1906.08152},
year = {2019}
}
@book{Mueller:2015:MusicProcessing:BOOK,
author = {M{\"{u}}ller, Meinard},
isbn = {978-3-319-21944-8},
publisher = {Springer},
title = {{Fundamentals of Music Processing}},
year = {2015}
}
@inproceedings{Mueller:2019:MusicTutorial:ICASSP,
address = {Brighton, UK},
author = {M{\"{u}}ller, Meinard and Arzt, Andreas and Balke, Stefan},
booktitle = {Tutorial at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Dropbox/ICASSP{\_}2019/conference/T7{\_}Notes.pdf:pdf},
isbn = {0780370414},
title = {{Cross-Modal Music Retrieval and Applications}},
year = {2019}
}

@InProceedings{Mafra:2016:CompactASC:DCASE,
  author =        {Mafra, Gustavo Sena and Duong, Ngoc Q. K. and Ozerov, Alexey and P{\'{e}}rez, Patrick},
  title =         {{Acoustic Scene Classification: An Evaluation of an Extremely Compact Feature Representations}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  file =          {:Users/jakobabeer/Downloads/SenaMafra-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Maka:2018:FeatureSpaceASC:DCASE,
address = {Surrey, UK},
author = {Maka, Tomasz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Maka - 2018 - Audio Feature Space Analysis for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Audio Feature Space Analysis for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Maksimovic:2019:AESForensics,
author = {Maksimovi{\'{c}}, Milica and Cuccovillo, Luca and Aichroth, Patrick},
booktitle = {AES International Conference on Audio Forensics},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Copy-Move Forgery Detection and Localization via Partial Audio Matching}},
year = {2019}
}
@article{Manilow2019,
abstract = {We present a single deep learning architecture that can both separate an audio recording of a musical mixture into constituent single-instrument recordings and transcribe these instruments into a human-readable format at the same time, learning a shared musical representation for both tasks. This novel architecture, which we call Cerberus, builds on the Chimera network for source separation by adding a third "head" for transcription. By training each head with different losses, we are able to jointly learn how to separate and transcribe up to 5 instruments in our experiments with a single network. We show that the two tasks are highly complementary with one another and when learned jointly, lead to Cerberus networks that are better at both separation and transcription and generalize better to unseen mixtures.},
archivePrefix = {arXiv},
arxivId = {1910.12621},
author = {Manilow, Ethan and Seetharaman, Prem and Pardo, Bryan},
eprint = {1910.12621},
file = {:Users/jakobabeer/Downloads/1910.12621.pdf:pdf},
title = {{Simultaneous Separation and Transcription of Mixtures with Multiple Polyphonic and Percussive Instruments}},
url = {http://arxiv.org/abs/1910.12621},
year = {2019}
}

@InProceedings{Marchi:2016:MKSL:DCASE,
  author =        {Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
  title =         {{Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  abstract =      {We propose a system for acoustic scene classification using pair-wise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contri-bution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the 'one-against-all' paradigm to perform a pair-wise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4{\%} is achieved.},
  file =          {:Users/jakobabeer/Downloads/Marchi-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Mariotti:2018:DeepVisionASC:DCASE,
address = {Surrey, UK},
author = {Mariotti, Octave and Cord, Matthieu and Schwander, Olivier},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mariotti, Cord, Schwander - 2018 - Exploring Deep Vision Models for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Exploring Deep Vision Models for Acoustic Scene Classification}},
year = {2018}
}
@article{Marolt:2004:AdaptiveOscillators:JNMR,
abstract = {In this paper, we present a technique for tracking partials in musical signals, based on networks of adaptive oscillators. We show how synchronization of adaptive oscillators can be utilized to detect periodic patterns in outputs of a human auditory model and thus track stable frequency components (partials) in musical signals. The model is further extended to track groups of harmonically related partials by grouping oscillators into networks. We present the integration of the partial tracking model into a system for transcription of polyphonic piano music. The transcription system is based on a connectionist architecture that employs networks of adaptive oscillators for partial tracking and feed forward neural networks for associating partial groups with notes. We provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings.},
author = {Marolt, Matija},
doi = {10.1076/jnmr.33.1.49.35391},
file = {:Users/jakobabeer/Downloads/2aa5c27fbf1859749be124445a68dcb1f29a.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {mar},
number = {1},
pages = {49--59},
title = {{Networks of Adaptive Oscillators for Partial Tracking and Transcription of Music Recordings}},
volume = {33},
year = {2004}
}
@article{Marolt:2004:ConnectionistTranscription:IEEE_TOM,
abstract = {In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.},
author = {Marolt, Matija},
doi = {10.1109/TMM.2004.827507},
file = {::},
journal = {IEEE Transactions on Multimedia},
keywords = {Adaptive oscillators,Music transcription,Neural networks},
month = {jun},
number = {3},
pages = {439--449},
title = {{A Connectionist Approach to Automatic Transcription of Polyphonic Piano Music}},
volume = {6},
year = {2004}
}
@inproceedings{Mars:2019:BinauralASC:DCASE,
address = {New York, NY, USA},
author = {Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chongsoon},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
doi = {10.33682/6c9z-gd15},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mars et al. - 2019 - Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {149--153},
title = {{Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks}},
year = {2019}
}
@inproceedings{Mauch:2014:PYINPitchTracker:ICASSP,
abstract = {We propose the Probabilistic YIN (PYIN) algorithm, a modification of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online1 as an open source C++ library for Vamp hosts.},
address = {Florence, Italy},
author = {Mauch, Matthias and Dixon, Simon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2014.6853678},
file = {::},
keywords = {Pitch estimation,YIN,pitch tracking,pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
number = {1},
pages = {659--663},
title = {{PYIN: A fundamental frequency estimator using probabilistic threshold distributions}},
year = {2014}
}
@article{McAulay:1986:SpeechAnalysisSynthesis:IEEE_TASSP,
abstract = {A sinusoidal model for the speech waveform is used to develop a new analysis/synthesis technique that is characterized by the amplitudes, frequencies, and phases of the component sine waves. These parameters are estimated from the short-time Fourier ...},
author = {McAulay, Robert J. and Quatieri, Thomas F.},
doi = {10.1109/TASSP.1986.1164910},
file = {::},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
number = {4},
pages = {744--754},
title = {{Speech Analysis/Synthesis Based on a Sinusoidal Representation}},
volume = {34},
year = {1986}
}
@inproceedings{McCallum:2019:Segmentation:ICASSP,
address = {Brighton, UK},
author = {McCallum, Matthew C.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683407},
file = {:Users/jakobabeer/Downloads/08683407.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {segmentation},
mendeley-tags = {segmentation},
month = {may},
pages = {346--350},
publisher = {IEEE},
title = {{Unsupervised Learning of Deep Features for Music Segmentation}},
url = {https://ieeexplore.ieee.org/document/8683407/},
year = {2019}
}
@inproceedings{Mcdonnell:2019:AcousticScenes:DCASE,
abstract = {This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output.},
address = {New York, NY, USA},
author = {Mcdonnell, Mark D and Gao, Wei},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acou,machine{\_}listening},
mendeley-tags = {acou,machine{\_}listening},
title = {{Acoustic Scene Classification Using Deep Residual Networks With Late Fusion of Separated High and Low Frequency Paths}},
url = {https://github.com/McDonnell-Lab/DCASE2019-Task1},
year = {2019}
}
@article{Meddis:1986:SimulationNeuralTransduction:ASOC,
abstract = {A probabilistic model is described for transmitter release from hair cells, auditory neuron EPSP's, and discharge patterns. The model assumes that the release fraction of the transmitter is a function of stimulus intensity. It further assumes that some of this transmitter substance is taken back into the cell while some is irretrievably lost from the cleft. These assumptions differ from other recent models which propose multiple release sites, fixed release fractions, and no transmitter reuptake. The model produces realistic mammalian rate intensity functions, interval and period histograms, incremental responses, and adaptation effects. It mimics successfully the adaptation of successive EPSP amplitudes of the afferent neuron of the goldfish sacculus and offers a reinterpretation of the implications of these studies for hair cell synaptic mechanism.},
author = {Meddis, Ray},
file = {::},
journal = {The Journal of the Acoustical Society of America},
pages = {702--11},
title = {{Simulation of mechanical to neural transduction in the auditory receptor}},
year = {1986}
}
@article{Melih,
author = {Melih, Ahmet and Bu, B A },
file = {:Users/jakobabeer/Downloads/08806301.pdf:pdf},
isbn = {9781509064946},
keywords = {00,17,2019 ieee,31,978-1-5090-6494-6,acoustic scene classification,convolutional neural,gated recurrent units,long short term memory,network},
pages = {2--5},
title = {{Akustik Sahne S  n  fland  rma i{\c{c}}in Derin {\"{O}}  renme Modellerinin Analizi Analysis of Deep Neural Network Models for Acoustic Scene Classification}}
}
@article{Mesaros:2016:ASC:IEEE_TASLP,
abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events DCASE 2016 has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: Acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
author = {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
doi = {10.1109/TASLP.2017.2778423},
file = {:Users/jakobabeer/Downloads/08123864.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Acoustic scene classification,audio datasets,pattern recognition,sound event detection},
number = {2},
pages = {379--393},
publisher = {IEEE},
title = {{Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge}},
volume = {26},
year = {2018}
}
@inproceedings{Mesaros:2018:MultiDeviceDataset:DCASE,
address = {Surrey, UK},
author = {Mesaros, Annemaria and Heittola, Toni and {Tuomas Virtanen}},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Heittola, Tuomas Virtanen - 2018 - A Multi-Device Dataset for Urban Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{A Multi-Device Dataset for Urban Acoustic Scene Classification}},
year = {2018}
}

@InProceedings{Mesaros:2017:HumanASC:WASPAA,
  author =    {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title =     {{Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study}},
  booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year =      {2017},
  pages =     {319--323},
  address =   {New Paltz, NY, USA},
  month =     {October}
}
@inproceedings{Mesaros:2019:ClosedOpenSet:DCASE,
address = {New York, NY, USA},
author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
doi = {10.33682/m5kp-fa97},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {164--168},
title = {{Acoustic Scene Classification in DCASE 2019 Challenge:Closed and Open Set Classification and Data Mismatch Setups}},
year = {2019}
}
@article{Mimilakis:2020:Denoising:TASLP,
author = {Mimilakis, Stylianos Ioannis and Drosos, Konstantinos and Cano, Estefan{\'{i}}a and Schuller, Gerald},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Examining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation}},
volume = {28},
year = {2020}
}
@inproceedings{Mimilakis:2019:CrossVersion:MML,
author = {Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and Arifi-M{\"{u}}ller, Vlora and Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 12th International Workshop on Machine Learning and Music (MML)},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Cross-Version Singing Voice Detection in Opera Recordings: Challenges for Supervised Learning}},
year = {2019}
}

@InProceedings{Mimilakis2019,
  author =        {Mimilakis, Stylianos I. and Wei{\ss}, Christof and Arifi-M{\"{u}}ller, Vlora and Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
  title =         {{Cross-Version Singing Voice Detection in Opera Recordings : Challenges for Supervised Learning}},
  booktitle =     {Proceedings of the 12th International Workshop on Machine Learning and Music (MML)},
  year =          {2019},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Mimilakis{\_}2019{\_}MWAM.pdf:pdf},
  keywords =      {idmt,opera,singing voice detection,supervised deep learning},
  mendeley-tags = {idmt}
}
@inproceedings{Mora-Angel:2019:ACMUS:DLFM,
address = {Delft, The Netherlands},
author = {Mora-{\'{A}}ngel, Fernando and Gil, Gustavo A L{\'{o}}pez and Cano, Estefan{\'{i}}a and Grollmisch, Sascha},
booktitle = {Proceedings of the Digital Libraries for Musicology (DLfM)},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{ACMUS-MIR: A new annotated data set of Andean Colombian music}},
year = {2019}
}

@InProceedings{Moritz:2016:TDNN:DCASE,
  author =        {Moritz, Niko and Schr{\"{o}}der, Jens and Goetze, Stefan and Anem{\"{u}}ller, J{\"{o}}rn and Kollmeier, Birger},
  title =         {{Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  abstract =      {This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual infor-mation similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 76.5 {\%}, which is 4.0 {\%} higher compared to the DCASE'16 baseline system.},
  file =          {:Users/jakobabeer/Downloads/Moritz-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2017:GANASC:DCASE,
  author =        {Mun, Seongkyu and Park, Sangwook and Han, David K. and Ko, Hanseok},
  title =         {{Generative Adversarial Networks based Acoustic Scene Training Set Augmentation and Selection using SVM Hyperplane}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Mun{\_}215.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Mun:2019:DomainMismatch:ICASSP,
abstract = {In recent acoustic scene classification (ASC) research field, training and test device channel mismatch have become an issue for the real world implementation. To address the issue, this paper proposes a channel domain conversion using factor-ized hierarchical variational autoencoder. Proposed method adapts both the source and target domain to a pre-defined specific domain. Unlike the conventional approach, the relationship between the target and source domain and information of each domain are not required in the adaptation process. Based on the experimental results using the IEEE Detection and Classification of Acoustic Scenes and Event 2018 task 1-B dataset and the baseline system, it is shown that the proposed approach can mitigate the channel mismatching issue of different recording devices.},
address = {Brighton, UK},
author = {Mun, Seongkyu and Shon, Suwon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683514},
file = {:Users/jakobabeer/Downloads/08683514.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {acoustic scene classification,acoustic{\_}scene{\_}classification,domain adaptation,factorized hierarchical variational autoencoder,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {845--849},
title = {{Domain Mismatch Robust Acoustic Scene Classification Using Channel Information Conversion}},
year = {2019}
}
@inproceedings{Mun:2017:ASC:ICASSP,
abstract = {Deep Neural Network (DNN) based transfer learning has been shown to be effective in Visual Object Classification (VOC) for complementing the deficit of target domain training samples by adapting classifiers that have been pre- trained for other large-scaled DataBase (DB). Although there exists an abundance of acoustic data, it can also be said that datasets of specific acoustic scenes are sparse for training Acoustic Scene Classification (ASC) models. By exploiting VOC DNNs ability of learning beyond its pre- trained environments, this paper proposes DNN based transfer learning for ASC. Effectiveness of the proposed method is demonstrated on the database of IEEE DCASE Challenge 2016 Task 1 and home surveillance environment via representative experiments. Its improved performance is verified by comparing it to prominent conventional methods.},
address = {New Orleans, LA, USA},
author = {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Han, David K. and Ko, Hanseok},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1097/IOP.0000000000000348},
file = {:Users/jakobabeer/Downloads/07952265.pdf:pdf},
isbn = {9781509041176},
issn = {15372677},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {796--800},
title = {{Deep Neural Network Based Learning and Transferring Mid-Level Audio Features for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Nadar:2019:ChordRecognitionCNN:SMC,
address = {M{\'{a}}laga, Spain},
author = {Nadar, Christon-Ragavan and Abe{\ss}er, Jakob and Grollmisch, Sascha},
booktitle = {Proceedings of the Sound {\&} Music Computing Conference (SMC)},
file = {::},
keywords = {chord{\_}recognition,idmt,m2d},
mendeley-tags = {chord{\_}recognition,idmt,m2d},
title = {{Towards CNN-based Acoustic Modeling of Seventh Chords for Recognition Chord Recognition}},
year = {2019}
}
@inproceedings{Neri:2018:PartialTrackingLinearProgramming:DAFX,
abstract = {This paper proposes a new partial tracking method, based on linear programming, that can run in real-time, is simple to implement , and performs well in difficult tracking situations by considering spurious peaks, crossing partials, and a non-stationary short-term sinusoidal model. Complex constant parameters of a generalized short-term signal model are explicitly estimated to inform peak matching decisions. Peak matching is formulated as a variation of the linear assignment problem. Combinatorially optimal peak-to-peak assignments are found in polynomial time using the Hungarian algorithm. Results show that the proposed method creates high-quality representations of monophonic and polyphonic sounds.},
address = {Aveiro, Portugal},
author = {Neri, Julian and Depalle, Philippe},
booktitle = {Proceedings of the 21st International Conference on Digital Audio Effects (DAFx-18)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Neri, Depalle - 2018 - Fast Partial Tracking of Audio with Real-Time Capability Through Linear Programming.pdf:pdf},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
title = {{Fast Partial Tracking of Audio with Real-Time Capability Through Linear Programming}},
year = {2018}
}
@inproceedings{Nguyen:2018:ASCEnsemble:DCASE,
address = {Surrey, UK},
author = {Nguyen, Truc and Pernkopf, Franz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Pernkopf - 2018 - Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters}},
year = {2018}
}
@inproceedings{Nwe:2017:MultiTaskASC:APSIPA,
abstract = {Deep Neural Network (DNN) with Multi-Task Learning (MTL) methods have recently demonstrated significant performance gains on a number of classification, detection, recognition tasks compared to conventional DNN. DNN with MTL framework involves cross-task and within-task knowledge sharing layers. MTL methods have benefit for regularization effect from the cross-task knowledge sharing layers. And, within- task knowledge sharing layers allow MTL based DNN to learn information to optimize the performance for individual task. We formulate our acoustic scene classification in MTL framework using Convolutional Neural Network to learn information specific to different types of environment. We conduct experiments using DCASE2016 dataset. Proposed approach achieves 83.8{\%} accuracy to classify 15 acoustic scene classes.},
address = {Malaysia},
author = {Nwe, Tin Lay and Dat, Tran Huy and Ma, Bin},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
doi = {10.1109/APSIPA.2017.8282241},
file = {:Users/jakobabeer/Downloads/08282241.pdf:pdf},
isbn = {9781538615423},
pages = {1347--1350},
title = {{Convolutional Neural Network with Multi-Task Learning Scheme for Acoustic Scene Classification}},
year = {2018}
}
@article{Okamoto:2019:EnvironmentalSoundSynthesis:ARXIV,
abstract = {Synthesizing and converting environmental sounds have the potential for many applications such as supporting movie and game production, data augmentation for sound event detection and scene classification. Conventional works on synthesizing and converting environmental sounds are based on a physical modeling or concatenative approach. However, there are a limited number of works that have addressed environmental sound synthesis and conversion with statistical generative models; thus, this research area is not yet well organized. In this paper, we review problem definitions, applications, and evaluation methods of environmental sound synthesis and conversion. We then report on environmental sound synthesis using sound event labels, in which we focus on the current performance of statistical environmental sound synthesis and investigate how we should conduct subjective experiments on environmental sound synthesis.},
archivePrefix = {arXiv},
arxivId = {1908.10055},
author = {Okamoto, Yuki and Imoto, Keisuke and Komatsu, Tatsuya and Takamichi, Shinnosuke and Yagyu, Takumi and Yamanishi, Ryosuke and Yamashita, Yoichi},
eprint = {1908.10055},
file = {:Users/jakobabeer/Downloads/1908.10055.pdf:pdf},
title = {{Overview of Tasks and Investigation of Subjective Evaluation Methods in Environmental Sound Synthesis and Conversion}},
url = {http://arxiv.org/abs/1908.10055},
year = {2019}
}
@article{Oord:2016:Wavenet:ARXIV,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:Users/jakobabeer/Downloads/1609.03499.pdf:pdf},
pages = {1--15},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@inproceedings{Perez-Lopez:2019:LocalizationDetectionDCASE,
abstract = {This work describes and discusses an algorithm submitted to the Sound Event Localization and Detection Task of DCASE2019 Challenge. The proposed methodology relies on parametric spatial audio analysis for source localization and detection, combined with a deep learning-based monophonic event classifier. The evaluation of the proposed algorithm yields overall results comparable to the baseline system. The main highlight is a reduction of the localization error on the evaluation dataset by a factor of 2.6, compared with the baseline performance.},
archivePrefix = {arXiv},
arxivId = {1908.10133},
author = {P{\'{e}}rez-L{\'{o}}pez, Andr{\'{e}}s and Fonseca, Eduardo and Serra, Xavier},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
eprint = {1908.10133},
file = {:Users/jakobabeer/Downloads/1908.10133.pdf:pdf},
number = {October},
title = {{A hybrid parametric-deep learning approach for sound event localization and detection}},
year = {2019}
}
@article{Pan2016,
abstract = {With the popularization of the Internet, permeation of sensor networks, emergence of big data, increase in size of the information community, and interlinking and fusion of data and information throughout human society, physical space, and cyberspace, the information environment related to the current development of artificial intelligence (AI) has profoundly changed. AI faces important adjustments, and scientific foundations are confronted with new breakthroughs, as AI enters a new stage: AI 2.0. This paper briefly reviews the 60-year developmental history of AI, analyzes the external environment promoting the formation of AI 2.0 along with changes in goals, and describes both the beginning of the technology and the core idea behind AI 2.0 development. Furthermore, based on combined social demands and the information environment that exists in relation to Chinese development, suggestions on the development of AI 2.0 are given.},
author = {Pan, Yunhe},
doi = {10.1016/J.ENG.2016.04.018},
file = {::},
issn = {20958099},
journal = {Engineering},
keywords = {Artificial intelligence 2.0,Autonomous-intelligent system,Big data,Cross-media,Crowd intelligence,Human-machine hybrid-augmented intelligence},
number = {4},
pages = {409--413},
publisher = {Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company},
title = {{Heading toward Artificial Intelligence 2.0}},
url = {http://dx.doi.org/10.1016/J.ENG.2016.04.018},
volume = {2},
year = {2016}
}
@inproceedings{Pantelli:2017:F0Contours:ICASSP,
author = {Panteli, Maria and Bittner, Rachel and Bello, Juan Pablo and Dixon, Simon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7952233},
file = {:Users/jakobabeer/Downloads/PanteliBittnerBelloDixon-ICASSP.pdf:pdf},
isbn = {978-1-5090-4117-6},
keywords = {melody{\_}contour{\_}analysis},
mendeley-tags = {melody{\_}contour{\_}analysis},
month = {mar},
pages = {636--640},
publisher = {IEEE},
title = {{Towards the characterization of singing styles in world music}},
url = {http://ieeexplore.ieee.org/document/7952233/},
year = {2017}
}
@article{Parisi:2019:ContinualLearning:NN,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.07569v4},
author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012},
eprint = {arXiv:1802.07569v4},
file = {:Users/jakobabeer/Downloads/1802.07569.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
pages = {54--71},
title = {{Continual lifelong learning with neural networks: A review}},
volume = {113},
year = {2019}
}
@article{Park:2019:SpecAugment:INTERSPEECH,
abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
archivePrefix = {arXiv},
arxivId = {1904.08779},
author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
doi = {10.21437/Interspeech.2019-2680},
eprint = {1904.08779},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2019 - Specaugment A simple data augmentation method for automatic speech recognition.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Data augmentation,End-to-end speech recognition},
pages = {2613--2617},
title = {{Specaugment: A simple data augmentation method for automatic speech recognition}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{Park:2017:Melody:ICASSP,
address = {New Orleans, USA},
author = {Park, Hyunsin and Yoo, Chang D.},
booktitle = {Proceedings of the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Park, Yoo - 2017 - Melody Extraction and Detection through LSTM-RNN with Harmonic Sum Loss.pdf:pdf},
pages = {2766--2770},
title = {{Melody Extraction and Detection through LSTM-RNN with Harmonic Sum Loss}},
year = {2017}
}
@article{Park:2019:Transformer:ARXIV,
abstract = {Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.},
archivePrefix = {arXiv},
arxivId = {1907.02698},
author = {Park, Jonggwon and Choi, Kyoyun and Jeon, Sungwook and Kim, Dokyun and Park, Jonghun},
eprint = {1907.02698},
file = {:Users/jakobabeer/Dropbox/{\_}LESEN/a-bi-directional-transformer-for-musical-chord-recognition.pdf:pdf},
journal = {ArXiV},
title = {{A Bi-directional Transformer for Musical Chord Recognition}},
url = {http://arxiv.org/abs/1907.02698},
year = {2019}
}

@InProceedings{Park:2017:DoubleImageASC:DCASE,
  author =        {Park, Sangwook and Mun, Seonkyu and Lee, Younglo and Ko, Hanseok},
  title =         {{Acoustic Scene Classification Based on Convolutional Neural Network using Double Image Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Park{\_}214.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Paseddula:2018:ASC:ICIIS,
abstract = {Herein, we propose an Acoustic Scene Classification (ASC) based on Deep Neural Networks (DNN). The design of Mel-filer bank helps in capturing the acoustic scene characteristics in the low-frequency regions during MFCC extraction. In this paper, inverse MFCC are used as interdependent to structure of Mel filter bank. We can effectively capture the acoustic information in the total audio frequency range using MFCC and IMFCC features. An experiment is carried on Tampere University of Technology (TUT) Acoustic Scenes 2017 Dataset. DNN architecture at utterance level classification with supervised learning is adopted. Scores from the DNN models corresponding to MFCC and IMFCC features are combined for testing the model. The relative improvement of 5.22{\%} wih respect to baseline system is achieved by the proposed system on setup of 4-fold cross-validation. We participated in the DCASE 2017 challenge for ASC task, also got 45.9{\%} accuracy on given evaluation dataset. This approach got 76th rank out of 97 submissions.},
address = {Ropar, India},
author = {Paseddula, Chandrasekhar and Gangashetty, Suryakanth V.},
booktitle = {Proceedings of the 13th IEEE International Conference on Industrial and Information Systems (ICIIS)},
doi = {10.1109/ICIINFS.2018.8721379},
file = {:Users/jakobabeer/Downloads/08721379.pdf:pdf},
isbn = {9781538616765},
keywords = {Acoustic Scene Classification,Adaptive Moments,Deep Neural Network,Discrete Cosine Transform,Inverse MFCC,MFCC,Multilayer Perceptron Model,acoustic{\_}event{\_}detection,machine{\_}listening},
mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening},
pages = {18--21},
title = {{DNN based Acoustic Scene Classification using Score Fusion of MFCC and Inverse MFCC}},
year = {2018}
}

@InProceedings{Paseddula2018,
  author =        {Paseddula, Chandrasekhar and Gangashetty, Suryakanth V.},
  title =         {{Input Fusion of MFCC and SCMC Features for Acoustic Scene Classification using DNN}},
  booktitle =     {Proceedings of the 13th International Conference on Industrial and Information Systems (ICIIS)},
  year =          {2018},
  pages =         {13--17},
  abstract =      {In this paper, we propose a feature set by concatenating Mel-Frequency Cepstral Coefficients (MFCC) and Spectral Centroid Magnitude Coefficients (SCMC) features for Acoustic Scene Classification (ASC) using Deep Neural Networks (DNN). MFCC features are used to hold the acoustic characteristics such as spectral envelope of an acoustic scene in each frame. It also carries the sub-band average energy as a single dimension. SCMC features are used to hold the distribution of energy in a sub-band effectively. A test is carried out on Tampere University of Technology (TUT) Acoustic Scenes 2017 Dataset. The DNN architecture for utterance level classification has been used. The proposed system's performance on a 4-fold cross-validation setup is 80.2{\%} and it gives 5.4{\%} relative improvement in performance when compared to the baseline system that uses log-Mel band energies with Multi-Layer Perceptron model.},
  doi =           {10.1109/ICIINFS.2018.8721416},
  file =          {:Users/jakobabeer/Downloads/08721416.pdf:pdf},
  isbn =          {9781538616765},
  keywords =      {Acoustic Scene Classification,Adaptive Moments,Deep Neural Network,Discrete Cosine Transform,Mel-Frequency Cepstral Coefficients,Multilayer Perceptron Model,Spectral Centroid Magnitude Coefficients,acoust,machine{\_}listening},
  mendeley-tags = {acoust,machine{\_}listening}
}
@article{Pham:2019:AcousticScene:INTERSPEECH,
abstract = {Acoustic scene classification (ASC) using front-end time-frequency features and back-end neural network classifiers has demonstrated good performance in recent years. However a profusion of systems has arisen to suit different tasks and datasets, utilising different feature and classifier types. This paper aims at a robust framework that can explore and utilise a range of different time-frequency features and neural networks, either singly or merged, to achieve good classification performance. In particular, we exploit three different types of front-end time-frequency feature; log energy Mel filter, Gammatone filter and constant Q transform. At the back-end we evaluate effective a two-stage model that exploits a Convolutional Neural Network for pre-trained feature extraction, followed by Deep Neural Network classifiers as a post-trained feature adaptation model and classifier. We also explore the use of a data augmentation technique for these features that effectively generates a variety of intermediate data, reinforcing model learning abilities, particularly for marginal cases. We assess performance on the DCASE2016 dataset, demonstrating good classification accuracies exceeding 90{\%}, significantly outperforming the DCASE2016 baseline and highly competitive compared to state-of-the-art systems.},
author = {Pham, Lam and McLoughlin, Ian and Phan, Huy and Palaniappan, Ramaswamy},
doi = {10.21437/Interspeech.2019-1841},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Acoustic scene classification,Constant Q transform,Convolutional neural network,Deep neural network,Gammatone filter,Log-Mel,Machine hearing,Spectrogram,acoustic{\_}scene{\_}classification},
mendeley-tags = {acoustic{\_}scene{\_}classification},
number = {September},
pages = {3634--3638},
title = {{A robust framework for acoustic scene classification}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{Phan:2019:EventDetection:ICASSP,
address = {Brighton, UK},
author = {Phan, Huy and Chen, Oliver Y. and Koch, Philipp and Pham, Lam and McLoughlin, Ian and Mertins, Alfred and Vos, Maarten De},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683064},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Phan et al. - 2019 - Unifying Isolated and Overlapping Audio Event Detection with Multi-label Multi-task Convolutional Recurrent Neural.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {51--55},
publisher = {IEEE},
title = {{Unifying Isolated and Overlapping Audio Event Detection with Multi-label Multi-task Convolutional Recurrent Neural Networks}},
url = {https://ieeexplore.ieee.org/document/8683064/},
year = {2019}
}
@inproceedings{Phaye:2019:Subspectralnet:ICASSP,
address = {Brighton, UK},
author = {Phaye, Sai Samarth R and Benetos, Emmanouil and Wang, Ye},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Downloads/08683288 (1).pdf:pdf},
isbn = {9781538646588},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {825--829},
title = {{Subspectralnet - Using Sub-Spectrogram based Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2019}
}

@InProceedings{Piczak:2017:ASC:DCASE,
  author =        {Piczak, Karol J.},
  title =         {{The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  abstract =      {This study describes a convolutional neural network model submit-ted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with dif-ferent frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolu-tional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration.},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Piczak{\_}210.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
  url =           {http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge{\_}technical{\_}reports/DCASE2017{\_}Piczak{\_}208.pdf}
}
@inproceedings{Pielemeier:1996:TimeFreqAnalysisMusicalSignals:IEEE,
abstract = {The major time and frequency analysis methods that have been applied to music processing are traced and application areas described. Techniques are examined in the context of Cohen's class, facilitating comparison and the design of new approaches. A trumpet example illustrates most techniques. The impact of different analysis methods on pitch and timbre examination is shown. Analyses spanning Fourier series and transform, pitch synchronous analysis, heterodyne jilter, short-time Fourier transform (STFT), phase vocoder, constant-{\&} and wavelet transforms, the Wigner distribution, and the modal distribution are all covered. The limitations of windowing methods and their reliance on steady-state assumptions and infnite duration sinusoids to define frequency and amplitude are detailed. The Wigner distribution, in contrast, uses the analytic signal to define instantaneous frequency and power parameters. The modal distribution is shown to be a linear transformation of the Wigner distribution optimized for estimating those parameters for a musical signal model. Application areas consider analysis, resynthesis, transcription, and visualization. The more stringent requirements for time-frequency (TF) distributions in these applications are compared with the weaker requirements found in speech analysis and highlight the need for further theoretical research.},
author = {Pielemeier, William J and Wakefield, Gregory H and Simoni, Mary H},
booktitle = {Proceedings of the IEEE},
file = {::},
isbn = {00189219/96{\$}05.0},
title = {{Time-frequency analysis of musical signals}},
year = {1996}
}
@inproceedings{Pons:2018:LargeScaleTagging:ISMIR,
abstract = {The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models - using waveforms as input with very small convolutional filters; and models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios.},
address = {Paris},
author = {Pons, Jordi and Nieto, Oriol and Prockup, Matthew and Schmidt, Erik and Ehmann, Andreas and Serra, Xavier},
booktitle = {Proceedings of the International Conference on Music Information Retrieval},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Pons - Unknown - End-to-end learning for music audio tagging at scale.pdf:pdf},
keywords = {tagging},
mendeley-tags = {tagging},
pages = {637--644},
title = {{End-to-end learning for music audio tagging at scale}},
year = {2018}
}
@inproceedings{Pons:2019:FewData:ICASSP,
address = {Brighton, UK},
author = {Pons, Jordi and Serra, Joan and Serra, Xavier},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682591},
file = {:Users/jakobabeer/Downloads/08682591.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {16--20},
title = {{Training Neural Audio Classifiers with Few Data}},
url = {https://ieeexplore.ieee.org/document/8682591/},
year = {2019}
}
@inproceedings{Poria2013,
abstract = {Music genres can be seen as categorical descriptions used to classify music basing on various characteristics such as instrumentation, pitch, rhythmic structure, and harmonic contents. Automatic music genre classification is important for music retrieval in large music collections on the web. We build a classifier that learns from very few labeled examples plus a large quantity of unlabeled data, and show that our methodology outperforms existing supervised and unsupervised approaches. We also identify salient features useful for music genre classification. We achieve 97.1{\%} accuracy of 10-way classification on real-world audio collections.},
address = {Berlin, Heidelberg},
annote = {- 10 genres from western music
- add large amount of unlabeled data
- use short-time, long-time and beat features
- short: MFCC, spectral centroid, roll-off, flux, root mean square, compactness, domain zero crossing
- long: aggreate short time -{\textgreater} derivate, std dev, running mean, derivative of running mean
- beat: beat histogram, bpm, beat sum, strongest beat
- extracted with Jaudio toolkit (Java)
- fuzzy training classifier for mapping 10 class problem to 2-way or 3-way problem
- first extract features -{\textgreater} different combinations tested
- fuzzy clustering: ignore labled and create clusters according to number of classes, fuzzy means one point can be assigned to more than one label
- Mapping: identify classes, majority voting performed for each cluster by using labeled data
- hard clustering: remove fuzziness and assign each item to a class
- 97{\%} accuracy with 10 fold cv
- spectral centroid and mfcc most important (short and long term)},
author = {Poria, Soujanya and Gelbukh, Alexander and Hussain, Amir and Bandyopadhyay, Sivaji and Howard, Newton},
booktitle = {Carrasco-Ochoa J.A., Mart{\'{i}}nez-Trinidad J.F., Rodr{\'{i}}guez J.S., di Baja G.S. (eds) Pattern Recognition. MCPR 2013. Lecture Notes in Computer Science},
doi = {10.1007/978-3-642-38989-4_26},
file = {::},
isbn = {978-3-642-38989-4},
issn = {03029743},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
pages = {254--263},
publisher = {Springer Berlin Heidelberg},
title = {{Music Genre Classification: A Semi-supervised Approach}},
url = {http://link.springer.com/10.1007/978-3-642-38989-4{\_}26},
volume = {7914 LNCS},
year = {2013}
}
@inproceedings{Prakruthi:2018:ASC:ICISC,
abstract = {Acoustic Scene Classification (ASC) has become an integral component in applications such as smart hearing AIDS, user alert applications for physically challenged persons and robot based navigation. The Deep Learning (DL) techniques such as Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) etc. improve the accuracy and efficiency of ASC but increase computational complexity. This paper has explored the possibility of using neural network as classifiers for ASC. The observations show that an appropriately trained simple neural network can achieve similar performance as DL techniques. The developed model utilizes Mel Frequency Cepstral Coefficients (MFCC) for feature extraction and has been verified on TUT Acoustic Scenes 2016 dataset. The developed model has attained 14.3{\%} better accuracy than existing DL models for frame based analysis.},
author = {Prakruthi, U. S. and Kiran, Divya and Ramasangu, Hariharan},
booktitle = {Proceedings of the 2nd International Conference on Inventive Systems and Control (ICISC)},
doi = {10.1109/ICISC.2018.8398905},
file = {:Users/jakobabeer/Downloads/08398905.pdf:pdf},
isbn = {9781538608074},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {781--784},
publisher = {IEEE},
title = {{High Performance Neural Network based Acoustic Scene Classification}},
year = {2018}
}
@article{Purwins:2019:DeepLearningASP:IEEE_JSTSP,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.00078v1},
author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schl{\"{u}}ter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
doi = {10.1109/JSTSP.2019.2908700},
eprint = {arXiv:1905.00078v1},
file = {:Users/jakobabeer/Dropbox/{\_}LESEN/1905.00078.pdf:pdf},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {deep{\_}learning},
mendeley-tags = {deep{\_}learning},
number = {8},
pages = {1--14},
title = {{Deep Learning for Audio Signal Processing}},
url = {https://ieeexplore.ieee.org/document/8678825/},
volume = {14},
year = {2019}
}
@article{Qandour2014,
abstract = {Recent developments in Wireless Sensor Networks (WSNs) have led to their use in remote data acquisition and automatic data analysis applications, which have proven to be an invaluable tool in a diverse range of fields including biosecurity. Further indications have been found that honeybee health can be monitored and determined through the use of acoustic analysis. In this paper, we present a system that has the ability to remotely detect the presence of pest infestation on a colony of honeybees by comparing the acoustic fingerprint of a hive to a fingerprint of known status. This will aid the goals of increasing surveillance programs by reducing the labour time and costs that are associated with managing and maintaining monitoring programs. Other benefits of the system proposed in this article include the ability to make available a collection of deterministic, standardised and nondiscriminatory statistical data for the purpose of research into determining the causes of colony collapse disorder.},
author = {Qandour, Amro and Ahmad, Iftekhar and Habibi, Daryoush and Leppard, Mark},
issn = {08146039},
journal = {Acoustics Australia},
keywords = {bee monitoring},
mendeley-tags = {bee monitoring},
number = {3},
pages = {204--209},
title = {{Remote beehive monitoring using acoustic signals}},
volume = {42},
year = {2014}
}

@InProceedings{Qian:2017:WaveletASC:DCASE,
  author =        {Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Wavelets Revisited for the Classification of Acoustic Scenes}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Qian{\_}132.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@article{Qiao:2019:SoundClassification:ARXIV,
abstract = {Environmental Sound Classification (ESC) is an important and challenging problem, and feature representation is a critical and even decisive factor in ESC. Feature representation ability directly affects the accuracy of sound classification. Therefore, the ESC performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. In this paper, we propose a subspectrogram segmentation based ESC classification framework. In addition, we adopt the proposed Convolutional Recurrent Neural Network (CRNN) and score level fusion to jointly improve the classification accuracy. Extensive truncation schemes are evaluated to find the optimal number and the corresponding band ranges of sub-spectrograms. Based on the numerical experiments, the proposed framework can achieve 81.9{\%} ESC classification accuracy on the public dataset ESC-50, which provides 9.1{\%} accuracy improvement over traditional baseline schemes.},
archivePrefix = {arXiv},
arxivId = {1908.05863},
author = {Qiao, Tianhao and Zhang, Shunqing and Zhang, Zhichao and Cao, Shan and Xu, Shugong},
eprint = {1908.05863},
file = {:Users/jakobabeer/Downloads/1908.05863.pdf:pdf},
title = {{Sub-Spectrogram Segmentation for Environmental Sound Classification via Convolutional Recurrent Neural Network and Score Level Fusion}},
url = {http://arxiv.org/abs/1908.05863},
year = {2019}
}
@inproceedings{Rafii:2012:REPET:ISMIR,
abstract = {Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. {\textcopyright} 2012 International Society for Music Information Retrieval.},
address = {Porto, Portugal},
author = {Rafii, Zafar and Pardo, Bryan},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0740488f2e930fa1b0a0ec546b876cf00ecb.pdf:pdf},
isbn = {9789727521449},
pages = {583--588},
title = {{Music/Voice Separation using the Similarity Matrix}},
year = {2012}
}
@article{Rasmus:2015:LadderNetworks,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02672v2},
author = {Rasmus, Antti and Valpola, Harri and Berglund, Mathias},
eprint = {arXiv:1507.02672v2},
file = {:Users/jakobabeer/Downloads/1507.02672.pdf:pdf},
title = {{Semi-Supervised Learning with Ladder Networks}},
year = {2015}
}
@inproceedings{Raspaud:2008:BinauralPartialTracking:DAFX,
abstract = {Partial tracking in sinusoidal models have been studied for over twenty years now, and have been enhanced, making it precise and useful to analyse noiseless harmonic sounds. However, such tools have always been used in a monophonic (single channel) context. A method is thus proposed to adapt the partial tracking to the case of binaural signals. This gives a tool to perform spectral analysis of such signals, keeping relevant information from both left and right channels. Moreover, azimuth (position in the horizontal plane) information for each partial is gained using interaural cues, such as interaural time differences (ITDs) and interaural level differences (ILDs). The azimuth information can then be used as an attribute or as a constraint in the binaural partial tracking algorithm. Finally, some classification results using the azimuth of partials are presented.},
address = {Espoo, Finland},
author = {Raspaud, Martin and Evangelista, Gianpaolo},
booktitle = {Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)},
isbn = {9789512295173},
title = {{Binaural partial tracking}},
year = {2008}
}
@inproceedings{Ren:2019:AttrousCNNAttention:ICASSP,
author = {Ren, Zhao and Kong, Qiuqiang and Han, Jing and Plumbley, Mark D and Schuller, Bjorn W.},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683434},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2019 - Attention-based Atrous Convolutional Neural Networks Visualisation and Understanding Perspectives of Acoustic Scenes.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {56--60},
title = {{Attention-based Atrous Convolutional Neural Networks: Visualisation and Understanding Perspectives of Acoustic Scenes}},
url = {https://ieeexplore.ieee.org/document/8683434/},
year = {2019}
}
@inproceedings{Ren:2018:AttentionASC:DCASE,
address = {Surrey, UK},
author = {Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark D. and Schuller, Bj{\"{o}}rn W.},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2018 - Attention-based Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Attention-based Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2018}
}

@InProceedings{Ren:2017:DeepSequentialASC:DCASE,
  author =        {Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Deep Sequential Image Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Ren{\_}133.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Rigaud:2016:Transcription:ISMIR,
abstract = {This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f 0 es-timation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The perfor-mance of the melody f 0 estimation system is compared with a state-of-the-art method and exhibits highest accu-racy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody tran-scription is presented.},
address = {New York, NY, USA},
author = {Rigaud, Francois and Radenen, Mathieu},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Desktop/journal refs/7.pdf:pdf},
pages = {737--743},
title = {{Singing Voice Melody Transcription using Deep Neural Networks}},
year = {2016}
}
@article{Robel:2006:AdapAddModelingContTraj:IEEE_TASLP,
author = {Robel, Axel},
doi = {10.1109/tsa.2005.858529},
file = {::},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
month = {jun},
number = {4},
pages = {1440--1453},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Adaptive Additive Modeling with Continuous Parameter Trajectories}},
volume = {14},
year = {2006}
}
@article{Roberts2018,
abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint = {1803.05428},
title = {{A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music}},
year = {2018}
}
@inproceedings{Roletscheck:2019:EvolutionaryASC:DCASE,
address = {New York, NY, USA},
author = {Roletscheck, Christian and Watzka, Tobias and Seiderer, Andreas and Schiller, Dominik and Andr{\'{e}}, Elisabeth},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Roletscheck et al. - 2019 - Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2019}
}
@article{Rolnick2019,
abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
archivePrefix = {arXiv},
arxivId = {1906.05433},
author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
eprint = {1906.05433},
file = {::},
title = {{Tackling Climate Change with Machine Learning}},
url = {http://arxiv.org/abs/1906.05433},
year = {2019}
}
@article{Rouditchenko2019,
abstract = {Segmenting objects in images and separating sound sources in audio are challenging tasks, in part because traditional approaches require large amounts of labeled data. In this paper we develop a neural network model for visual object segmentation and sound source separation that learns from natural videos through self-supervision. The model is an extension of recently proposed work that maps image pixels to sounds [1]. Here, we introduce a learning approach to disentangle concepts in the neural networks, and assign semantic categories to network feature channels to enable independent image segmentation and sound source separation after audio-visual training on videos. Our evaluations show that the disentangled model outperforms several baselines in semantic segmentation and sound source separation.},
archivePrefix = {arXiv},
arxivId = {1904.09013},
author = {Rouditchenko, Andrew and Zhao, Hang and Gan, Chuang and McDermott, Josh and Torralba, Antonio},
doi = {10.1109/ICASSP.2019.8682467},
eprint = {1904.09013},
file = {::},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {audio-visual,co-segmentation,disentangled,self-supervised,source separation},
pages = {2357--2361},
title = {{Self-supervised Audio-visual Co-segmentation}},
volume = {2019-May},
year = {2019}
}
@article{Ruder:2017:MultitaskLearning:ARXIV,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1706.05098.pdf:pdf},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{Russakovsky:2015:ImageNet:IJCV,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/10.1.1.876.2726.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Rusu:2016:ProgressiveNN:ARXIV,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {:Users/jakobabeer/Downloads/1606.04671.pdf:pdf},
journal = {ArXiv pre-prints},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@article{Ryynaenen:2008:Transcription:CMJ,
author = {Ryyn{\"{a}}nen, Matti P. and Klapuri, Anssi P.},
file = {:Users/jakobabeer/Desktop/journal refs/Ryynaenen{\_}2008{\_}CMJ - Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music Kopie.pdf:pdf},
journal = {Computer Music Journal},
keywords = {bass{\_}transcription,melody{\_}transcription},
mendeley-tags = {bass{\_}transcription,melody{\_}transcription},
number = {3},
pages = {72--86},
title = {{Automatic Transcription of Melody , Bass Line , and Chords in Polyphonic Music}},
volume = {32},
year = {2008}
}
@article{Sajjadi2016,
abstract = {In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.},
archivePrefix = {arXiv},
arxivId = {1606.03141},
author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
eprint = {1606.03141},
file = {::},
journal = {Ieee Icip},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {jun},
title = {{Mutual Exclusivity Loss for Semi-Supervised Deep Learning}},
url = {http://arxiv.org/abs/1606.03141},
year = {2016}
}
@inproceedings{Saki:2019:OpenSetASC:DCASE,
address = {New York, NY, USA},
author = {Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Saki, Guo, Hung - 2019 - Open-Set Evolving Acoustic Scene Classification System.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {219--223},
title = {{Open-Set Evolving Acoustic Scene Classification System}},
year = {2019}
}
@article{Salamon:2017:ASC:SPL,
abstract = {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a 'shallow' dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
archivePrefix = {arXiv},
arxivId = {1608.04363},
author = {Salamon, Justin and Bello, Juan Pablo},
doi = {10.1109/LSP.2017.2657381},
eprint = {1608.04363},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1608.04363.pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset},
number = {3},
pages = {279--283},
title = {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
volume = {24},
year = {2017}
}
@inproceedings{Salamon:2017:AnalysisSynthesis:ISMIR,
address = {Suzhou, China},
author = {Salamon, Justin and Bittner, Rachel M. and Bonada, Jordi and Bosch, Juan J. and G{\'{o}}mez, Emilia and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/jakobabeer/Desktop/journal refs/10.pdf:pdf},
keywords = {f0{\_}tracking,synthesis},
mendeley-tags = {f0{\_}tracking,synthesis},
pages = {71--78},
title = {{An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets}},
year = {2017}
}
@article{Salamon:2012:Melodia:IEEE_TASLP,
author = {Salamon, Justin and G{\'{o}}mez, Emilia},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Salamon, G{\'{o}}mez - 2012 - Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {music transcription,pitch{\_}tracking},
mendeley-tags = {music transcription,pitch{\_}tracking},
number = {6},
pages = {1759--1770},
title = {{Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics}},
volume = {20},
year = {2012}
}
@inproceedings{Sandler:2018:MobileNet:CVPR,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00474},
eprint = {1801.04381},
file = {::},
isbn = {9781538664209},
issn = {10636919},
pages = {4510--4520},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
year = {2018}
}
@inproceedings{Satar:2005:PartialTrackingMusic:DAFX,
abstract = {In this paper we propose a novel approach for tracking of partials in music signals based on a robust Kalman filter. Our tracker is based on a regularized least-squares approach that is designed to minimize the worst-possible regularized residual norm over the class of admissible uncertainties at each iteration. We introduce a set of state-space models for our signals based on the evolution of frequency and amplitude in different classes of musical instruments. These prior models are used to estimate future values of partial tracks in successive time frames of our spectral data. Here, the parameters of evolution models are treated as bounded uncertainties and our tracker can robustly track partials in all frequency regions. Unlike the conventional Kalman tracker, performance of this tracker is not influenced by the magnified track variations in higher frequencies. This tracker promises an improved performance over conventional Kalman tracker while preserving its good properties and superiority over existing methodologies.},
address = {Madrid, Spain},
author = {Satar-Boroujeni, Hamid and Shafai, Bahram},
booktitle = {Proceedings of the 8th International Conference on Digital Audio Effects (DAFX-05)},
file = {::},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
title = {{A Robust Algorithm for Partial Tracking of Music Signals}},
year = {2005}
}
@article{Schoneich2015,
abstract = {From human language to birdsong and the chirps of insects, acoustic communication is based on amplitude and frequency modulation of sound signals. Whereas frequency processing starts at the level of the hearing organs, temporal features of the sound amplitude such as rhythms or pulse rates require processing by central auditory neurons. Besides several theoretical concepts, brain circuits that detect temporal features of a sound signal are poorly understood. We focused on acoustically communicating field crickets and show how five neurons in the brain of females form an auditory feature detector circuit for the pulse pattern of the male calling song. The processing is based on a coincidence detector mechanism that selectively responds when a direct neural response and an intrinsically delayed response to the sound pulses coincide. This circuit provides the basis for auditory mate recognition in field crickets and reveals a principal mechanism of sensory processing underlying the perception of temporal patterns.},
author = {Sch{\"{o}}neich, Stefan and Kostarakos, Konstantinos and Hedwig, Berthold},
doi = {10.1126/sciadv.1500325},
file = {::},
journal = {Science Advances},
keywords = {KISH,auditory feature detection},
mendeley-tags = {KISH,auditory feature detection},
number = {8},
pages = {e1500325},
title = {{An auditory feature detection circuit for sound pattern recognition}},
volume = {1},
year = {2015}
}
@inproceedings{Schreiber:2018:TempoEstimation:ISMIR,
abstract = {We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional mel-spectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system's CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances.},
address = {Paris, France},
author = {Schreiber, Hendrik and M{\"{u}}ller, Meinard},
booktitle = {ISMIR, International Society for Music Information Retrieval Conference},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Schreiber, M{\"{u}}ller - 2018 - A Single-Step Approach to Musical Tempo Estimation using a Convolutional Neural Network.pdf:pdf},
pages = {98--105},
title = {{a Single-Step Approach To Musical Tempo Estimation Using a Convolutional Neural Network}},
year = {2018}
}
@inproceedings{Schroff:2015:FaceNet:CVPR,
address = {Boston, MA, USA},
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Schroff, Philbin - Unknown - FaceNet A Unified Embedding for Face Recognition and Clustering.pdf:pdf},
pages = {815--823},
publisher = {{\{}IEEE{\}} Computer Society},
title = {{FaceNet: {\{}A{\}} unified embedding for face recognition and clustering}},
year = {2015}
}
@inproceedings{Seo:2019:ASC:DCASE,
annote = {Ensemble approach, computationally too expensive},
author = {Seo, Hyeji and Park, Jihwan and Park, Yongjin},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Seo{\_}72.pdf:pdf},
pages = {3--6},
title = {{Acoustic Scene Classification using Various Pre-Processed Features and Convolutional Neural Networks}},
year = {2019}
}
@article{Serra:1990:SpectralModelingSynthesis:CMJ,
author = {{Serra Xavier}, Julius Smith},
file = {::},
journal = {Computer Music Journal},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
number = {4},
pages = {12--24},
title = {{Spectral Modeling Synthesis: A Sound Analysis/Synthesis Based on a Deterministic plus Stochastic Decomposition}},
volume = {14},
year = {1990}
}
@incollection{Serra:1997:MusicalSoundModeling:MSP,
author = {Serra, Xavier},
booktitle = {Musical Signal Processing},
editor = {Roads, C. and Pope, S. and Picialli, A. and {De Poli}, G.},
file = {::},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
pages = {91--122},
publisher = {Swets {\&} Zeitlinger},
title = {{Musical Sound Modeling with Sinusoids plus Noise}},
year = {1997}
}
@article{Sharma:2019:SoundClassification:ARXIV,
abstract = {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN). The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. Also, we employ a deeper CNN (DCNN) compared to previous models, consisting of 2D separable convolutions working on time and feature domain separately. The model also consists of max pooling layers that downsample time and feature domain separately. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (98.60{\%}), ESC-10 (97.25{\%}) and ESC-50 (95.50{\%}). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets and by a considerable margin over the previous models. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7{\%} and 81.3{\%} respectively.},
archivePrefix = {arXiv},
arxivId = {1908.11219},
author = {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
eprint = {1908.11219},
file = {:Users/jakobabeer/Downloads/1908.11219.pdf:pdf},
journal = {ArXiv pre-prints},
number = {8},
pages = {1--11},
title = {{Environment Sound Classification using Multiple Feature Channels and Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1908.11219},
volume = {14},
year = {2019}
}
@article{Shi:2019:SED:DCASE,
annote = {Ensemble of 3 semisupervised learning algorithms

CRNN as core model
},
archivePrefix = {arXiv},
arxivId = {1903.03825},
author = {Shi, Ziqiang},
eprint = {1903.03825},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Shi{\_}11.pdf:pdf},
pages = {9--10},
title = {{Hodgepodge: Sound Event Detection based on Ensemble of Semi-Supervised Learning Methods}},
url = {http://arxiv.org/abs/1903.03825},
year = {2019}
}
@inproceedings{Shibata:2019:JointTranscription:ICASSP,
address = {Brighton, UK},
author = {Shibata, Kentaro and Nishikimi, Ryo and Fukayama, Satoru and Goto, Masataka and Nakamura, Eita and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682817},
file = {:Users/jakobabeer/Dropbox/ICASSP{\_}2019/to read/08682817.pdf:pdf},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {236--240},
publisher = {IEEE},
title = {{Joint Transcription of Lead, Bass, and Rhythm Guitars Based on a Factorial Hidden Semi-markov Model}},
url = {https://ieeexplore.ieee.org/document/8682817/},
year = {2019}
}
@article{Sigtia2016,
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network-based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yield the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic music transcription,Deep learning,Music language models,Recurrent neural networks},
number = {5},
pages = {927--939},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
volume = {24},
year = {2016}
}
@article{Sigtia:2016:PerformanceCost:IEEE_TASLP,
abstract = {In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.},
author = {Sigtia, Siddharth and Stark, Adam M. and Krstulovi{\'{c}}, Sacha and Plumbley, Mark D.},
doi = {10.1109/TASLP.2016.2592698},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic environmental sound recognition,computational auditory scene analysis,deep learning,machine learning,machine{\_}listening},
mendeley-tags = {machine{\_}listening},
number = {11},
pages = {2096--2107},
publisher = {IEEE},
title = {{Automatic Environmental Sound Recognition: Performance Versus Computational Cost}},
volume = {24},
year = {2016}
}
@inproceedings{Singh:2019:MultiViewFeatures:DCASE,
address = {New Paltz, NY, USA},
author = {Singh, Arshdeep and Rajan, Padmanabhan and Bhavsar, Arnav},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Rajan, Bhavsar - 2019 - Deep Multi-View Features from Raw Audio for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {229--233},
title = {{Deep Multi-View Features from Raw Audio for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Singh:2018:EnsembleASC:EUSIPCO,
abstract = {Scene classification based on acoustic information is a challenging task due to various factors such as the non-stationary nature of the environment and multiple overlapping acoustic events. In this paper, we address the acoustic scene classification problem using SoundNet, a deep convolution neural network, pre-trained on raw audio signals. We propose a classification strategy by combining scores from each layer. This is based on the hypothesis that layers of the deep convolutional network learn complementary information and combining this layer-wise information provides better classification than the features extracted from an individual layer. In addition, we also propose a pooling strategy to reduce the dimensionality of features extracted from different layers of SoundNet. Our experiments on DCASE 2016 acoustic scene classification dataset reveals the effectiveness of this layer-wise ensemble approach. The proposed approach provides a relative improvement of approx. 30.85{\%} over the classification accuracy provided by the best individual layer of SoundNet.},
address = {Rome, Italy},
author = {Singh, Arshdeep and Thakur, Anshul and Rajan, Padmanabhan and Bhavsar, Arnav},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
doi = {10.23919/EUSIPCO.2018.8553052},
file = {:Users/jakobabeer/Downloads/08553052.pdf:pdf},
isbn = {9789082797015},
issn = {22195491},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {837--841},
title = {{A Layer-Wise Score Level Ensemble Framework for Acoustic Scene Detection}},
year = {2018}
}
@techreport{Srinivasamurthy,
abstract = {Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music.},
author = {Srinivasamurthy, Ajay and Holzapfel, Andre and {Taylan Cemgil}, Ali and Serra, Xavier},
file = {::},
title = {{PARTICLE FILTERS FOR EFFICIENT METER TRACKING WITH DYNAMIC BAYESIAN NETWORKS}}
}
@article{Srivastava:2014:Dropout:JMLR,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:Users/jakobabeer/Downloads/srivastava14a.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{Sterian:1998:PartialTracking:SPIE,
abstract = {We present a new method for musical partial tracking in the context of musical transcription using a time-frequency Kalman filter structure. The filter is based upon a model for the evolution of a partial in amplitude and frequency. The parameters of this model are determined from a statistical analysis of partial behavior across a wide range of pitch from four brass instruments. Statistics are computed independently for the partial attributes of frequency and log-power first differences. We present observed power spectral density shapes, total powers, and histograms, as well as least-squares approximations to these. We demonstrate that a Kalman filter tracker using this partial model is capable of tracking partials in music. We discuss how the filter structure naturally provides quality-of-fit information about the data for use in further processing and how this information can be used to perform partial track initiation and termination within a common framework. We propose that a model-based approach to partial tracking is preferable to existing approaches which generally use heuristic rules or birth/death notions over a small time neighborhood. The advantages include better performance in the presence of cluttered data (e.g., multi-voice material) and simplified tracking over missed observations.},
address = {San Diego, California},
annote = {abr:
- use Kalman filter},
author = {Sterian, Andrew and Wakefield, Gregory H.},
booktitle = {Proceedings of the Annual Meeting and International Symposium on Optical Science, Engineering and Instrumentation (SPIE)},
doi = {10.1117/12.325677},
file = {:Users/jakobabeer/Downloads/10.1.1.94.657.pdf:pdf},
keywords = {kalman filter,music analysis,musical transcription,partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {oct},
title = {{Model-based approach to partial tracking for musical transcription}},
year = {1998}
}
@inproceedings{Stoller:2019:LyricsAudioAlignment:ICASSP,
abstract = {Time-aligned lyrics can enrich the music listening experience by enabling karaoke, text-based song retrieval and intra-song navigation, and other applications. Compared to text-to-speech alignment, lyrics alignment remains highly challenging, despite many attempts to combine numerous sub-modules including vocal separation and detection in an effort to break down the problem. Furthermore, training required fine-grained annotations to be available in some form. Here, we present a novel system based on a modified Wave-U-Net architecture, which predicts character probabilities directly from raw audio using learnt multi-scale representations of the various signal components. There are no sub-modules whose interdependencies need to be optimized. Our training procedure is designed to work with weak, line-level annotations available in the real world. With a mean alignment error of 0.35s on a standard dataset our system outperforms the state-of-the-art by an order of magnitude.},
address = {Brighton, UK},
archivePrefix = {arXiv},
arxivId = {1902.06797},
author = {Stoller, Daniel and Durand, Simon and Ewert, Sebastian},
booktitle = {Proceedings of the IEEE Inter},
doi = {10.1109/ICASSP.2019.8683470},
eprint = {1902.06797},
file = {::},
isbn = {9781479981311},
issn = {15206149},
keywords = {CTC training,Lyrics alignment,lyrics,lyrics transcription,multi-scale representation,neural networks},
mendeley-tags = {lyrics},
pages = {181--185},
title = {{End-to-end Lyrics Alignment for Polyphonic Music Using an Audio-to-character Recognition Model}},
year = {2019}
}
@article{Stoller:2018:WaveUNet:ISMIR,
abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
archivePrefix = {arXiv},
arxivId = {1806.03185},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
eprint = {1806.03185},
file = {:Users/jakobabeer/Downloads/1806.03185.pdf:pdf},
title = {{Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation}},
url = {http://arxiv.org/abs/1806.03185},
year = {2018}
}
@inproceedings{Sumi:2008:ChordBass:ISMIR,
address = {Philadelphia, PA, USA},
author = {Sumi, Kouhei and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Komatani, Kazunori and Ogata, Tetsuya and Okuno, Hiroshi G.},
booktitle = {Proceedings of the 9th International Conference of Music Information Retrieval (ISMIR)},
file = {:Users/jakobabeer/Downloads/Automatic{\_}Chord{\_}Recognition{\_}Based{\_}on{\_}Probabilistic.pdf:pdf},
keywords = {bass{\_}transcription,chord{\_}transcription},
mendeley-tags = {bass{\_}transcription,chord{\_}transcription},
pages = {39--44},
title = {{Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation}},
year = {2008}
}
@inproceedings{Taenzer:2019:Instrument:ISMIR,
address = {Delft, The Netherlands},
author = {Taenzer, Michael and Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and M{\"{u}}ller, Meinard and Lukashevich, Hanna},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Investigating CNN-based Instrument Family Recognition for Western Classical Music Recordings}},
year = {2019}
}
@article{Taenzer:2019:InvestigatingInstFamRecognition:ISMIR,
abstract = {Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation.},
author = {Taenzer, Michael and Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and M{\"{u}}ller, Meinard and Lukashevich, Hanna},
file = {::},
keywords = {instrument{\_}recognition,isad,md},
mendeley-tags = {instrument{\_}recognition,isad,md},
title = {{Investigating CNN-Based Instrument Family Recognition for Western Classical Music Recordings}},
year = {2019}
}
@book{Tag2019,
author = {Tag, King},
keywords = {acmus,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,audio{\_}matching,audio{\_}tagging,bass{\_}transcription,beat{\_}tracking,chord{\_}recognition,deep{\_}learning,few{\_}shot{\_}learning,generative{\_}adversarial{\_}network,hashing,idmt,instrument{\_}recognition,isad,m2d,machine{\_}learning,machine{\_}listening,melody{\_}contour{\_}analysis,melody{\_}transcription,meta{\_}learning,model{\_}investigation,multipitch{\_}estimation,music{\_}generation,music{\_}transcription,partial{\_}tracking,pitch{\_}tracking,rhythm{\_}analysis,semi{\_}supervised{\_}learning,siamese{\_}networks,source{\_}separation,speech{\_}analysis{\_}synthesis,speech{\_}recognition,tempo{\_}estimation,time{\_}series{\_}analysis,transfer{\_}learning,unsupervised{\_}learning,weakly{\_}labeled},
mendeley-tags = {acmus,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,audio{\_}matching,audio{\_}tagging,bass{\_}transcription,beat{\_}tracking,chord{\_}recognition,deep{\_}learning,few{\_}shot{\_}learning,generative{\_}adversarial{\_}network,hashing,idmt,instrument{\_}recognition,isad,m2d,machine{\_}learning,machine{\_}listening,melody{\_}contour{\_}analysis,melody{\_}transcription,meta{\_}learning,model{\_}investigation,multipitch{\_}estimation,music{\_}generation,music{\_}transcription,partial{\_}tracking,pitch{\_}tracking,rhythm{\_}analysis,semi{\_}supervised{\_}learning,siamese{\_}networks,source{\_}separation,speech{\_}analysis{\_}synthesis,speech{\_}recognition,tempo{\_}estimation,time{\_}series{\_}analysis,transfer{\_}learning,unsupervised{\_}learning,weakly{\_}labeled},
title = {{THE BOOK OF TAGS}},
year = {2019}
}
@article{Tagliasacchi2019,
abstract = {We explore self-supervised models that can be potentially deployed on mobile devices to learn general purpose audio representations. Specifically, we propose methods that exploit the temporal context in the spectrogram domain. One method estimates the temporal gap between two short audio segments extracted at random from the same audio clip. The other methods are inspired by Word2Vec, a popular technique used to learn word embeddings, and aim at reconstructing a temporal spectrogram slice from past and future slices or, alternatively, at reconstructing the context of surrounding slices from the current slice. We focus our evaluation on small encoder architectures, which can be potentially run on mobile devices during both inference (re-using a common learned representation across multiple downstream tasks) and training (capturing the true data distribution without compromising users' privacy when combined with federated learning). We evaluate the quality of the embeddings produced by the self-supervised learning models, and show that they can be re-used for a variety of downstream tasks, and for some tasks even approach the performance of fully supervised models of similar size.},
archivePrefix = {arXiv},
arxivId = {1905.11796},
author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, F{\'{e}}lix de Chaumont and Roblek, Dominik},
eprint = {1905.11796},
file = {::},
title = {{Self-supervised audio representation learning for mobile devices}},
url = {http://arxiv.org/abs/1905.11796},
year = {2019}
}
@inproceedings{Taigman:2014:DeepFace:CVPR,
address = {Columbus, OH, USA},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc' Aurelio and Wolf, Lior},
booktitle = {Proceeedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Taigman et al. - Unknown - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
keywords = {siamese{\_}networks},
mendeley-tags = {siamese{\_}networks},
pages = {1701--1708},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@inproceedings{Takahashi:2017:ASC:APSIPA,
abstract = {We previously proposed a method of acoustic scene classification using a deep neural network-Gaussian mixture model (DNN-GMM) and frame-concatenated acoustic features. It was submitted to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 Challenge and was ranked eighth among 49 algorithms. In the proposed method, acoustic features in temporally distant frames were concatenated to capture their temporal relationship. The experimental results indicated that the classification accuracy is improved by increasing the number of concatenated frames. On the other hand, the frame concatenation interval, which is the interval with which the frames used for frame concatenation are selected, is another important parameter. In our previous method, the frame concatenation interval was fixed to 100 ms. In this paper, we optimize the number of concatenated frames and the frame concatenation interval for the previously proposed method. As a result, it was confirmed that the classification accuracy of the method was improved by 2.61{\%} in comparison with the result submitted to the DCASE 2016.},
author = {Takahashi, Gen and Yamada, Takeshi and Ono, Nobutaka and Makino, Shoji},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
doi = {10.1109/APSIPA.2017.8282314},
file = {:Users/jakobabeer/Downloads/08282314.pdf:pdf},
isbn = {9781538615423},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {1739--1743},
title = {{Performance Evaluation of Acoustic Scene Classification using DNN-GMM and Frame-Concatenated Acoustic Features}},
year = {2018}
}
@inproceedings{Tan:2019:TCNN:ICASSP,
address = {Brighton, UK},
author = {Tan, Ke and Wang, Deliang},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Downloads/TCNN{\_}ICASSP{\_}2019.pdf:pdf},
isbn = {9781538646588},
keywords = {time{\_}series{\_}analysis},
mendeley-tags = {time{\_}series{\_}analysis},
pages = {6865--6869},
title = {{TCNN: Temporal Convolutional Neural Network for Real-Time Speech Enhancement in the Time Domain}},
year = {2019}
}
@inproceedings{Tan:2019:EfficientNet:ICML,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
address = {Long Beach, CA, USA},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1905.11946},
file = {::},
isbn = {9781510886988},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@article{Thakur:2019:DeepMetric:JASA,
abstract = {This paper proposes multiscale convolutional neural network (CNN)-based deep metric learning for bioacoustic classification, under low training data conditions. The proposed CNN is characterized by the utilization of four different filter sizes at each level to analyze input feature maps. This multiscale nature helps in describing different bioacoustic events effectively: smaller filters help in learning the finer details of bioacoustic events, whereas, larger filters help in analyzing a larger context leading to global details. A dynamic triplet loss is employed in the proposed CNN architecture to learn a transformation from the input space to the embedding space, where classification is performed. The triplet loss helps in learning this transformation by analyzing three examples, referred to as triplets, at a time where intra-class distance is minimized while maximizing the inter-class separation by a dynamically increasing margin. The number of possible triplets increases cubically with the dataset size, making triplet loss more suitable than the softmax cross-entropy loss in low training data conditions. Experiments on three different publicly available datasets show that the proposed framework performs better than existing bioacoustic classification frameworks. Experimental results also confirm the superiority of the triplet loss over the cross-entropy loss in low training data conditions},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10713v2},
author = {Thakur, Anshul and Thapar, Daksh and Rajan, Padmanabhan and Nigam, Aditya},
doi = {10.1121/1.5118245},
eprint = {arXiv:1903.10713v2},
file = {:Users/jakobabeer/Dropbox/{\_}LESEN/2019{\_}09{\_}22 5d4151984585153e5930236c.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {534--547},
title = {{Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss}},
volume = {146},
year = {2019}
}
@article{Thickstun2018,
abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
doi = {10.1109/ICASSP.2018.8461686},
file = {::},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural networks,Invariances,Learning,Music information retrieval},
pages = {2241--2245},
publisher = {IEEE},
title = {{Invariances and Data Augmentation for Supervised Music Transcription}},
volume = {2018-April},
year = {2018}
}
@inproceedings{Turpault2019,
abstract = {Deep neural networks are particularly useful to learn relevant representations from data. Recent studies have demonstrated the potential of unsupervised representation learning for ambient sound analysis using various flavors of the triplet loss. They have compared this approach to supervised learning. However, in real situations, it is common to have a small labeled dataset and a large unlabeled one. In this paper, we combine unsupervised and supervised triplet loss based learning into a semi-supervised representation learning approach. We propose two flavors of this approach, whereby the positive samples for those triplets whose anchors are unlabeled are obtained either by applying a transformation to the anchor, or by selecting the nearest sample in the training set. We compare our approach to supervised and unsupervised representation learning as well as the ratio between the amount of labeled and unlabeled data. We evaluate all the above approaches on an audio tagging task using the DCASE 2018 Task 4 dataset, and we show the impact of this ratio on the tagging performance.},
address = {Brighton, UK},
annote = {- ambient sound analysis
- combine unsupervised and supervised triplet loss to semi-supervised triplet loss
- positive example takne from unlabeled
- for example by taking nearest sample 
- tested on DCASE2018 task 4 audio tagging
- compared to super und unsupervised versions
- two strategies for taking postive: transform acnhor or taking nearest sample in input space -{\textgreater} input space closeness should lead to embedding closeness -{\textgreater} transformation is better strategy, see results
- semi-supervised better than unsupservised
- transform better than nearset
- un{\"{o}}abeled only helpful up to a certain point
- semi-supervsied outperforms supervised baseline
- loss using different examples, not same data but unsupervised statistics -{\textgreater} npt part of classification loss, just for embedding},
author = {Turpault, Nicolas and Serizel, Romain and Vincent, Emmanuel},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683774},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {may},
pages = {760--764},
publisher = {IEEE},
title = {{Semi-supervised Triplet Loss Based Learning of Ambient Audio Embeddings}},
url = {https://ieeexplore.ieee.org/document/8683774/},
year = {2019}
}

@InProceedings{Vafeiadis:2017:HybridASC:DCASE,
  author =        {Vafeiadis, Anastasios and Kalatzis, Dimitrios and Votis, Konstantinos and Giakoumis, Dimitrios and Tzovaras, Dimitrios and Chen, Liming and {Raouf Hamzaoui}},
  title =         {{Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  doi =           {10.1109/TMM.2015.2428998},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Vafeiadis{\_}135.pdf:pdf},
  issn =          {15209210},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Valenti:2016:ASC:DCASE,
  author =    {Valenti, Michele and Diment, Aleksandr and Parascandolo, Giambattista and Squartini, Stefano and Virtanen, Tuomas},
  title =     {{DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =      {2016},
  address =   {Budapest, Hungary},
  month =     {3 September},
  doi =       {10.1111/j.1558-5646.2010.01180.x},
  file =      {:Users/jakobabeer/Downloads/Valenti-DCASE2016workshop.pdf:pdf},
  isbn =      {1558-5646 (Electronic)$\backslash$r0014-3820 (Linking)},
  keywords =  {Acoustic scene classification,DCASE,computational audio processing,convolutional neural networks},
  pmid =      {21361918}
}
@inproceedings{Virtanen:2000:HarmonicSeparationSinusoidalModel:ICASSP,
author = {Virtanen, Tuomas and Klapuri, Annsi},
booktitle = {in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {isad,partial{\_}tracking,source{\_}separation},
mendeley-tags = {isad,partial{\_}tracking,source{\_}separation},
title = {{Separation of Harmonic Sound Sources Using Sinusoidal Modeling}},
year = {2000}
}
@inproceedings{Virtanen:2019:MachineListeningTutorial:ICASSP,
address = {Brighton, UK},
author = {Virtanen, Tuomas and Mesaros, Annamaria and Heittola, Toni},
booktitle = {Tutorial at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/jakobabeer/Dropbox/ICASSP{\_}2019/conference/T9{\_}Notes.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
title = {{Detection and Classification of Acoustic Scenes and Events}},
year = {2019}
}
@article{Waddell:2019:MusicLearning:FIICT,
author = {Waddell, George and Williamon, Aaron},
doi = {10.3389/fict.2019.00011},
file = {:Users/jakobabeer/Downloads/fict-06-00011.pdf:pdf},
issn = {2297-198X},
journal = {Frontiers in ICT},
keywords = {learning,music{\_}learning,musicians,performance,technology,technology acceptance model},
mendeley-tags = {music{\_}learning},
number = {May},
pages = {1--14},
title = {{Technology Use and Attitudes in Music Learning}},
url = {https://www.frontiersin.org/article/10.3389/fict.2019.00011/full},
volume = {6},
year = {2019}
}
@inproceedings{Wang:2018:SelfDeterminationASC:APSIPA,
address = {Malaysia},
author = {Wang, Chien-Yao and Santoso, Andri and Wang, Jia-Ching},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
doi = {10.1109/APSIPA.2017.8281995},
file = {:Users/jakobabeer/Downloads/08281995.pdf:pdf},
isbn = {9781538615423},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {19--22},
title = {{Acoustic Scene Classification using Self-Determination Convolutional Neural Network}},
year = {2018}
}
@inproceedings{Wang:2017:ASC:ISCE,
address = {Kuala Lumpur, Malaysia},
author = {Wang, Chien-Yao and Wang, Jia-Ching and Wu, Yu-Chi and Chang, Pao-Chi},
booktitle = {Proceedings of the IEEE International Symposium on Consumer Electronics (ISCE)},
file = {:Users/jakobabeer/Downloads/08355533.pdf:pdf},
isbn = {9781538621899},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {11--12},
title = {{Asymmetric Kernel Convolution Neural Networks for Acoustic Scenes Classification}},
year = {2017}
}
@inproceedings{Wang:2017:PCEN:ICASSP,
abstract = {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
archivePrefix = {arXiv},
arxivId = {1607.05666},
author = {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2017.7953242},
eprint = {1607.05666},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45911.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
keywords = {automatic gain control,deep neural networks,robust and far-field speech recognition},
number = {1},
pages = {5670--5674},
title = {{Trainable Frontend for Robust and Far-Field Keyword Spotting}},
year = {2017}
}
@inproceedings{Wang:2019:MultipleInstanceLearning:ICASSP,
address = {Brighton, UK},
author = {Wang, Yun and Li, Juncheng and Metze, Florian},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8682847},
file = {:Users/jakobabeer/Downloads/08682847.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {31--35},
title = {{A Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling}},
url = {https://ieeexplore.ieee.org/document/8682847/},
year = {2019}
}
@inproceedings{Weiss:2015:Complexity:ICASSP,
address = {Brisbane, Australia},
author = {Wei{\ss}, Christof and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781467369978},
pages = {688--692},
title = {{Tonal Complexity Features for Style Classification of Classical Music}},
year = {2015}
}

@InProceedings{Weiping:2017:SpectrogramFusion:DCASE,
  author =        {Weiping, Zheng and Jiantao, Yi and Xiaotao, Xing and Xiangtao, Liu and Shaohu, Peng},
  title =         {{Acoustic Scene Classification using Deep Convolutional Neural Networks and Multiple Spectrogram Fusions}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16 - 17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Zheng{\_}159.pdf:pdf},
  isbn =          {1299670261},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@techreport{Whiteley,
abstract = {This paper presents a probabilistic model of temporal structure in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model naturally quantifies these three musical concepts in terms of hidden state-variables, allowing resolution of otherwise apparent ambiguities in musical structure. At the heart of the system is a probabilistic model of a hypothetical 'bar-pointer' which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates different input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior probability distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo.},
author = {Whiteley, Nick and Cemgil, A Taylan and Godsill, Simon},
file = {::},
keywords = {Bayesian inference,meter recog-nition,rhythm recognition,tempo tracking},
title = {{Bayesian Modelling of Temporal Structure in Musical Audio}}
}
@inproceedings{Wiggins:2019:GuitarCNN:ISMIR,
abstract = {Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation , followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset [24], and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation.},
address = {Delft, The Netherlands},
author = {Wiggins, Andrew and Kim, Youngmoo},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {music transcription},
mendeley-tags = {music transcription},
title = {{Guitar Tablature Estimation With a Convolutional Neural Network}},
url = {https://github.com/andywiggins/tab-cnn},
year = {2019}
}
@inproceedings{Wilkinghoff:2019:OpenSetASC:DCASE,
address = {New York, NY, USA},
author = {Wilkinghoff, Kevin and {Frank Kurth}},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinghoff, Frank Kurth - 2019 - Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {258--262},
title = {{Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders}},
year = {2019}
}@inproceedings{Wu:2019:SoundTexture:ICASSP,
address = {Brighton, UK},
archivePrefix = {arXiv},
arxivId = {1901.01502},
author = {Wu, Yuzhong and Lee, Tan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2019.8683490},
eprint = {1901.01502},
file = {:Users/jakobabeer/Downloads/08683490 (1).pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {815--819},
title = {{Enhancing Sound Texture in CNN-based Acoustic Scene Classification}},
year = {2019}
}
@article{Xia:2019:EventDetection:CSSR,
abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
author = {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
doi = {10.1007/s00034-019-01094-1},
issn = {15315878},
journal = {Circuits, Systems, and Signal Processing},
keywords = {Acoustic event detection,Deep learning,Strongly labeled,Weakly labeled},
title = {{A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection}},
year = {2019}
}
@article{Xie2019,
abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than {\$}30\backslash{\%}{\$} of the error rates of state-of-the-art methods: going from 7.66{\%} to 5.27{\%} and from 3.53{\%} to 2.46{\%} respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36{\%} to 79.04/94.45{\%} when compared to AutoAugment.},
archivePrefix = {arXiv},
arxivId = {1904.12848},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
eprint = {1904.12848},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Xie et al. - 2019 - Unsupervised Data Augmentation.pdf:pdf},
month = {apr},
title = {{Unsupervised Data Augmentation}},
url = {http://arxiv.org/abs/1904.12848},
year = {2019}
}
@article{Xu:2018:ASCMobileNet:ISM,
author = {Xu, Jun-Xiang and Lin, Tzu-Ching and Yu, Tsai-Ching and Tai, Tzu-Chiang and Chang, Pao-Chi},
doi = {10.1109/ISM.2018.00038},
file = {:Users/jakobabeer/Downloads/08603300.pdf:pdf},
isbn = {9781538668573},
journal = {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {267--270},
title = {{Acoustic Scene Classification Using Reduced MobileNet Architecture}},
year = {2018}
}

@InProceedings{Xu:2016:HierarchicalASC:DCASE,
  author =        {Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.},
  title =         {{Hierarchical Learning for DNN-Based Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Xu-a-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@inproceedings{Xu:2018:WeaklyLabeledEventDetection:ICASSP,
abstract = {In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classification, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labeled with one or a few audio tags but without timestamps of the audio events, which is called as weakly labeled data. Two sub-tasks are defined in this challenge including audio tagging and sound event detection using this weakly labeled data. A convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram is proposed. In addition, a temporal attention method is proposed along the frames to predicate the locations of each audio event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6$\backslash${\%} and Equal error 0.73, respectively.},
address = {Calgary, AB, Canada},
annote = {? how long are the input audio frames (10s?)},
archivePrefix = {arXiv},
arxivId = {1710.00343},
author = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8461975},
eprint = {1710.00343},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network(2).pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {Attention,Audio tagging,DCASE2017 challenge,Gated linear unit,Weakly supervised sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
pages = {121--125},
title = {{Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network}},
year = {2018}
}
@inproceedings{Xue:2019:DetectionLocalization:DCASE,
abstract = {Joint sound event detection (SED) and sound source localization (SSL) is essential since it provides both the temporal and spatial information of the events that appear in an acoustic scene. Although the problem can be tackled by designing a system based on the deep neural networks (DNNs) and fundamental spectral and spatial features, in this paper, we largely leverage the conventional microphone array signal processing techniques to generate more comprehensive representations for both SED and SSL, and to perform post-processing such that stable SED and SSL results can be obtained. Specifically, the features extracted from signals of multiple beams are utilized, which orient towards different directions of arrival (DOAs), and are formed according to the estimated steering vector of each DOA. Smoothed cross-power spectra (CPS) are computed based on the signal presence probability (SPP), and are used both as the input features of the DNNs, and for estimating the steering vectors of different DOAs. A triple-task learning scheme is developed, which jointly exploits the classification and regression based criterion for DOA estimation, and uses the classification based criterion as a regularization for the DNN. Experimental results demonstrate that the proposed method yields substantial improvements compared with the baseline method for the task 3 of the DCASE challenge 2019.},
author = {Xue, Wei and Tong, Ying and Zhang, Chao and Ding, Guohong},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}Xue{\_}91.pdf:pdf},
keywords = {Deep neural networks,Index Terms-Sound event detection,Sound source localiza-tion},
title = {{Multi-Beam and Multi-Task Learning for Joint Sound Event Detection and Localization}},
url = {http://dcase.community/documents/challenge2019/technical{\_}reports/DCASE2019{\_}Xue{\_}91.pdf},
year = {2019}
}
@inproceedings{Yang:2018:MultiScaleFeatures:DCASE,
address = {Surrey, UK},
author = {Yang, Liping and Chen, Xinxing and Tao, Lianjie},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Chen, Tao - 2018 - Acoustic Scene Classification using Multi-Scale Features.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using Multi-Scale Features}},
year = {2018}
}
@article{Ye:2018:ASC:AS,
abstract = {This paper presents a novel approach for acoustic scene classification based on efficient acoustic feature extraction using spectro-temporal descriptors fusion. Grounded on the finding in neuroscience-"auditory system summarizes the temporal details of sounds using time-averaged statistics to understand acoustic scenes", we devise an efficient computational framework for sound scene classification by using multipe time-frequency descriptors fusion with discriminant information enhancement. To characterize rich information of sound, i.e., local structures on the time-frequency plane, we adopt 2-dimensional local descriptors. A more critical issue raised in how to logically 'summarize' those local details into a compact feature vector for scene classification. Although 'time-averaged statistics' is suggested by the psychological investigation, directly computing time average of local acoustic features is not a logical way, since arithmetic mean is vulnerable to extreme values which are anticipated to be generated by interference sounds which are irrelevant to the scene category. To tackle this problem, we develop time-frame weighting approach to enhance sound textures as well as to suppress scene-irrelevant events. Subsequently, robust acoustic feature for scene classification can be efficiently characterized. The proposed method had been validated by using Rouen dataset which consists of 19 acoustic scene categories with 3029 real samples. Extensive results demonstrated the effectiveness of the proposed scheme.},
author = {Ye, Jiaxing and Kobayashi, Takumi and Toyama, Nobuyuki and Tsuda, Hiroshi and Murakawa, Masahiro},
doi = {10.3390/app8081363},
file = {:Users/jakobabeer/Downloads/applsci-08-01363.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences},
keywords = {Acoustic scene classification,Convex combination,Local descriptor,Summary statistics,Time-frequency analysis},
number = {8},
pages = {1--12},
title = {{Acoustic scene classification using efficient summary statistics and multiple spectro-temporal descriptor fusion}},
volume = {8},
year = {2018}
}

@InProceedings{Zoehrer:2016:GRN_ASC:DCASE,
  author =        {Z{\"{o}}hrer, Matthias and Pernkopf, Franz},
  title =         {{Gated Recurrent Networks Applied to Acoustic Scene Classification and Acoustic Event Detection}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Zohrer-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}
@techreport{Zahray,
abstract = {This paper presents a novel approach to recurrent neural network (RNN)-based beat and downbeat tracking that simultaneously performs chord recognition as an assisting task to improve performance. RNNs have been used successfully, but independently, for these two tasks. Since downbeat detection is still a challenging problem and chords are likely to change on downbeats, we propose two architectures of multi-task learning sharing the same latent representations of beats, downbeats, and chords. One is a Y-shaped architecture that decodes in parallel from the shared representations, and the other is a cascade architecture that decodes in a sequentially-dependent manner. Our comparative experiments using 85 popular songs with the time signature 4/4 from the RWC Music database showed that the F-measures of beat and down-beat detection are significantly improved on the same feature set due to the addition of simultaneous chord recognition. The Y-shaped architecture achieved the best results with a beat F-measure of 0.901 and a downbeat F-measure of 0.866, which are comparable with the current state-of-the-art.},
author = {Zahray, Lisa and Nakamura, Eita and Yoshii, Kazuyoshi},
file = {::},
keywords = {Index Terms-Beat and downbeat detection,chord recognition,multi-task learning,recurrent neural network},
title = {{BEAT AND DOWNBEAT DETECTION WITH CHORD RECOGNITION BASED ON MULTI-TASK LEARNING OF RECURRENT NEURAL NETWORKS}}
}
@inproceedings{Zeghidour:2016:JointLearning:INTERSPEECH,
address = {San Francisco, CA, USA},
author = {Zeghidour, Neil and Synnaeve, Gabriel and Usunier, Nicolas and Dupoux, Emmanuel},
booktitle = {Proceedings of the 17th Annual Conference of the International Speech Communication Association (Interspeech)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zeghidour et al. - 2016 - Joint Learning of Speaker and Phonetic Similarities with Siamese Networks(2).pdf:pdf},
pages = {1295--1299},
title = {{Joint Learning of Speaker and Phonetic Similarities with Siamese Networks.}},
year = {2016}
}
@inproceedings{Zeinali:2018:XVektorEmbeddings:DCASE,
address = {Surrey, UK},
author = {Zeinali, Hossein and Burget, Luk{\'{a}}s and Cernocky, Jan},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zeinali, Burget, Cernocky - 2018 - Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Cha.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Challenge}},
year = {2018}
}
@inproceedings{Zhang:2018:Mixup:ICLR,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1710.09412},
file = {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:pdf},
title = {{mixup: Beyond Empirical Risk Minimization}},
year = {2018}
}
@article{Zhang2020,
abstract = {Learning concepts from the limited number of datapoints is a challenging task usually addressed by the so-called one- or few-shot learning. Recently, an application of second-order pooling in few-shot learning demonstrated its superior performance due to the aggregation step handling varying image resolutions without the need of modifying CNNs to fit to specific image sizes, yet capturing highly descriptive co-occurrences. However, using a single resolution per image (even if the resolution varies across a dataset) is suboptimal as the importance of image contents varies across the coarse-to-fine levels depending on the object and its class label e. g., generic objects and scenes rely on their global appearance while fine-grained objects rely more on their localized texture patterns. Multi-scale representations are popular in image deblurring, super-resolution and image recognition but they have not been investigated in few-shot learning due to its relational nature complicating the use of standard techniques. In this paper, we propose a novel multi-scale relation network based on the properties of second-order pooling to estimate image relations in few-shot setting. To optimize the model, we leverage a scale selector to re-weight scale-wise representations based on their second-order features. Furthermore, we propose to a apply self-supervised scale prediction. Specifically, we leverage an extra discriminator to predict the scale labels and the scale discrepancy between pairs of images. Our model achieves state-of-the-art results on standard few-shot learning datasets.},
archivePrefix = {arXiv},
arxivId = {2001.01600},
author = {Zhang, Hongguang and Torr, Philip H. S. and Koniusz, Piotr},
eprint = {2001.01600},
file = {::},
title = {{Few-shot Learning with Multi-scale Self-supervision}},
url = {http://arxiv.org/abs/2001.01600},
year = {2020}
}
@inproceedings{Zhang:2019:LocalizationDetection:DCASE,
author = {Zhang, Jingyang and Ding, Wenhao and He, Liang},
file = {:Users/jakobabeer/Downloads/DCASE2019{\_}He{\_}97.pdf:pdf},
pages = {1--5},
title = {{Data Augmentation and Prior Knowledge-Based Regularization for Sound Event Localization and Detection}},
year = {2019}
}
@article{Zheng2017,
author = {Zheng, Nan-ning and Liu, Zi-yi and Ren, Peng-ju and Ma, Yong-qiang and Chen, Shi-tao},
file = {::},
keywords = {10,1631,1700053,causal model,cognitive computing,cognitive mapping,doi,dx,fitee,http,human-machine collaboration,hybrid-augmented intelligence,intuitive,org,reasoning,self-driving cars,visual scene understanding},
number = {2},
pages = {153--179},
title = {{Hybrid-augmented intelligence :}},
volume = {18},
year = {2017}
}
@article{Zhong:2017:RandomErasing:ARXIV,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
eprint = {1708.04896},
file = {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1708.04896.pdf:pdf},
journal = {Arxiv},
keywords = {data{\_}augmentation},
mendeley-tags = {data{\_}augmentation},
title = {{Random Erasing Data Augmentation}},
url = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@inproceedings{Zielinski:2018:BinauralASC:FEDCSIS,
abstract = {Binaural technology becomes increasingly popular in the multimedia systems. This paper identifies a set of features of binaural recordings suitable for the automatic classification of the four basic spatial audio scenes representing the most typical patterns of audio content distribution around a listener. Moreover, it compares the five artificial-intelligence-based methods applied to the classification of binaural recordings. The results show that both the spatial and the spectro-temporal features are essential to accurate classification of binaurally rendered acoustic scenes. The spectro-temporal features appear to have a stronger influence on the classification results than the spatial metrics. According to the obtained results, the method based on the support vector machine, exploiting the features identified in the study, yields the classification accuracy approaching 84{\%}.},
address = {Pozna{\'{n}}, Poland},
author = {Zieli{\'{n}}ski, S{\l}awomir K. and Lee, Hyunkook},
booktitle = {Proceedings of the Federated Conference on Computer Science and Information Systems (FedCSIS)},
doi = {10.15439/2018F182},
file = {:Users/jakobabeer/Downloads/08511268.pdf:pdf},
isbn = {9788394941970},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {585--588},
title = {{Feature Extraction of Binaural Recordings for Acoustic Scene Classification}},
year = {2018}
}
@book{Pfleiderer:2017:Jazzomat:BOOK,
editor = {Pfleiderer, Martin and Frieler, Klaus and Abe{\ss}er, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Pfleiderer{\_}2017{\_}BOOK.pdf:pdf},
publisher = {Schott Campus},
title = {{Inside the Jazzomat - New Perspectives for Jazz Research}},
year = {2017}
}
@book{Pfleiderer:2018:Jazzomat:BOOK,
editor = {Pfleiderer, Martin and Frieler, Klaus and Abe{\ss}er, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
file = {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Pfleiderer{\_}2017{\_}BOOK.pdf:pdf},
publisher = {Schott Campus},
title = {{Inside the Jazzomat - New Perspectives for Jazz Research}},
year = {2018}
}
@book{Virtanen:2018:SoundSceneBook:BOOK,
address = {Cham, Switzerland},
doi = {10.1007/978-3-319-63450-0},
editor = {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
file = {:Users/jakobabeer/Sync/Jakob/Knowhow/Books/Ellis, Dan{\_} Plumbley, Mark D.{\_} Virtanen, Tuomas-Computational analysis of sound scenes and events-Springer (2018).pdf:pdf},
publisher = {Springer International Publishing},
title = {{Computational Analysis of Sound Scenes and Events}},
url = {http://link.springer.com/10.1007/978-3-319-63450-0},
year = {2018}
}
