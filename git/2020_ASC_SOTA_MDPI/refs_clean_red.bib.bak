% Encoding: ISO-8859-1

@InProceedings{Abesser:2019:Stadtlaerm:DCASE,
  author =        {Abe{\ss}er, Jakob and G{\"{o}}tze, Marco and Clau{\ss}, Tobias and Zapf, Dominik and K{\"{u}}hn, Christian and Lukashevich, Hanna and K{\"{u}}hnlenz, Stephanie and Mimilakis, Stylianos},
  title =         {{Urban Noise Monitoring in the Stadtl{\"{a}}rm Project - A Field Report}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}DCASE.pdf:pdf},
  keywords =      {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
  mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city}
}

@InProceedings{Abesser:2017:ASC:DCASE,
  author =    {Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Gr{\"{a}}fe, Robert and Lukashevich, Hanna},
  title =     {{Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2017},
  address =   {Munich, Germany},
  month =     {16-17 November},
  abstract =  {Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban en-vironments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-the-art CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configura-tion leads to a classification accuracy of 75{\%} for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.},
  file =      {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Abesser{\_}165.pdf:pdf}
}

@InProceedings{Abidin:2017:LBP:ICASSP,
  author =        {Abidin, Shamsiah and Togneri, Roberto and Sohel, Ferdous},
  title =         {{Enhanced LBP Texture Features from Time Frequency Representations for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {626--630},
  address =       {New Orleans, LA, USA},
  month =         {5-9 March},
  file =          {:Users/jakobabeer/Downloads/07952231.pdf:pdf},
  isbn =          {9781509041176},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Amiriparian:2018:GANASC:EUSIPCO,
  author =    {Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Gerczuk, Maurice and Pugachevskiy, Sergey and Schuller, Bj{\"{o}}rn},
  title =     {{A Fusion of Deep Convolutional Generative Adversarial Networks and Sequence to Sequence Autoencoders for Acoustic Scene Classification}},
  booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
  year =      {2018},
  pages =     {977--981},
  address =   {Rome, Italy},
  month =     {3-7 September},
  abstract =  {Unsupervised representation learning shows high promise for generating robust features for acoustic scene analysis. In this regard, we propose and investigate a novel combination of features learnt using both a deep convolutional generative adversarial network (DCGAN) and a recurrent sequence to sequence autoencoder (S2SAE). Each of the representation learning algorithms is trained individually on spectral features extracted from audio instances. The learnt representations are: (i) the activations of the discriminator in case of the DCGAN, and (ii) the activations of a fully connected layer between the decoder and encoder units in case of the S2SAE. We then train two multilayer perceptron neural networks on the DCGAN and S2SAE feature vectors to predict the class labels. The individual predicted labels are combined in a weighted decision-level fusion to achieve the final prediction. The system is evaluated on the development partition of the acoustic scene classification data set of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). In comparison to the baseline, the accuracy is increased from 74.8 {\%} to 86.4 {\%} using only the DCGAN, to 88.5 {\%} on the development set using only the S2SAE, and to 91.1 {\%} after fusion of the individual predictions.},
  doi =       {10.23919/EUSIPCO.2018.8553225},
  file =      {:Users/jakobabeer/Downloads/08553225.pdf:pdf},
  isbn =      {9789082797015},
  issn =      {22195491},
  keywords =  {Acoustic scene classification,Generative adversarial networks,Sequence to sequence autoencoders,Unsupervised feature learning}
}

@Article{Aytar:2016:SoundNet:NIPS,
  author =        {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  title =         {{SoundNet: Learning Sound Representations from Unlabeled Video}},
  journal =       {Advances in Neural Information Processing Systems (NIPS)},
  year =          {2016},
  number =        {Nips},
  pages =         {892--900},
  abstract =      {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
  archiveprefix = {arXiv},
  arxivid =       {1610.09001},
  eprint =        {1610.09001},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Aytar, Vondrick, Torralba - 2016 - SoundNet Learning Sound Representations from Unlabeled Video.pdf:pdf},
  issn =          {10495258}
}

@Article{Bach:2015:LRP:PLOS,
  author =   {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
  title =    {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
  journal =  {PLoS ONE},
  year =     {2015},
  volume =   {10},
  number =   {7},
  pages =    {1--46},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest.We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  doi =      {10.1371/journal.pone.0130140},
  file =     {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/73bbd4448083b01b5a9389b3c37f5425aac0.pdf:pdf},
  issn =     {19326203},
  pmid =     {26161953}
}

@InProceedings{Bae:2016:LSTMCNN:DCASE,
  author =    {Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo},
  title =     {{Acoustic Scene Classification using Parallel Combination of LSTM and CNN}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2016},
  address =   {Budapest, Hungary},
  month =     {3 September},
  file =      {:Users/jakobabeer/Downloads/Bae-DCASE2016workshop.pdf:pdf}
}

@InProceedings{Basbug:2019:SpatialPyramidPoolingASC:ICSC,
  author =        {Basbug, Ahmet Melih and Sert, Mustafa},
  title =         {{Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks}},
  booktitle =     {Proceedings of the 13th IEEE International Conference on Semantic Computing (ICSC),},
  year =          {2019},
  pages =         {128--131},
  address =       {Newport, CA, USA, 30 January - 1 February},
  month =         {18-20 November},
  doi =           {10.1109/ICOSC.2019.8665547},
  file =          {:Users/jakobabeer/Downloads/08665547.pdf:pdf},
  isbn =          {9781538667835},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Bear:2019:JointASCAED:INTERSPEECH,
  author =        {Bear, Helen L. and Nolasco, In{\^{e}}s and Benetos, Emmanouil},
  title =         {{Towards joint sound scene and polyphonic sound event recognition}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2019},
  volume =        {2019-Septe},
  pages =         {4594--4598},
  address =       {Graz, Austria},
  month =         {2-15 November},
  abstract =      {Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two separate tasks in the field of computational sound scene analysis. In this work, we present a new dataset with both sound scene and sound event labels and use this to demonstrate a novel method for jointly classifying sound scenes and recognizing sound events. We show that by taking a joint approach, learning is more efficient and whilst improvements are still needed for sound event detection, SED results are robust in a dataset where the sample distribution is skewed towards sound scenes.},
  archiveprefix = {arXiv},
  arxivid =       {1904.10408},
  doi =           {10.21437/Interspeech.2019-2169},
  eprint =        {1904.10408},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Bear, Nolasco, Benetos - 2019 - Towards joint sound scene and polyphonic sound event recognition.pdf:pdf},
  issn =          {19909772},
  journal =       {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  keywords =      {Acoustic scene classification,CRNN,Computational sound scene analysis,Sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
  mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification}
}

@Article{Bello:2018:SONYC:CACM,
  author =        {Bello, Juan Pablo and Silva, Claudio and Nov, Oded and DuBois, R. Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish},
  title =         {{SONYC: A System for the Monitoring, Analysis and Mitigation of Urban Noise Pollution}},
  journal =       {Communications of the ACM (CACM)},
  year =          {2018},
  volume =        {62},
  number =        {2},
  abstract =      {We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs. SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment. SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.},
  archiveprefix = {arXiv},
  arxivid =       {1805.00889},
  eprint =        {1805.00889},
  file =          {:Users/jakobabeer/Downloads/bello{\_}sonyc{\_}cacm{\_}2018.pdf:pdf},
  isbn =          {1234567245},
  keywords =      {acm reference format,citizen science,cyber-physical systems,machine listening,noise pollution,sensor networks,smart cities,visualization},
  url =           {http://arxiv.org/abs/1805.00889}
}

@InProceedings{Benetos:2012:ASC:DAFX,
  author =        {Benetos, Emmanouil and Lagrange, Mathieu and Dixon, Simon},
  title =         {{Characterisation of Acoustic Scenes using a Temporally-Constrained Shift-Invariant Model}},
  booktitle =     {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
  year =          {2012},
  pages =         {1--7},
  address =       {York, UK},
  month =         {17-21 September},
  abstract =      {In this paper, we propose a method for modeling and classifying acoustic scenes using temporally-constrained shift-invariant probabilistic latent component analysis (SIPLCA). SIPLCA can be used for extracting time-frequency patches from spectrograms in an unsupervised manner. Component-wise hidden Markov models are incorporated to the SIPLCA formulation for enforcing temporal constraints on the activation of each acoustic component. The time-frequency patches are converted to cepstral coefficients in order to provide a compact representation of acoustic events within a scene. Experiments are made using a corpus of train station recordings, classified into 6 scene classes. Results show that the proposed model is able to model salient events within a scene and outperforms the non-negative matrix factorization algorithm for the same task. In addition, it is demonstrated that the use of temporal constraints can lead to improved performance.},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/dafx12{\_}submission{\_}30.pdf:pdf},
  journal =       {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Bisot:2015:ASC:EUSIPCO,
  author =        {Bisot, Victor and Essid, Slim and Richard, Gael},
  title =         {{HOG and Subband Power Distribution Image Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the 23rd European Signal Processing Conference (EUSIPCO)},
  year =          {2015},
  pages =         {719--723},
  address =       {Nice, France},
  month =         {31 August - 4 September},
  abstract =      {Acoustic scene classification is a difficult problem mostly due to the high density of events concurrently occurring in audio scenes. In order to capture the occurrences of these events we propose to use the Subband Power Distribution (SPD) as a feature. We extract it by computing the histogram of amplitude values in each frequency band of a spectrogram image. The SPD allows us to model the density of events in each frequency band. Our method is evaluated on a large acoustic scene dataset using support vector machines. We outperform the previous methods when using the SPD in conjunction with the histogram of gradients. To reach further improvement, we also consider the use of an approximation of the earth mover's distance kernel to compare histograms in a more suitable way. Using the so-called Sinkhorn kernel improves the results on most of the feature configurations. Best performances reach a 92.8{\%} F1 score.},
  doi =           {10.1109/EUSIPCO.2015.7362477},
  file =          {:Users/jakobabeer/Downloads/07362477.pdf:pdf},
  isbn =          {9780992862633},
  keywords =      {Acoustic scene classification,Sinkhorn distance,acoustic{\_}scene{\_}classification,machine{\_}listening,subband power distribution image,support vector machine},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Bisot:2017:ASC:TASLP,
  author =   {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Ga{\"{e}}l},
  title =    {{Feature Learning with Matrix Factorization Applied to Acoustic Scene Classification}},
  journal =  {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =     {2017},
  volume =   {25},
  number =   {6},
  pages =    {1216--1229},
  abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
  doi =      {10.1109/TASLP.2017.2690570},
  file =     {:Users/jakobabeer/Downloads/main{\_}bisot2016.pdf:pdf},
  issn =     {23299290},
  keywords = {Acoustic scene classification,feature learning,matrix factorization}
}

@InProceedings{Bisot:2017:NFASC:DCASE,
  author =        {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael},
  title =         {{Nonnegative Feature Learning Methods for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Bisot{\_}194.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Boddapati:2017:ASC:PCS,
  author =    {Boddapati, Venkatesh and Petef, Andrej and Rasmusson, Jim and Lundberg, Lars},
  title =     {{Classifying environmental sounds using image recognition networks}},
  journal =   {Procedia Computer Science},
  year =      {2017},
  volume =    {112},
  pages =     {2048--2056},
  abstract =  {Automatic classification of environmental sounds, such as dog barking and glass breaking, is becoming increasingly interesting, especially for mobile devices. Most mobile devices contain both cameras and microphones, and companies that develop mobile devices would like to provide functionality for classifying both videos/images and sounds. In order to reduce the development costs one would like to use the same technology for both of these classification tasks. One way of achieving this is to represent environmental sounds as images, and use an image classification neural network when classifying images as well as sounds. In this paper we consider the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds. We evaluate the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition (AlexNet and GoogLeNet). Our experiments show that we obtain good classification accuracy for the three datasets.},
  doi =       {10.1016/j.procs.2017.08.250},
  file =      {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Boddapati et al. - 2017 - Classifying environmental sounds using image recognition networks.pdf:pdf},
  issn =      {18770509},
  keywords =  {Convolutional Neural Networks,Deep Learning,Environmental Sound Classification,GPU Processing,Image Classification},
  publisher = {Elsevier B.V.},
  url =       {http://dx.doi.org/10.1016/j.procs.2017.08.250}
}

@InProceedings{Chen:2019:ASC:DCASE,
  author =    {Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong},
  title =     {{Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  address =   {New York, NY, USA},
  month =     {25-26 October},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Zhang{\_}34.pdf:pdf}
}

@InProceedings{Chen:2018:Scalogram:INTERSPEECH,
  author =        {Chen, Hangting and Zhang, Pengyuan and Bai, Haichuan and Yuan, Qingsheng and Bao, Xiuguo and Yan, Yonghong},
  title =         {{Deep convolutional neural network with scalogram for audio scene modeling}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2018},
  pages =         {3304--3308},
  address =       {Hyderabad, India},
  month =         {2-6 September},
  abstract =      {Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5{\%} in the evaluation step in DCASE2016.},
  doi =           {10.21437/Interspeech.2018-1524},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1524.pdf:pdf},
  issn =          {19909772},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Chen:2019:ASCFilters:ICASSP,
  author =        {Chen, Hangting and Zhang, Pengyuan and Yan, Yonghong},
  title =         {{An Audio Scene Classification Framework with Embedded Filters and a DCT-Based Temporal Module}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {835--839},
  address =       {Brighton, UK},
  month =         {12-17 May},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0000835.pdf:pdf},
  isbn =          {9781538646588},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Cho:DCASE:LargeMarginCNN:DCASE,
  author =        {Cho, Janghoon and Yun, Sungrack and Park, Hyoungwoo and Eum, Jungyun and Hwang, Kyuwoong},
  title =         {{Acoustic Scene Classification Based on a Large-Margin Factorized CNN}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {45--49},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  archiveprefix = {arXiv},
  arxivid =       {1910.06784},
  doi =           {10.33682/8xh4-jm46},
  eprint =        {1910.06784},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2019 - Acoustic Scene Classification Based on a Large-Margin Factorized CNN.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Dang:2018:ASCMulti:ICCE,
  author =        {Dang, An and Vu, Toan H. and Wang, Jia-Ching},
  title =         {{Acoustic Scene Classification using Convolutional Neural Networks and Multi-Scale Multi-Feature Extraction}},
  booktitle =     {Proceedings of the IEEE International Conference on Consumer Electronics (ICCE)},
  year =          {2018},
  address =       {Hue City, Vietnam},
  month =         {18-20 July},
  doi =           {10.1109/ICCE.2018.8326315},
  file =          {:Users/jakobabeer/Downloads/08326315.pdf:pdf},
  isbn =          {9781538630259},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Drossos:2019:DomainAdaptation:WASPAA,
  author =        {Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas},
  title =         {{Unsupervised Adversarial Domain Adaptation based on the Wasserstein Distance for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year =          {2019},
  pages =         {259--263},
  address =       {New Paltz, NY, USA},
  month =         {20-23 October},
  publisher =     {IEEE},
  abstract =      {A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of $\Delta$-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\%} to 45{\%}, using the TUT Acoustic Scenes dataset.},
  file =          {::},
  keywords =      {Acoustic scene classification,Wasserstein distance,acoustic{\_}scene{\_}classificaiton,adversarial training,domain{\_}adaptation,machine{\_}listening,unsupervised domain adaptation},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,domain{\_}adaptation,machine{\_}listening}
}

@Article{Drossos:2020:SED:ARXIV,
  author =        {Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas},
  title =         {{Sound Event Detection with Depthwise Separable and Dilated Convolutions}},
  journal =       {ArXiv pre-prints},
  year =          {2020},
  abstract =      {State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85{\%} and average training time per epoch by 78{\%}, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6{\%} and 3.8{\%}, respectively.},
  archiveprefix = {arXiv},
  arxivid =       {2002.00476},
  eprint =        {2002.00476},
  file =          {::},
  url =           {http://arxiv.org/abs/2002.00476}
}

@InProceedings{Fonseca:2017:ASC:DACSE,
  author =        {Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier},
  title =         {{Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Fonseca{\_}181.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Gemmeke:2017:Audioset:ICASSP,
  author =    {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  title =     {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2017},
  pages =     {776--780},
  address =   {New Orleans, LA, USA},
  month =     {5-9 March},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45857.pdf:pdf}
}

@InProceedings{Gharib:2018:DomainAdaptationASC:DCASE,
  author =        {Gharib, Shayan and Drossos, Konstantinos and Emre, Cakir and Serdyuk, Dmitriy and Virtanen, Tuomas},
  title =         {{Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Gharib et al. - 2018 - Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
  mendeley-tags = {acoustic{\_}scene{\_}classification,domain{\_}adaptation}
}

@InProceedings{Goodfellow:2014:GAN:NIPS,
  author =    {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and {Yoshua Bengio}},
  title =     {{Generative Adversarial Nets}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year =      {2014},
  pages =     {2672--2680},
  abstract =  {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/5423-generative-adversarial-nets.pdf:pdf},
  issn =      {10495258}
}

@InProceedings{Gordon:2018:MorphNet:CVPR,
  author =        {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien Ju and Choi, Edward},
  title =         {{MorphNet: Fast {\&} Simple Resource-Constrained Structure Learning of Deep Networks}},
  booktitle =     {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year =          {2018},
  number =        {1},
  pages =         {1586--1595},
  address =       {Salt Lake City, UT, USA},
  month =         {18-23 June},
  abstract =      {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
  archiveprefix = {arXiv},
  arxivid =       {1711.06798},
  doi =           {10.1109/CVPR.2018.00171},
  eprint =        {1711.06798},
  file =          {::},
  isbn =          {9781538664209},
  issn =          {10636919},
  journal =       {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@InProceedings{Green:2017:SpatialFeaturesASC:DCASE,
  author =        {Green, Marc C. and Murphy, Damian},
  title =         {{Acoustic Scene Classification using Spatial Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  abstract =      {Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or bin-aural datasets. This paper introduces EigenScape -a new dataset of 4th-order Ambisonic acoustic scene recordings -and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient -Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Direc-tional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The dif-ferences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Green{\_}126.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Grollmisch:2019:ISA:EUSIPCO,
  author =        {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
  title =         {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis (ISA)}},
  booktitle =     {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
  year =          {2019},
  pages =         {1--5},
  address =       {A Coruña, Spain},
  month =         {2-6 September},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Grollmisch{\_}2019{\_}EUSIPCO.pdf:pdf},
  keywords =      {idmt},
  mendeley-tags = {idmt}
}

@InProceedings{Han:2017:BinauralASC:DCASE,
  author =        {Han, Yoonchang and Park, Jeongsoo and Lee, Kyogu},
  title =         {{Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Han{\_}206.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Huang:2019:ASCEnsemble:DCASE,
  author =        {Huang, Jonathan and Lu, Hong and Lopez-Meyer, Paulo and Maruri, Hector A. Cordourier and Ontiveros, Juan A. del Hoyo},
  title =         {{Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {94--98},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2019 - Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Jati:2020:ASC:ICASSP,
  author =        {Jati, Arindam and Nadarajan, Amrutha and Mundnich, Karel and Narayanan, Shrikanth},
  title =         {{Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting}},
  booktitle =     {submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2020},
  address =       {Barcelona, Spain},
  abstract =      {Devices capable of detecting and categorizing acoustic scenes have numerous applications such as providing context-aware user experiences. In this paper, we address the task of characterizing acoustic scenes in a workplace setting from audio recordings collected with wearable microphones. The acoustic scenes, tracked with Bluetooth transceivers, vary dynamically with time from the egocentric perspective of a mobile user. Our dataset contains experience sampled long audio recordings collected from clinical providers in a hospital, who wore the audio badges during multiple work shifts. To handle the long egocentric recordings, we propose a Time Delay Neural Network{\~{}}(TDNN)-based segment-level modeling. The experiments show that TDNN outperforms other models in the acoustic scene classification task. We investigate the effect of primary speaker's speech in determining acoustic scenes from audio badges, and provide a comparison between performance of different models. Moreover, we explore the relationship between the sequence of acoustic scenes experienced by the users and the nature of their jobs, and find that the scene sequence predicted by our model tend to possess similar relationship. The initial promising results reveal numerous research directions for acoustic scene classification via wearable devices as well as egocentric analysis of dynamic acoustic scenes encountered by the users.},
  archiveprefix = {arXiv},
  arxivid =       {1911.03843},
  eprint =        {1911.03843},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jati et al. - 2020 - Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting(2).pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  url =           {http://arxiv.org/abs/1911.03843}
}

@InProceedings{Jimenez:2017:ShiftInvariantASC:DCASE,
  author =        {Jim{\'{e}}nez, Abelino and Elizalde, Benjam{\'{i}}n and Raj, Bhiksha},
  title =         {{DCASE 2017 Task 1: Acoustic Scene Classification using Shift-Invariant Kernels and Random Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Jimenez{\_}195.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Kong:2019:SceneGeneration:ICASSP,
  author =    {Kong, Qiuqiang and Xu, Yong and Iqbal, Turab and Cao, Yin and Wang, Wenwu and Plumbley, Mark D},
  title =     {{Acoustic Scene Generation with Conditional SampleRNN}},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2019},
  pages =     {925--929},
  address =   {Brighton, UK},
  month =     {12-17 May},
  file =      {::},
  isbn =      {9781538646588}
}

@InProceedings{Kosmider:2019:DeviceCalibration:DCASE,
  author =    {Kosmider, Michal},
  title =     {{Calibrating Neural Networks for Secondary Recording Devices}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  address =   {New York, NY, USA},
  month =     {25-26 October},
  file =      {::}
}

@InProceedings{Koutini:2019:ReceptiveField:DCASE,
  author =        {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
  title =         {{Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {124--128},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Koutini, Eghbal-zadeh, Widmer - 2019 - Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Koutini:2019:ASC:DCASE,
  author =    {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard and Kepler, Johannes},
  title =     {{CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  pages =     {1--5},
  address =   {New York, NY, USA},
  month =     {25-26 October},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Koutini{\_}99.pdf:pdf}
}

@InProceedings{Kumar:2018:ASC:ICASSP,
  author =        {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
  title =         {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2018},
  pages =         {326--330},
  address =       {Alberta, Canada},
  month =         {15-20 April},
  abstract =      {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
  annote =        {- for weakly labeled data - CNN - audio of variable length - usable for transfer learning for domain and task adaptation - problem on scene classification: -- few datasets -- labeling expensive/difficult -- begin and end of event subjective - audioset with weak labels (DCASE2017) - "Soundnet" transfer visual model to audio data - train on "Audioset"(weakly labeled example from Youtube with 527 classes) - tested on ESC-50 (same task but different domain) and DCASE2016 (task adaption) - SLAT -{\textgreater} strong label assumption training (class correct for all patches) - logmel spec as input, 44.1kHz, 128 mel bands, winsize 23ms, overlap 11.5 - global pooling at the end -{\textgreater} fully conv for variable length of input - ESC-50: outperform sota by 9.3{\%} - DCASE2016: absolute improvement of 4.1{\%} - Audioset: sota results},
  doi =           {10.1109/ICASSP.2018.8462200},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Khadkevich, Fugen - 2018 - Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events an(3).pdf:pdf},
  isbn =          {978-1-5386-4658-8},
  issn =          {15206149},
  keywords =      {Audio Event Classification,Learning Representations,Transfer Learning,Weak Label Learning,acmus,dnn better},
  mendeley-tags = {acmus,dnn better},
  url =           {https://ieeexplore.ieee.org/document/8462200/}
}

@InProceedings{Lassseck:2018:Bird:DCASE,
  author =        {Lasseck, Mario},
  title =         {{Acoustic bird detection with deep convolutional neural networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE)},
  year =          {2018},
  pages =         {143--147},
  address =       {Surrey, UK},
  month =         {19-20 November},
  abstract =      {This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 $\backslash$$\backslash${\%} on the public challenge leaderboard.},
  file =          {::},
  keywords =      {Bird Detection,Data Augmentation,Deep Convolutional Neural Networks,Deep Learning,bird{\_}recognition,machine{\_}listening},
  mendeley-tags = {bird{\_}recognition,machine{\_}listening}
}

@InProceedings{Lehner:2019:ASCReject:DCASE,
  author =        {Lehner, Bernhard and Koutini, Khaled and Schwarzlm{\"{u}}ller, Christopher and Gallien, Thomas and Widmer, Gerhard},
  title =         {{Acoustic Scene Classification with Reject Option based on Resnets}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Downloads/DCASE20191.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening}
}

@InProceedings{Li:2018:ASC:ICALIP,
  author =        {Li, Yanxiong and Li, Xianku and Zhang, Yuhan and Wang, Wucheng and Liu, Mingle and Feng, Xiaohui},
  title =         {{Acoustic Scene Classification Using Deep Audio Feature and BLSTM Network}},
  booktitle =     {Proceedings of the 6th International Conference on Audio, Language and Image Processing (ICALIP)},
  year =          {2018},
  pages =         {371--374},
  address =       {Shanghai, China},
  month =         {16-17 July},
  abstract =      {Although acoustic scene classification has been received great attention from researchers in the field of audio signal processing, it is still a challenging and unsolved task to date. In this paper, we present our work of acoustic scene classification for the challenge of the Detection and Classification of Acoustic Scenes and Events 2017, i.e., DCASE2017 challenge, using a feature of Deep Audio Feature (DAF) for acoustic scene representation and a classifier of Bidirectional Long Short Term Memory (BLSTM) network for acoustic scene classification. We first use a deep neural network to generate the DAF from Mel frequency cepstral coefficients, and then adopt a network of BLSTM fed by the DAF for acoustic scene classification. When evaluated on the official datasets of the DCASE2017 challenge, the proposed system outperforms the baseline system in terms of classification accuracy.},
  doi =           {10.1109/ICALIP.2018.8455765},
  file =          {:Users/jakobabeer/Downloads/08455765.pdf:pdf},
  isbn =          {9781538651957},
  keywords =      {acoustic scene classification,acoustic{\_}scene{\_}classification,bidirectional long short term memory network,deep audio feature,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Li:2019:MultilevelAttention:ICMEW,
  author =        {Li, Zhitong and Hou, Yuanbo and Xie, Xiang and Li, Shengchen and Zhang, Liqiang and Du, Shixuan and Liu, Wei},
  title =         {{Multi-Level Attention Model with Deep Scattering Spectrum for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year =          {2019},
  pages =         {396--401},
  address =       {Shanghai, China},
  month =         {8-12 July},
  abstract =      {Acoustic scene classification (ASC) refers to the classification of audio into one of predefined classes that characterize the environment. People are used to combine log-mel filterbank features with convolutional neural network (CNN) to build ASC system. In this paper, we explore the use of deep scattering spectrum (DSS) features combined with a multi-level attention model based on CNN for ASC tasks. First, the time scatter and frequency scatter coefficients of DSS with different resolutions are explored as ASC features. Second, we incorporate a multi-level attention model into CNN to build the classification system. We then evaluate the proposed approach on the IEEE challenge of detection and classification of acoustic scenes and events 2018 (DCASE 2018) dataset. Results show that the DSS features provide between a 11{\%}-14{\%} relative improvement in accuracy over log-mel features, within a state-of-the-art framework. The application of multilevel attention model on CNN can improve the accuracy by nearly 5{\%}. The highest accuracy of our proposed system is 78.3{\%} on the development set.},
  doi =           {10.1109/ICMEW.2019.00074},
  file =          {:Users/jakobabeer/Downloads/08794892.pdf:pdf},
  isbn =          {9781538692141},
  keywords =      {Acoustic scene classification,DCASE 2018,Deep scattering spectrum,Multi-level attention mechanism,acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Lostanlen:2018:PCEN:SPL,
  author =        {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
  title =         {{Per-channel energy normalization: Why and how}},
  journal =       {IEEE Signal Processing Letters},
  year =          {2019},
  volume =        {26},
  number =        {1},
  pages =         {39--43},
  abstract =      {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named per-channel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This letter investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Second, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Third, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise, PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
  doi =           {10.1109/LSP.2018.2878620},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Lostanlen et al. - 2019 - Per-channel energy normalization Why and how(2).pdf:pdf},
  issn =          {10709908},
  keywords =      {Acoustic noise,acoustic sensors,acoustic signal detection,machine{\_}listening,signal classification,spectrogram},
  mendeley-tags = {machine{\_}listening}
}

@InProceedings{Maka:2018:FeatureSpaceASC:DCASE,
  author =        {Maka, Tomasz},
  title =         {{Audio Feature Space Analysis for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Maka - 2018 - Audio Feature Space Analysis for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Marchi:2016:MKSL:DCASE,
  author =        {Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
  title =         {{Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  abstract =      {We propose a system for acoustic scene classification using pair-wise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contri-bution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the 'one-against-all' paradigm to perform a pair-wise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4{\%} is achieved.},
  file =          {:Users/jakobabeer/Downloads/Marchi-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mariotti:2018:DeepVisionASC:DCASE,
  author =        {Mariotti, Octave and Cord, Matthieu and Schwander, Olivier},
  title =         {{Exploring Deep Vision Models for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mariotti, Cord, Schwander - 2018 - Exploring Deep Vision Models for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mars:2019:BinauralASC:DCASE,
  author =        {Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chongsoon},
  title =         {{Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {149--153},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  doi =           {10.33682/6c9z-gd15},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mars et al. - 2019 - Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mcdonnell:2019:AcousticScenes:DCASE,
  author =        {Mcdonnell, Mark D and Gao, Wei},
  title =         {{Acoustic Scene Classification Using Deep Residual Networks With Late Fusion of Separated High and Low Frequency Paths}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  abstract =      {This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output.},
  file =          {::},
  keywords =      {acou,machine{\_}listening},
  mendeley-tags = {acou,machine{\_}listening},
  url =           {https://github.com/McDonnell-Lab/DCASE2019-Task1}
}

@Article{Mesaros:2016:ASC:IEEE_TASLP,
  author =    {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
  title =     {{Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge}},
  journal =   {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =      {2018},
  volume =    {26},
  number =    {2},
  pages =     {379--393},
  abstract =  {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events DCASE 2016 has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: Acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
  doi =       {10.1109/TASLP.2017.2778423},
  file =      {:Users/jakobabeer/Downloads/08123864.pdf:pdf},
  issn =      {23299290},
  keywords =  {Acoustic scene classification,audio datasets,pattern recognition,sound event detection},
  publisher = {IEEE}
}

@InProceedings{Mesaros:2018:MultiDeviceDataset:DCASE,
  author =        {Mesaros, Annemaria and Heittola, Toni and {Tuomas Virtanen}},
  title =         {{A Multi-Device Dataset for Urban Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Heittola, Tuomas Virtanen - 2018 - A Multi-Device Dataset for Urban Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mesaros:2017:HumanASC:WASPAA,
  author =    {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title =     {{Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study}},
  booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year =      {2017},
  pages =     {319--323},
  address =   {New Paltz, NY, USA},
  month =     {15-18 October}
}

@InProceedings{Mesaros:2019:ClosedOpenSet:DCASE,
  author =        {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title =         {{Acoustic Scene Classification in DCASE 2019 Challenge:Closed and Open Set Classification and Data Mismatch Setups}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {164--168},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  doi =           {10.33682/m5kp-fa97},
  file =          {::},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Moritz:2016:TDNN:DCASE,
  author =        {Moritz, Niko and Schr{\"{o}}der, Jens and Goetze, Stefan and Anem{\"{u}}ller, J{\"{o}}rn and Kollmeier, Birger},
  title =         {{Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  abstract =      {This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual infor-mation similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 76.5 {\%}, which is 4.0 {\%} higher compared to the DCASE'16 baseline system.},
  file =          {:Users/jakobabeer/Downloads/Moritz-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2017:GANASC:DCASE,
  author =        {Mun, Seongkyu and Park, Sangwook and Han, David K. and Ko, Hanseok},
  title =         {{Generative Adversarial Networks based Acoustic Scene Training Set Augmentation and Selection using SVM Hyperplane}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Mun{\_}215.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2019:DomainMismatch:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon},
  title =         {{Domain Mismatch Robust Acoustic Scene Classification Using Channel Information Conversion}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {845--849},
  address =       {Brighton, UK},
  month =         {12-17 May},
  abstract =      {In recent acoustic scene classification (ASC) research field, training and test device channel mismatch have become an issue for the real world implementation. To address the issue, this paper proposes a channel domain conversion using factor-ized hierarchical variational autoencoder. Proposed method adapts both the source and target domain to a pre-defined specific domain. Unlike the conventional approach, the relationship between the target and source domain and information of each domain are not required in the adaptation process. Based on the experimental results using the IEEE Detection and Classification of Acoustic Scenes and Event 2018 task 1-B dataset and the baseline system, it is shown that the proposed approach can mitigate the channel mismatching issue of different recording devices.},
  doi =           {10.1109/ICASSP.2019.8683514},
  file =          {:Users/jakobabeer/Downloads/08683514.pdf:pdf},
  isbn =          {9781479981311},
  issn =          {15206149},
  keywords =      {acoustic scene classification,acoustic{\_}scene{\_}classification,domain adaptation,factorized hierarchical variational autoencoder,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2017:ASC:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Han, David K. and Ko, Hanseok},
  title =         {{Deep Neural Network Based Learning and Transferring Mid-Level Audio Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {796--800},
  address =       {New Orleans, LA, USA},
  month =         {5-9 March},
  abstract =      {Deep Neural Network (DNN) based transfer learning has been shown to be effective in Visual Object Classification (VOC) for complementing the deficit of target domain training samples by adapting classifiers that have been pre- trained for other large-scaled DataBase (DB). Although there exists an abundance of acoustic data, it can also be said that datasets of specific acoustic scenes are sparse for training Acoustic Scene Classification (ASC) models. By exploiting VOC DNNâs ability of learning beyond its pre- trained environments, this paper proposes DNN based transfer learning for ASC. Effectiveness of the proposed method is demonstrated on the database of IEEE DCASE Challenge 2016 Task 1 and home surveillance environment via representative experiments. Its improved performance is verified by comparing it to prominent conventional methods.},
  doi =           {10.1097/IOP.0000000000000348},
  file =          {:Users/jakobabeer/Downloads/07952265.pdf:pdf},
  isbn =          {9781509041176},
  issn =          {15372677},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Nguyen:2018:ASCEnsemble:DCASE,
  author =        {Nguyen, Truc and Pernkopf, Franz},
  title =         {{Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Pernkopf - 2018 - Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Nwe:2017:MultiTaskASC:APSIPA,
  author =    {Nwe, Tin Lay and Dat, Tran Huy and Ma, Bin},
  title =     {{Convolutional Neural Network with Multi-Task Learning Scheme for Acoustic Scene Classification}},
  booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =      {2018},
  pages =     {1347--1350},
  address =   {Honolulu, Hawaii, USA},
  month =     {2-15 November},
  abstract =  {Deep Neural Network (DNN) with Multi-Task Learning (MTL) methods have recently demonstrated significant performance gains on a number of classification, detection, recognition tasks compared to conventional DNN. DNN with MTL framework involves cross-task and within-task knowledge sharing layers. MTL methods have benefit for regularization effect from the cross-task knowledge sharing layers. And, within- task knowledge sharing layers allow MTL based DNN to learn information to optimize the performance for individual task. We formulate our acoustic scene classification in MTL framework using Convolutional Neural Network to learn information specific to different types of environment. We conduct experiments using DCASE2016 dataset. Proposed approach achieves 83.8{\%} accuracy to classify 15 acoustic scene classes.},
  doi =       {10.1109/APSIPA.2017.8282241},
  file =      {:Users/jakobabeer/Downloads/08282241.pdf:pdf},
  isbn =      {9781538615423}
}

@InProceedings{Park:2019:SpecAugment:INTERSPEECH,
  author =        {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  title =         {{Specaugment: A simple data augmentation method for automatic speech recognition}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2019},
  volume =        {2019-Septe},
  pages =         {2613--2617},
  address =       {Graz, Austria},
  month =         {2-15 November},
  abstract =      {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
  archiveprefix = {arXiv},
  arxivid =       {1904.08779},
  doi =           {10.21437/Interspeech.2019-2680},
  eprint =        {1904.08779},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2019 - Specaugment A simple data augmentation method for automatic speech recognition.pdf:pdf},
  issn =          {19909772},
  journal =       {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  keywords =      {Data augmentation,End-to-end speech recognition}
}

@InProceedings{Park:2017:DoubleImageASC:DCASE,
  author =        {Park, Sangwook and Mun, Seonkyu and Lee, Younglo and Ko, Hanseok},
  title =         {{Acoustic Scene Classification Based on Convolutional Neural Network using Double Image Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Park{\_}214.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Phaye:2019:Subspectralnet:ICASSP,
  author =        {Phaye, Sai Samarth R and Benetos, Emmanouil and Wang, Ye},
  title =         {{Subspectralnet - Using Sub-Spectrogram based Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {825--829},
  address =       {Brighton, UK},
  month =         {12-17 May},
  file =          {:Users/jakobabeer/Downloads/08683288 (1).pdf:pdf},
  isbn =          {9781538646588},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Qian:2017:WaveletASC:DCASE,
  author =        {Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Wavelets Revisited for the Classification of Acoustic Scenes}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Qian{\_}132.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Rafii:2012:REPET:ISMIR,
  author =    {Rafii, Zafar and Pardo, Bryan},
  title =     {{Music/Voice Separation using the Similarity Matrix}},
  booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
  year =      {2012},
  pages =     {583--588},
  address =   {Porto, Portugal},
  month =     {8-12 October},
  abstract =  {Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. {\textcopyright} 2012 International Society for Music Information Retrieval.},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0740488f2e930fa1b0a0ec546b876cf00ecb.pdf:pdf},
  isbn =      {9789727521449}
}

@InProceedings{Ren:2019:AttrousCNNAttention:ICASSP,
  author =        {Ren, Zhao and Kong, Qiuqiang and Han, Jing and Plumbley, Mark D and Schuller, Bjorn W.},
  title =         {{Attention-based Atrous Convolutional Neural Networks: Visualisation and Understanding Perspectives of Acoustic Scenes}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {56--60},
  address =       {Brighton, UK},
  month =         {12-17 May},
  doi =           {10.1109/ICASSP.2019.8683434},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2019 - Attention-based Atrous Convolutional Neural Networks Visualisation and Understanding Perspectives of Acoustic Scenes.pdf:pdf},
  keywords =      {machine{\_}listening},
  mendeley-tags = {machine{\_}listening},
  url =           {https://ieeexplore.ieee.org/document/8683434/}
}

@InProceedings{Ren:2018:AttentionASC:DCASE,
  author =        {Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark D. and Schuller, Bj{\"{o}}rn W.},
  title =         {{Attention-based Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2018 - Attention-based Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Ren:2017:DeepSequentialASC:DCASE,
  author =        {Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Deep Sequential Image Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Ren{\_}133.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Roletscheck:2019:EvolutionaryASC:DCASE,
  author =        {Roletscheck, Christian and Watzka, Tobias and Seiderer, Andreas and Schiller, Dominik and Andr{\'{e}}, Elisabeth},
  title =         {{Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Roletscheck et al. - 2019 - Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Ruder:2017:MultitaskLearning:ARXIV,
  author =        {Ruder, Sebastian},
  title =         {{An Overview of Multi-Task Learning in Deep Neural Networks}},
  journal =       {ArXiv pre-prints},
  year =          {2017},
  abstract =      {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archiveprefix = {arXiv},
  arxivid =       {1706.05098},
  eprint =        {1706.05098},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1706.05098.pdf:pdf},
  url =           {http://arxiv.org/abs/1706.05098}
}

@Article{Russakovsky:2015:ImageNet:IJCV,
  author =        {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title =         {{ImageNet Large Scale Visual Recognition Challenge}},
  journal =       {International Journal of Computer Vision},
  year =          {2015},
  volume =        {115},
  number =        {3},
  pages =         {211--252},
  abstract =      {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  arxivid =       {1409.0575},
  doi =           {10.1007/s11263-015-0816-y},
  eprint =        {1409.0575},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/10.1.1.876.2726.pdf:pdf},
  issn =          {15731405},
  keywords =      {Benchmark,Dataset,Large-scale,Object detection,Object recognition}
}

@InProceedings{Saki:2019:OpenSetASC:DCASE,
  author =        {Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu},
  title =         {{Open-Set Evolving Acoustic Scene Classification System}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {219--223},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Saki, Guo, Hung - 2019 - Open-Set Evolving Acoustic Scene Classification System.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Salamon:2017:ASC:SPL,
  author =        {Salamon, Justin and Bello, Juan Pablo},
  title =         {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
  journal =       {IEEE Signal Processing Letters},
  year =          {2017},
  volume =        {24},
  number =        {3},
  pages =         {279--283},
  abstract =      {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a 'shallow' dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
  archiveprefix = {arXiv},
  arxivid =       {1608.04363},
  doi =           {10.1109/LSP.2017.2657381},
  eprint =        {1608.04363},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1608.04363.pdf:pdf},
  issn =          {10709908},
  keywords =      {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset}
}

@InProceedings{Sandler:2018:MobileNet:CVPR,
  author =        {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
  title =         {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
  booktitle =     {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year =          {2018},
  pages =         {4510--4520},
  address =       {Salt Lake City, UT, USA},
  month =         {18-23 June},
  abstract =      {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
  archiveprefix = {arXiv},
  arxivid =       {1801.04381},
  doi =           {10.1109/CVPR.2018.00474},
  eprint =        {1801.04381},
  file =          {::},
  isbn =          {9781538664209},
  issn =          {10636919}
}

@InProceedings{Seo:2019:ASC:DCASE,
  author =    {Seo, Hyeji and Park, Jihwan and Park, Yongjin},
  title =     {{Acoustic Scene Classification using Various Pre-Processed Features and Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  pages =     {3--6},
  address =   {New York, NY, USA},
  month =     {25-26 October},
  annote =    {Ensemble approach, computationally too expensive},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Seo{\_}72.pdf:pdf}
}

@Article{Sharma:2019:SoundClassification:ARXIV,
  author =        {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
  title =         {{Environment Sound Classification using Multiple Feature Channels and Deep Convolutional Neural Networks}},
  journal =       {ArXiv pre-prints},
  year =          {2019},
  volume =        {14},
  number =        {8},
  pages =         {1--11},
  abstract =      {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN). The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. Also, we employ a deeper CNN (DCNN) compared to previous models, consisting of 2D separable convolutions working on time and feature domain separately. The model also consists of max pooling layers that downsample time and feature domain separately. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (98.60{\%}), ESC-10 (97.25{\%}) and ESC-50 (95.50{\%}). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets and by a considerable margin over the previous models. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7{\%} and 81.3{\%} respectively.},
  archiveprefix = {arXiv},
  arxivid =       {1908.11219},
  eprint =        {1908.11219},
  file =          {:Users/jakobabeer/Downloads/1908.11219.pdf:pdf},
  url =           {http://arxiv.org/abs/1908.11219}
}

@Article{Sigtia:2016:PerformanceCost:IEEE_TASLP,
  author =        {Sigtia, Siddharth and Stark, Adam M. and Krstulovi{\'{c}}, Sacha and Plumbley, Mark D.},
  title =         {{Automatic Environmental Sound Recognition: Performance Versus Computational Cost}},
  journal =       {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =          {2016},
  volume =        {24},
  number =        {11},
  pages =         {2096--2107},
  abstract =      {In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.},
  doi =           {10.1109/TASLP.2016.2592698},
  file =          {::},
  issn =          {23299290},
  keywords =      {Automatic environmental sound recognition,computational auditory scene analysis,deep learning,machine learning,machine{\_}listening},
  mendeley-tags = {machine{\_}listening},
  publisher =     {IEEE}
}

@InProceedings{Singh:2019:MultiViewFeatures:DCASE,
  author =        {Singh, Arshdeep and Rajan, Padmanabhan and Bhavsar, Arnav},
  title =         {{Deep Multi-View Features from Raw Audio for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {229--233},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Rajan, Bhavsar - 2019 - Deep Multi-View Features from Raw Audio for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Singh:2018:EnsembleASC:EUSIPCO,
  author =        {Singh, Arshdeep and Thakur, Anshul and Rajan, Padmanabhan and Bhavsar, Arnav},
  title =         {{A Layer-Wise Score Level Ensemble Framework for Acoustic Scene Detection}},
  booktitle =     {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
  year =          {2018},
  pages =         {837--841},
  address =       {Rome, Italy},
  month =         {3-7 September},
  abstract =      {Scene classification based on acoustic information is a challenging task due to various factors such as the non-stationary nature of the environment and multiple overlapping acoustic events. In this paper, we address the acoustic scene classification problem using SoundNet, a deep convolution neural network, pre-trained on raw audio signals. We propose a classification strategy by combining scores from each layer. This is based on the hypothesis that layers of the deep convolutional network learn complementary information and combining this layer-wise information provides better classification than the features extracted from an individual layer. In addition, we also propose a pooling strategy to reduce the dimensionality of features extracted from different layers of SoundNet. Our experiments on DCASE 2016 acoustic scene classification dataset reveals the effectiveness of this layer-wise ensemble approach. The proposed approach provides a relative improvement of approx. 30.85{\%} over the classification accuracy provided by the best individual layer of SoundNet.},
  doi =           {10.23919/EUSIPCO.2018.8553052},
  file =          {:Users/jakobabeer/Downloads/08553052.pdf:pdf},
  isbn =          {9789082797015},
  issn =          {22195491},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Takahashi:2017:ASC:APSIPA,
  author =        {Takahashi, Gen and Yamada, Takeshi and Ono, Nobutaka and Makino, Shoji},
  title =         {{Performance Evaluation of Acoustic Scene Classification using DNN-GMM and Frame-Concatenated Acoustic Features}},
  booktitle =     {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =          {2018},
  pages =         {1739--1743},
  address =       {Honolulu, Hawaii, USA},
  month =         {2-15 November},
  abstract =      {We previously proposed a method of acoustic scene classification using a deep neural network-Gaussian mixture model (DNN-GMM) and frame-concatenated acoustic features. It was submitted to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 Challenge and was ranked eighth among 49 algorithms. In the proposed method, acoustic features in temporally distant frames were concatenated to capture their temporal relationship. The experimental results indicated that the classification accuracy is improved by increasing the number of concatenated frames. On the other hand, the frame concatenation interval, which is the interval with which the frames used for frame concatenation are selected, is another important parameter. In our previous method, the frame concatenation interval was fixed to 100 ms. In this paper, we optimize the number of concatenated frames and the frame concatenation interval for the previously proposed method. As a result, it was confirmed that the classification accuracy of the method was improved by 2.61{\%} in comparison with the result submitted to the DCASE 2016.},
  doi =           {10.1109/APSIPA.2017.8282314},
  file =          {:Users/jakobabeer/Downloads/08282314.pdf:pdf},
  isbn =          {9781538615423},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Tan:2019:EfficientNet:ICML,
  author =        {Tan, Mingxing and Le, Quoc V.},
  title =         {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
  booktitle =     {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year =          {2019},
  address =       {Long Beach, CA, USA},
  month =         {9-15 June},
  abstract =      {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  arxivid =       {1905.11946},
  eprint =        {1905.11946},
  file =          {::},
  isbn =          {9781510886988},
  url =           {http://arxiv.org/abs/1905.11946}
}

@InProceedings{Wang:2018:SelfDeterminationASC:APSIPA,
  author =        {Wang, Chien-Yao and Santoso, Andri and Wang, Jia-Ching},
  title =         {{Acoustic Scene Classification using Self-Determination Convolutional Neural Network}},
  booktitle =     {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =          {2018},
  pages =         {19--22},
  address =       {Honolulu, Hawaii, USA},
  month =         {2-15 November},
  doi =           {10.1109/APSIPA.2017.8281995},
  file =          {:Users/jakobabeer/Downloads/08281995.pdf:pdf},
  isbn =          {9781538615423},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wang:2017:ASC:ISCE,
  author =        {Wang, Chien-Yao and Wang, Jia-Ching and Wu, Yu-Chi and Chang, Pao-Chi},
  title =         {{Asymmetric Kernel Convolution Neural Networks for Acoustic Scenes Classification}},
  booktitle =     {Proceedings of the IEEE International Symposium on Consumer Electronics (ISCE)},
  year =          {2017},
  pages =         {11--12},
  address =       {Kuala Lumpur, Malaysia},
  month =         {14-15 November},
  file =          {:Users/jakobabeer/Downloads/08355533.pdf:pdf},
  isbn =          {9781538621899},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wang:2017:PCEN:ICASSP,
  author =        {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
  title =         {{Trainable Frontend for Robust and Far-Field Keyword Spotting}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {5670--5674},
  address =       {New Orleans, LA, USA},
  month =         {5-9 March},
  abstract =      {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
  archiveprefix = {arXiv},
  arxivid =       {1607.05666},
  doi =           {10.1109/ICASSP.2017.7953242},
  eprint =        {1607.05666},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45911.pdf:pdf},
  isbn =          {9781509041176},
  issn =          {15206149},
  keywords =      {automatic gain control,deep neural networks,robust and far-field speech recognition}
}

@InProceedings{Weiping:2017:SpectrogramFusion:DCASE,
  author =        {Weiping, Zheng and Jiantao, Yi and Xiaotao, Xing and Xiangtao, Liu and Shaohu, Peng},
  title =         {{Acoustic Scene Classification using Deep Convolutional Neural Networks and Multiple Spectrogram Fusions}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  address =       {Munich, Germany},
  month =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Zheng{\_}159.pdf:pdf},
  isbn =          {1299670261},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wilkinghoff:2019:OpenSetASC:DCASE,
  author =        {Wilkinghoff, Kevin and {Frank Kurth}},
  title =         {{Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {258--262},
  address =       {New York, NY, USA},
  month =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinghoff, Frank Kurth - 2019 - Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wu:2019:SoundTexture:ICASSP,
  author =        {Wu, Yuzhong and Lee, Tan},
  title =         {{Enhancing Sound Texture in CNN-based Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {815--819},
  address =       {Brighton, UK},
  month =         {12-17 May},
  archiveprefix = {arXiv},
  arxivid =       {1901.01502},
  doi =           {10.1109/ICASSP.2019.8683490},
  eprint =        {1901.01502},
  file =          {:Users/jakobabeer/Downloads/08683490 (1).pdf:pdf},
  isbn =          {9781479981311},
  issn =          {15206149},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Xia:2019:EventDetection:CSSR,
  author =   {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
  title =    {{A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection}},
  journal =  {Circuits, Systems, and Signal Processing},
  year =     {2019},
  number =   {38},
  pages =    {3433?3453},
  abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
  doi =      {10.1007/s00034-019-01094-1},
  issn =     {15315878},
  keywords = {Acoustic event detection,Deep learning,Strongly labeled,Weakly labeled}
}

@InProceedings{Xu:2018:ASCMobileNet:ISM,
  author =        {Xu, Jun-Xiang and Lin, Tzu-Ching and Yu, Tsai-Ching and Tai, Tzu-Chiang and Chang, Pao-Chi},
  title =         {{Acoustic Scene Classification Using Reduced MobileNet Architecture}},
  booktitle =     {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
  year =          {2018},
  pages =         {267--270},
  address =       {Taichung, Taiwan},
  month =         {10-12 December},
  doi =           {10.1109/ISM.2018.00038},
  file =          {:Users/jakobabeer/Downloads/08603300.pdf:pdf},
  isbn =          {9781538668573},
  journal =       {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Xu:2016:HierarchicalASC:DCASE,
  author =        {Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.},
  title =         {{Hierarchical Learning for DNN-Based Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Xu-a-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Yang:2018:MultiScaleFeatures:DCASE,
  author =        {Yang, Liping and Chen, Xinxing and Tao, Lianjie},
  title =         {{Acoustic Scene Classification using Multi-Scale Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Chen, Tao - 2018 - Acoustic Scene Classification using Multi-Scale Features.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Ye:2018:ASC:AS,
  author =   {Ye, Jiaxing and Kobayashi, Takumi and Toyama, Nobuyuki and Tsuda, Hiroshi and Murakawa, Masahiro},
  title =    {{Acoustic scene classification using efficient summary statistics and multiple spectro-temporal descriptor fusion}},
  journal =  {Applied Sciences},
  year =     {2018},
  volume =   {8},
  number =   {8},
  pages =    {1--12},
  abstract = {This paper presents a novel approach for acoustic scene classification based on efficient acoustic feature extraction using spectro-temporal descriptors fusion. Grounded on the finding in neuroscience-"auditory system summarizes the temporal details of sounds using time-averaged statistics to understand acoustic scenes", we devise an efficient computational framework for sound scene classification by using multipe time-frequency descriptors fusion with discriminant information enhancement. To characterize rich information of sound, i.e., local structures on the time-frequency plane, we adopt 2-dimensional local descriptors. A more critical issue raised in how to logically 'summarize' those local details into a compact feature vector for scene classification. Although 'time-averaged statistics' is suggested by the psychological investigation, directly computing time average of local acoustic features is not a logical way, since arithmetic mean is vulnerable to extreme values which are anticipated to be generated by interference sounds which are irrelevant to the scene category. To tackle this problem, we develop time-frame weighting approach to enhance sound textures as well as to suppress scene-irrelevant events. Subsequently, robust acoustic feature for scene classification can be efficiently characterized. The proposed method had been validated by using Rouen dataset which consists of 19 acoustic scene categories with 3029 real samples. Extensive results demonstrated the effectiveness of the proposed scheme.},
  doi =      {10.3390/app8081363},
  file =     {:Users/jakobabeer/Downloads/applsci-08-01363.pdf:pdf},
  issn =     {20763417},
  keywords = {Acoustic scene classification,Convex combination,Local descriptor,Summary statistics,Time-frequency analysis}
}

@InProceedings{Zoehrer:2016:GRN_ASC:DCASE,
  author =        {Z{\"{o}}hrer, Matthias and Pernkopf, Franz},
  title =         {{Gated Recurrent Networks Applied to Acoustic Scene Classification and Acoustic Event Detection}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  address =       {Budapest, Hungary},
  month =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Zohrer-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Zeinali:2018:XVektorEmbeddings:DCASE,
  author =        {Zeinali, Hossein and Burget, Luk{\'{a}}s and Cernocky, Jan},
  title =         {{Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Challenge}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  address =       {Surrey, UK},
  month =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zeinali, Burget, Cernocky - 2018 - Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Cha.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Zhang:2018:Mixup:ICLR,
  author =        {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  title =         {{mixup: Beyond Empirical Risk Minimization}},
  booktitle =     {Proceedings of the International Conference on Learning Representations (ICLR)},
  year =          {2018},
  address =       {Vancouver, Canada},
  month =         {30 April - 3 May},
  abstract =      {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  arxivid =       {1710.09412},
  eprint =        {1710.09412},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:pdf}
}

@Article{Zhong:2017:RandomErasing:ARXIV,
  author =        {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  title =         {{Random Erasing Data Augmentation}},
  journal =       {ArXiv pre-prints},
  year =          {2017},
  abstract =      {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  archiveprefix = {arXiv},
  arxivid =       {1708.04896},
  eprint =        {1708.04896},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1708.04896.pdf:pdf},
  keywords =      {data{\_}augmentation},
  mendeley-tags = {data{\_}augmentation},
  url =           {http://arxiv.org/abs/1708.04896}
}

@InProceedings{Zielinski:2018:BinauralASC:FEDCSIS,
  author =        {Zieli{\'{n}}ski, S{\l}awomir K. and Lee, Hyunkook},
  title =         {{Feature Extraction of Binaural Recordings for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Federated Conference on Computer Science and Information Systems (FedCSIS)},
  year =          {2018},
  pages =         {585--588},
  address =       {Pozna{\'{n}}, Poland},
  month =         {9-12 September},
  abstract =      {Binaural technology becomes increasingly popular in the multimedia systems. This paper identifies a set of features of binaural recordings suitable for the automatic classification of the four basic spatial audio scenes representing the most typical patterns of audio content distribution around a listener. Moreover, it compares the five artificial-intelligence-based methods applied to the classification of binaural recordings. The results show that both the spatial and the spectro-temporal features are essential to accurate classification of binaurally rendered acoustic scenes. The spectro-temporal features appear to have a stronger influence on the classification results than the spatial metrics. According to the obtained results, the method based on the support vector machine, exploiting the features identified in the study, yields the classification accuracy approaching 84{\%}.},
  doi =           {10.15439/2018F182},
  file =          {:Users/jakobabeer/Downloads/08511268.pdf:pdf},
  isbn =          {9788394941970},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Book{Virtanen:2018:SoundSceneBook:BOOK,
  title =     {{Computational Analysis of Sound Scenes and Events}},
  publisher = {Springer International Publishing},
  year =      {2018},
  editor =    {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
  address =   {Cham, Switzerland},
  doi =       {10.1007/978-3-319-63450-0},
  file =      {:Users/jakobabeer/Sync/Jakob/Knowhow/Books/Ellis, Dan{\_} Plumbley, Mark D.{\_} Virtanen, Tuomas-Computational analysis of sound scenes and events-Springer (2018).pdf:pdf},
  url =       {http://link.springer.com/10.1007/978-3-319-63450-0}
}
