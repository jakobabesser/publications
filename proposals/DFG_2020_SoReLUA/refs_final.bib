@Book{Virtanen:2018:SoundSceneBook:BOOK,
  title =     {{Computational Analysis of Sound Scenes and Events}},
  publisher = {Springer International Publishing},
  year =      {2018},
  editor =    {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
  organization =   {Cham, Switzerland},
  xdoi =       {10.1007/978-3-319-63450-0},
  file =      {:Users/jakobabeer/Sync/Jakob/Knowhow/Books/Ellis, Dan{\_} Plumbley, Mark D.{\_} Virtanen, Tuomas-Computational analysis of sound scenes and events-Springer (2018).pdf:pdf},
  xurl =       {http://link.springer.com/10.1007/978-3-319-63450-0}
}

@article{Barchiesi:2015:ASC:SPM,
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {Barchiesi, Daniele and Giannoulis, D. Dimitrios and Stowell, Dan and Plumbley, Mark D.},
xdoi = {10.1109/MSP.2014.2326181},
file = {:C$\backslash$:/Users/abr/Downloads/BarchiesiGiannoulisStowellP15-asc{\_}accepted.pdf:pdf},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
number = {3},
pages = {16--34},
title = {{Acoustic Scene Classification: Classifying environments from the sounds they produce}},
volume = {32},
year = {2015}
}

@article{Abesser:2020:ASC:AS,
author = {Abe{\ss}er, Jakob},
xdoi = {10.3390/app10062020},
file = {:C$\backslash$:/Users/abr/Downloads/applsci-10-02020.pdf:pdf},
journal = {applied sciences},
keywords = {abt-md,acoustic scene classification,deep neural networks,idmt,machine listening},
mendeley-tags = {abt-md,acoustic scene classification,idmt,machine listening},
number = {6},
title = {{A Review of Deep Learning Based Methods for Acoustic Scene Classification}},
volume = {10},
year = {2020}
}

@article{Xia:2019:EventDetection:CSSR,
abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
author = {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
xdoi = {10.1007/s00034-019-01094-1},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia et al. - 2019 - A Survey Neural Network-Based Deep Learning for Acoustic Event Detection.pdf:pdf},
issn = {15315878},
journal = {Circuits, Systems, and Signal Processing},
keywords = {Acoustic event detection,Deep learning,Strongly labeled,Weakly labeled},
title = {{A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection}},
year = {2019}
}

@inproceedings{Dang:2017:SurveyAED:ICOT,
abstract = {Deep learning has achieved state of the art in various machine learning problems, such as computer vision, speech recognition, and natural language processing. Sound event detection (SED), which is about recognizing audio events in real-life environments, has attracted a lot of attention recently. Many works have been successful when applying deep learning techniques for the SED problem as can be seen in Detection and Classification of Acoustic Scenes and Events (DCASE) challenge 2016-2017. In this paper, we present a review of the SED problem and discuss different deep learning approaches for the problem.},
address = {Singapore, Singapore},
author = {Dang, An and Vu, Toan H. and Wang, Jia Ching},
booktitle = {Proceedings of the International Conference on Orange Technologies (ICOT)},
xdoi = {10.1109/ICOT.2017.8336092},
file = {:C$\backslash$:/Users/abr/Downloads/08336092.pdf:pdf},
keywords = {Convolutional neural networks,Deep learning,Neural networks,Recurrent neural networks,Sound event detection},
pages = {75--78},
title = {{A survey of Deep Learning for Polyphonic Sound Event Detection}},
year = {2017},
note = {8 - 10 December}
}

@inproceedings{Hou:2019:SoundEvent:ICASSP,
author = {Hou, Yuanbo and Kong, Qiuqiang and Li, Shengchen and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/icassp.2019.8683627},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {46--50},
title = {{Sound Event Detection with Sequentially Labelled Data Based on Connectionist Temporal Classification and Unsupervised Clustering}},
year = {2019}
}

@article{Cakir:2018:EndtoEndAED:IJCNN,
abstract = {Sound event detection systems typically consist of two stages: Extracting hand-crafted features from the raw audio waveform, and learning a mapping between these features and the target sound events using a classifier. Recently, the focus of sound event detection research has been mostly shifted to the latter stage using standard features such as mel spectrogram as the input for classifiers such as deep neural networks. In this work, we utilize end-to-end approach and propose to combine these two stages in a single deep neural network classifier. The feature extraction over the raw waveform is conducted by a feedforward layer block, whose parameters are initialized to extract the time-frequency representations. The feature extraction parameters are updated during training, resulting with a representation that is optimized for the specific task. This feature extraction block is followed by (and jointly trained with) a convolutional recurrent network, which has recently given state-of-the-art results in many sound recognition tasks. The proposed system does not outperform a convolutional recurrent network with fixed hand-crafted features. The final magnitude spectrum characteristics of the feature extraction block parameters indicate that the most relevant information for the given task is contained in 0 - 3 kHz frequency range, and this is also supported by the empirical results on the SED performance.},
archivePrefix = {arXiv},
arxivId = {1805.03647},
author = {Cakir, Emre and Virtanen, Tuomas},
xdoi = {10.1109/IJCNN.2018.8489470},
eprint = {1805.03647},
file = {::},
isbn = {9781509060146},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {convolutional recurrent neural networks,end-to-end,feature learning,neural networks},
title = {{End-to-End Polyphonic Sound Event Detection Using Convolutional Recurrent Neural Networks with Learned Time-Frequency Representation Input}},
volume = {2018-July},
year = {2018}
}

@article{Lostanlen:2018:PCEN:SPL,
abstract = {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named per-channel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This letter investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Second, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Third, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise, PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
author = {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
xdoi = {10.1109/LSP.2018.2878620},
file = {::},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Acoustic noise,acoustic sensors,acoustic signal detection,machine{\_}listening,signal classification,spectrogram},
mendeley-tags = {machine{\_}listening},
number = {1},
pages = {39--43},
title = {{Per-channel energy normalization: Why and how}},
volume = {26},
year = {2019}
}

@inproceedings{Arora2017,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)},
xdoi = {10.1109/MMSP.2017.8122258},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario(4).pdf:pdf},
isbn = {978-1-5090-3649-3},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{A study on transfer learning for acoustic event detection in a real life scenario}},
xurl = {http://ieeexplore.ieee.org/document/8122258/},
volume = {2017-Janua},
year = {2017}
}

@inproceedings{Cramer2019:openl3,
address = {Brighton, United Kingdom},
author = {Cramer, Jason and Wu, Ho-hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {IEEE ICASSP},
xdoi = {10.1109/ICASSP.2019.8682475},
isbn = {978-1-4799-8131-1},
pages = {3852--3856},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}},
year = {2019}
}


@inproceedings{Kumar2018:embedding,
author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
booktitle = {IEEE ICASSP},
isbn = {978-1-5386-4658-8},
issn = {15206149},
pages = {326--330},
title = {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
year = {2018}
}

@article{Hershey:2017:CNN:ICASSP,
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.09430v2},
author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P.W. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
xdoi = {10.1109/ICASSP.2017.7952132},
eprint = {arXiv:1609.09430v2},
file = {::},
isbn = {9781509041176},
issn = {15206149},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification},
month = {mar},
pages = {131--135},
publisher = {IEEE},
title = {{CNN architectures for large-scale audio classification}},
xurl = {http://ieeexplore.ieee.org/document/7952132/},
year = {2017}
}

@article{Favory:2020:COALA:ARXIV,
abstract = {Audio representation learning based on deep neural networks (DNNs) emerged as an alternative approach to hand-crafted features. For achieving high performance, DNNs often need a large amount of annotated data which can be difficult and costly to obtain. In this paper, we propose a method for learning audio representations, aligning the learned latent representations of audio and associated tags. Aligning is done by maximizing the agreement of the latent representations of audio and tags, using a contrastive loss. The result is an audio embedding model which reflects acoustic and semantic characteristics of sounds. We evaluate the quality of our embedding model, measuring its performance as a feature extractor on three different tasks (namely, sound event recognition, and music genre and musical instrument classification), and investigate what type of characteristics the model captures. Our results show that our method is in par with the state-of-the-art in the considered tasks and the embeddings produced with our method are well correlated with some acoustic descriptors.},
archivePrefix = {arXiv},
arxivId = {2006.08386},
author = {Favory, Xavier and Drossos, Konstantinos and Virtanen, Tuomas and Serra, Xavier},
eprint = {2006.08386},
file = {::},
title = {{COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio Representations}},
xurl = {http://arxiv.org/abs/2006.08386},
year = {2020}
}

@inproceedings{Grollmisch2020:embeddings,
abstract = {In the context of deep learning, the availability of large amounts of training data can play a critical role in a model's performance. Recently, several models for audio classification have been pre-trained in a supervised or self-supervised fashion on large datasets to learn complex feature representations, so- called embeddings. These embeddings can then be extracted from smaller datasets and used to train subsequent classifiers. In the field of audio event detection (AED) for example, classifiers using these features have achieved high accuracy without the need of additional domain knowledge. This paper evaluates three state-of-the-art embeddings on six audio classification tasks from the fields of music information retrieval and industrial sound analysis. The embeddings are systematically evaluated by analyzing the influence on classification accuracy of classifier architecture, fusion methods for file-wise predictions, amount of training data, and initial training domain of the embeddings. To better understand the impact of the pre-training step, results are also compared with those acquired with models trained from scratch. On average, the OpenL3 embeddings performed best with a linear SVM classifier. For a reduced amount of training examples, OpenL3 outperforms the initial baseline.},
address = {Amsterdam, The Netherlands},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Kehling, Christian and Taenzer, Michael},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/goh/Desktop/eusipco{\_}2020{\_}embeddings.pdf:pdf},
title = {{Analyzing the Potential of Pre-Trained Embeddings for Audio Classification Tasks}},
year = {2020}
}

@inproceedings{Xu:2018:WeaklyLabeledEventDetection:ICASSP,
abstract = {In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classification, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labeled with one or a few audio tags but without timestamps of the audio events, which is called as weakly labeled data. Two sub-tasks are defined in this challenge including audio tagging and sound event detection using this weakly labeled data. A convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram is proposed. In addition, a temporal attention method is proposed along the frames to predicate the locations of each audio event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6$\backslash${\%} and Equal error 0.73, respectively.},
address = {Calgary, AB, Canada},
annote = {? how long are the input audio frames (10s?)},
archivePrefix = {arXiv},
arxivId = {1710.00343},
author = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2018.8461975},
eprint = {1710.00343},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {Attention,Audio tagging,DCASE2017 challenge,Gated linear unit,Weakly supervised sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
pages = {121--125},
title = {{Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network}},
year = {2018}
}

@InProceedings{Koutini:2019:ReceptiveField:DCASE,
  author =        {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
  title =         {{Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {124--128},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Koutini, Eghbal-zadeh, Widmer - 2019 - Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}


@article{Li:2020:DilatedCRNN:ARXIV,
abstract = {Convolutional recurrent neural networks (CRNNs) have achieved state-of-the-art performance for sound event detection (SED). In this paper, we propose to use a dilated CRNN, namely a CRNN with a dilated convolutional kernel, as the classifier for the task of SED. We investigate the effectiveness of dilation operations which provide a CRNN with expanded receptive fields to capture long temporal context without increasing the amount of CRNN's parameters. Compared to the classifier of the baseline CRNN, the classifier of the dilated CRNN obtains a maximum increase of 1.9{\%}, 6.3{\%} and 2.5{\%} at F1 score and a maximum decrease of 1.7{\%}, 4.1{\%} and 3.9{\%} at error rate (ER), on the publicly available audio corpora of the TUTSED Synthetic 2016, the TUT Sound Event 2016 and the TUT Sound Event 2017, respectively.},
author = {Li, Yanxiong and Liu, Mingle and Drossos, Konstantinos and Virtanen, Tuomas},
xdoi = {10.1109/icassp40776.2020.9054433},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks.pdf:pdf},
isbn = {6191101570},
pages = {286--290},
title = {{Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks}},
year = {2020}
}

@article{Drossos:2020:SED:ARXIV,
abstract = {State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85{\%} and average training time per epoch by 78{\%}, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6{\%} and 3.8{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {2002.00476},
author = {Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas},
eprint = {2002.00476},
file = {:X$\backslash$:/knowhow/publica{\_}exports/new{\_}publications/mis{\_}ijcnn{\_}2020.pdf:pdf},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Sound Event Detection with Depthwise Separable and Dilated Convolutions}},
xurl = {http://arxiv.org/abs/2002.00476},
year = {2020}
}

@article{hinton2006autoencoders,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}


@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}

@inproceedings{baldi2012autoencoders,
  title={Autoencoders, unsupervised learning, and deep architectures},
  author={Baldi, Pierre},
  booktitle={Proceedings of ICML workshop on unsupervised and transfer learning},
  pages={37--49},
  year={2012}
}

@article{Hayashi2020,
author = {Hayashi, Tomoki and Komatsu, Tatsuya and Kondo, Reishi and Toda, Tomoki and Takeda, Kazuya},
xdoi = {10.23919/EUSIPCO.2018.8553423},
file = {::},
isbn = {9789082797015},
issn = {22195491},
journal = {ICASSP 2020},
keywords = {Anomalous sound event detection,Anomaly detection,Neural network,WaveNet},
pages = {2494--2498},
publisher = {IEEE},
title = {{ANOMALOUS SOUND DETECTION BASED ON INTERPOLATION DEEP NEURAL NETWORK Kaori}},
volume = {2018-Septe},
year = {2020}
}

@article{Chalapathy:2019:AD:ARXIV,
abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
archivePrefix = {arXiv},
arxivId = {1901.03407},
author = {Chalapathy, Raghavendra and Chawla, Sanjay},
eprint = {1901.03407},
file = {::},
month = {jan},
title = {{Deep Learning for Anomaly Detection: A Survey}},
xurl = {http://arxiv.org/abs/1901.03407},
year = {2019}
}

@inproceedings{Gemmeke:2017:Audioset:ICASSP,
address = {New Orleans, LA, USA},
author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
pages = {776--780},
title = {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
year = {2017}
}

@article{Fonseca:2020:FSD50K:ARXIV,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.00475v1},
author = {Fonseca, Eduardo and Member, Student and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
eprint = {arXiv:2010.00475v1},
file = {:C$\backslash$:/Users/abr/Downloads/2010.00475.pdf:pdf},
number = {8},
pages = {1--21},
title = {{FSD50K : an Open Dataset of Human-labeled Sound Events}},
volume = {14},
year = {2020}
}

@InProceedings{Grollmisch:2019:ISA:EUSIPCO,
  author =        {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
  title =         {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis (ISA)}},
  booktitle =     {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
  year =          {2019},
  pages =         {1--5},
  organization =       {A Coruna, Spain},
  note =         {2-6 September},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Grollmisch{\_}2019{\_}EUSIPCO.pdf:pdf},
  keywords =      {idmt},
  mendeley-tags = {idmt}
}

@article{Koizumi2019d,
abstract = {This paper introduces a new dataset called "ToyADMOS" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at https://github.com/YumaKoizumi/ToyADMOS-dataset},
archivePrefix = {arXiv},
arxivId = {1908.03299},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Harada, Noboru and Imoto, Keisuke},
eprint = {1908.03299},
file = {::},
number = {Waspaa},
pages = {2--7},
title = {{ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection}},
xurl = {http://arxiv.org/abs/1908.03299},
year = {2019}
}

@article{Purohit2019a,
abstract = {Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388},
archivePrefix = {arXiv},
arxivId = {1909.09347},
author = {Purohit, Harsh and Tanabe, Ryo and Ichige, Kenji and Endo, Takashi and Nikaido, Yuki and Suefusa, Kaori and Kawaguchi, Yohei},
eprint = {1909.09347},
file = {::},
month = {sep},
title = {{MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection}},
xurl = {http://arxiv.org/abs/1909.09347},
year = {2019}
}

@article{Salamon:2017:ASC:SPL,
abstract = {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a 'shallow' dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
archivePrefix = {arXiv},
arxivId = {1608.04363},
author = {Salamon, Justin and Bello, Juan Pablo},
xdoi = {10.1109/LSP.2017.2657381},
eprint = {1608.04363},
file = {::},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset},
number = {3},
pages = {279--283},
title = {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
volume = {24},
year = {2017}
}

@article{Xu:2018:ASCMobileNet:ISM,
author = {Xu, Jun-Xiang and Lin, Tzu-Ching and Yu, Tsai-Ching and Tai, Tzu-Chiang and Chang, Pao-Chi},
xdoi = {10.1109/ISM.2018.00038},
file = {::},
isbn = {9781538668573},
journal = {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {267--270},
title = {{Acoustic Scene Classification Using Reduced MobileNet Architecture}},
year = {2018}
}

@article{Park:2019:SpecAugment:INTERSPEECH,
abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
archivePrefix = {arXiv},
arxivId = {1904.08779},
author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
xdoi = {10.21437/Interspeech.2019-2680},
eprint = {1904.08779},
file = {:C$\backslash$:/Users/abr/Downloads/1904.08779.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Data augmentation,End-to-end speech recognition},
pages = {2613--2617},
title = {{Specaugment: A simple data augmentation method for automatic speech recognition}},
volume = {2019-Septe},
year = {2019}
}

@Article{Zhong:2017:RandomErasing:ARXIV,
  author =        {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  title =         {{Random Erasing Data Augmentation}},
  journal =       {ArXiv pre-prints},
  year =          {2017},
  abstract =      {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  archiveprefix = {arXiv},
  arxivid =       {1708.04896},
  eprint =        {1708.04896},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1708.04896.pdf:pdf},
  keywords =      {data{\_}augmentation},
  mendeley-tags = {data{\_}augmentation},
  xurl =           {http://arxiv.org/abs/1708.04896}
}

@inproceedings{Zhang:2018:Mixup:ICLR,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By xdoing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1710.09412},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:pdf},
title = {{mixup: Beyond Empirical Risk Minimization}},
year = {2018}
}

@InProceedings{Gharib:2018:DomainAdaptationASC:DCASE,
  author =        {Gharib, Shayan and Drossos, Konstantinos and Emre, Cakir and Serdyuk, Dmitriy and Virtanen, Tuomas},
  title =         {{Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Gharib et al. - 2018 - Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
  mendeley-tags = {acoustic{\_}scene{\_}classification,domain{\_}adaptation}
}

@InProceedings{Mun:2019:DomainMismatch:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon},
  title =         {{Domain Mismatch Robust Acoustic Scene Classification Using Channel Information Conversion}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {845--849},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  abstract =      {In recent acoustic scene classification (ASC) research field, training and test device channel mismatch have become an issue for the real world implementation. To address the issue, this paper proposes a channel domain conversion using factor-ized hierarchical variational autoencoder. Proposed method adapts both the source and target domain to a pre-defined specific domain. Unlike the conventional approach, the relationship between the target and source domain and information of each domain are not required in the adaptation process. Based on the experimental results using the IEEE Detection and Classification of Acoustic Scenes and Event 2018 task 1-B dataset and the baseline system, it is shown that the proposed approach can mitigate the channel mismatching issue of different recording devices.},
  xdoi =           {10.1109/ICASSP.2019.8683514},
  file =          {:Users/jakobabeer/Downloads/08683514.pdf:pdf},
  isbn =          {9781479981311},
  issn =          {15206149},
  keywords =      {acoustic scene classification,acoustic{\_}scene{\_}classification,domain adaptation,factorized hierarchical variational autoencoder,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@inproceedings{Drossos:2019:DomainAdaptation:WASPAA,
abstract = {A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of $\Delta$-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\%} to 45{\%}, using the TUT Acoustic Scenes dataset.},
address = {New Paltz, NY, USA},
author = {Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {:C$\backslash$:/Users/abr/Downloads/08937231.pdf:pdf},
keywords = {Acoustic scene classification,Wasserstein distance,acoustic{\_}scene{\_}classificaiton,adversarial training,domain{\_}adaptation,machine{\_}listening,unsupervised domain adaptation},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,domain{\_}adaptation,machine{\_}listening},
pages = {259--263},
publisher = {IEEE},
title = {{Unsupervised Adversarial Domain Adaptation based on the Wasserstein Distance for Acoustic Scene Classification}},
year = {2019}
}

@inproceedings{Goodfellow:2014:GAN:NIPS,
abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and {Yoshua Bengio}},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {::},
issn = {10495258},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
year = {2014}
}

@InProceedings{Kong:2019:SceneGeneration:ICASSP,
  author =    {Kong, Qiuqiang and Xu, Yong and Iqbal, Turab and Cao, Yin and Wang, Wenwu and Plumbley, Mark D},
  title =     {{Acoustic Scene Generation with Conditional SampleRNN}},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2019},
  pages =     {925--929},
  organization =   {Brighton, UK},
  note =     {12-17 May},
  file =      {::},
  isbn =      {9781538646588}
}

@InProceedings{Mun:2017:ASC:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Han, David K. and Ko, Hanseok},
  title =         {{Deep Neural Network Based Learning and Transferring Mid-Level Audio Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {796--800},
  organization =       {New Orleans, LA, USA},
  note =         {5-9 March},
  abstract =      {Deep Neural Network (DNN) based transfer learning has been shown to be effective in Visual Object Classification (VOC) for complementing the deficit of target domain training samples by adapting classifiers that have been pre- trained for other large-scaled DataBase (DB). Although there exists an abundance of acoustic data, it can also be said that datasets of specific acoustic scenes are sparse for training Acoustic Scene Classification (ASC) models. By exploiting VOC DNN‟s ability of learning beyond its pre- trained environments, this paper proposes DNN based transfer learning for ASC. Effectiveness of the proposed method is demonstrated on the database of IEEE DCASE Challenge 2016 Task 1 and home surveillance environment via representative experiments. Its improved performance is verified by comparing it to prominent conventional methods.},
  xdoi =           {10.1097/IOP.0000000000000348},
  file =          {:Users/jakobabeer/Downloads/07952265.pdf:pdf},
  isbn =          {9781509041176},
  issn =          {15372677},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}


@inproceedings{Chen:2019:ASC:DCASE,
author = {Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling.pdf:pdf},
title = {{Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling}},
year = {2019}
}

@InProceedings{Mun:2017:GANASC:DCASE,
  author =        {Mun, Seongkyu and Park, Sangwook and Han, David K. and Ko, Hanseok},
  title =         {{Generative Adversarial Networks based Acoustic Scene Training Set Augmentation and Selection using SVM Hyperplane}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Mun{\_}215.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@article{salakhutdinov2015learning,
  title={Learning deep generative models},
  author={Salakhutdinov, Ruslan},
  journal={ANNU REV STAT APPL},
  volume={2},
  pages={361--385},
  year={2015},
  publisher={Annual Reviews}
}

@inproceedings{kingma2013vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  xjournal={International Conference on Learning Representations (ICLR'14)},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{rezende2014vae,
  title={Stochastic backpropagation and variational inference in deep latent gaussian models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  xbooktitle={International Conference on Machine Learning (ICML'14)},
  booktitle={ICML},
  year={2014}
}

@inproceedings{vandenoord2017vqvae,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and others},
  booktitle={NeurIPS},
  pages={6306--6315},
  year={2017}
}

@inproceedings{gregor2018tdvae,
  title={Temporal Difference Variational Auto-Encoder},
  author={Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  zbooktitle={International Conference on Learning Representations (ICLR'18)},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{razavi2019vqvae2,
  title={Generating diverse high-fidelity images with {VQ-VAE-2}},
  author={Razavi, Ali and v d Oord, Aaron and Vinyals, Oriol},
  xbooktitle={Advances in Neural Information Processing Systems (NeurIPS'19)},
  booktitle={NeurIPS},
  pages={14866--76},
  year={2019}
}

@inproceedings{engel2020ddsp,
  title={{DDSP}: Differentiable Digital Signal Processing},
  author={Jesse Engel and Lamtharn Hantrakul and Chenjie Gu and Adam Roberts},
  booktitle={ICLR},
  year={2020}
}

@article{stober2015arXiv:1511.04306,
  author = {Sebastian Stober and Avital Sternin and Adrian M. Owen and Jessica A. Grahn},
  title = {Deep Feature Learning for {EEG} Recordings},
  journal = {arXiv preprint arXiv:1511.04306},
  year = {2015},
  note = {submitted as conference paper for ICLR 2016},
  xurl = {http://arxiv.org/abs/1511.04306}
}

@article{tsai2020demystifying,
  title={Demystifying Self-Supervised Learning: An Information-Theoretical Framework},
  author={Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2006.05576},
  year={2020}
}

@InProceedings{saunshi19contrastive,
  title =    {A Theoretical Analysis of Contrastive Unsupervised Representation Learning},
  author =   {Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {5628--5637},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  address =    {Long Beach, California, USA},
  month =    {06},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v97/saunshi19a/saunshi19a.pdf},
  xurl =    {http://proceedings.mlr.press/v97/saunshi19a.html},
  abstract =   {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically “similar" data points and “negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term \emph{contrastive learning} for such algorithms and presents a theoretical framework for analyzing them by introducing \emph{latent classes} and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{tung2017self,
  title={Self-supervised learning of motion capture},
  author={Tung, Hsiao-Yu and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina},
  xbooktitle={Advances in Neural Information Processing Systems (NeurIPS'17)},
  booktitle={NeurIPS},
  pages={5236--5246},
  year={2017}
}

@inproceedings{zhai2019s4l,
  title={S4l: Self-supervised semi-supervised learning},
  author={Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1476--1485},
  year={2019}
}

@InProceedings{dwibedi2019temporal,
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
title = {Temporal Cycle-Consistency Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {06},
year = {2019}
}


@article{jing2020self,
  title={Self-supervised visual feature learning with deep neural networks: A survey},
  author={Jing, Longlong and Tian, Yingli},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}


@INPROCEEDINGS{sermanet2018timecontrastive,
  author={P. {Sermanet} and C. {Lynch} and Y. {Chebotar} and J. {Hsu} and E. {Jang} and S. {Schaal} and S. {Levine} and G. {Brain}},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title={Time-Contrastive Networks: Self-Supervised Learning from Video},
  year={2018},
  volume={},
  number={},
  pages={1134-1141},
  abstract={We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.},
  keywords={image representation;learning (artificial intelligence);pose estimation;robot programming;robot vision;video signal processing;time-contrastive networks;robotic behaviors;robotic imitation settings;human poses;viewpoint-invariant representation;end-effectors;reinforcement learning algorithm;self-supervised learning;robotic systems;Robots;Task analysis;Visualization;Learning (artificial intelligence);Training;Liquids;Lighting},
  xdoi={10.1109/ICRA.2018.8462891},
  ISSN={2577-087X},
  month={05},}

  @inproceedings{lee2019making,
    title={Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
    author={Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette},
    booktitle={2019 International Conference on Robotics and Automation (ICRA)},
    pages={8943--8950},
    year={2019},
    organization={IEEE}
  }

  @article{huang2011predictive,
    title={Predictive coding},
    author={Huang, Yanping and Rao, Rajesh PN},
    journal={Wiley Interdisciplinary Reviews: Cognitive Science},
    volume={2},
    number={5},
    pages={580--593},
    year={2011},
    publisher={Wiley Online Library}
  }

  @InProceedings{rane2020icmr,
    author    = {Rane, Roshan Prakash and Sz\"{u}gyi, Edit and Saxena, Vageesh and Ofner, Andr\'{e} and Stober, Sebastian},
    booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
    title     = {PredNet and Predictive Coding: A Critical Review},
    year      = {2020},
    address   = {New York, NY, USA},
    pages     = {233–241},
    publisher = {Association for Computing Machinery},
    series    = {ICMR ’20},
    xdoi       = {10.1145/3372278.3390694},
    isbn      = {9781450370875},
    keywords  = {semi-supervised, convolutional neural networks, predictive coding, video prediction, deep learning, video classification},
    location  = {Dublin, Ireland},
    numpages  = {9},
    timestamp = {2020.07.28},
    xurl       = {https://xdoi.org/10.1145/3372278.3390694},
  }

  @InProceedings{ofner2020smc,
  author    = {André Ofner and Sebastian Stober},
  booktitle = {IEEE International Conference on Systems, Man, and Cybernetics (SMC 2020)},
  title     = {Balancing Active Inference and Active Learning with Deep Variational Predictive Coding for {EEG}},
  year      = {2020},
  timestamp = {2020.07.28},
}


@article{krug2020gradient,
  title={Gradient-Adjusted Neuron Activation Profiles for Comprehensive Introspection of Convolutional Speech Recognition Models},
  author={Krug, Andreas and Stober, Sebastian},
  journal={arXiv preprint arXiv:2002.08125},
  year={2020}
}



@Article{Russakovsky:2015:ImageNet:IJCV,
  author =        {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title =         {{ImageNet Large Scale Visual Recognition Challenge}},
  journal =       {International Journal of Computer Vision},
  year =          {2015},
  volume =        {115},
  number =        {3},
  pages =         {211--252},
  abstract =      {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  arxivid =       {1409.0575},
  xdoi =           {10.1007/s11263-015-0816-y},
  eprint =        {1409.0575},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/10.1.1.876.2726.pdf:pdf},
  issn =          {15731405},
  keywords =      {Benchmark,Dataset,Large-scale,Object detection,Object recognition}
}


@book{Mueller:2015:MusicProcessing:BOOK,
author = {M{\"{u}}ller, Meinard},
isbn = {978-3-319-21944-8},
publisher = {Springer},
title = {{Fundamentals of Music Processing}},
year = {2015}
}

@article{McCallum:2019:Segmentation:ICASSP,
abstract = {Music segmentation refers to the dual problem of identifying boundaries between, and labeling, distinct music segments, e.g., the chorus, verse, bridge etc. in popular music. The performance of a range of music segmentation algorithms has been shown to be dependent on the audio features chosen to represent the audio. Some approaches have proposed learning feature transformations from music segment annotation data, although, such data is time consuming or expensive to create and as such these approaches are likely limited by the size of their datasets. While annotated music segmentation data is a scarce resource, the amount of available music audio is much greater. In the neighboring field of semantic audio unsupervised deep learning has shown promise in improving the performance of solutions to the query-by-example and sound classification tasks. In this work, unsupervised training of deep feature embeddings using convolutional neural networks (CNNs) is explored for music segmentation. The proposed techniques exploit only the time proximity of audio features that is implicit in any audio timeline. Employing these embeddings in a classic music segmentation algorithm is shown not only to significantly improve the performance of this algorithm, but obtain state of the art performance in unsupervised music segmentation.},
author = {Mccallum, Matthew C.},
xdoi = {10.1109/ICASSP.2019.8683407},
file = {:C$\backslash$:/Users/abr/Downloads/08683407.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Acoustic signal processing,Convolutional neural network,Deep learning,Music information retrieval},
pages = {346--350},
title = {{Unsupervised Learning of Deep Features for Music Segmentation}},
year = {2019}
}

@article{Zhao:2020:ActiveLearningSED:ARXIV,
abstract = {This paper proposes an active learning system for sound event detection (SED). It aims at maximizing the accuracy of a learned SED model with limited annotation effort. The proposed system analyzes an initially unlabeled audio dataset, from which it selects sound segments for manual annotation. The candidate segments are generated based on a proposed change point detection approach, and the selection is based on the principle of mismatch-first farthest-traversal. During the training of SED models, recordings are used as training inputs, preserving the long-term context for annotated segments. The proposed system clearly outperforms reference methods in the two datasets used for evaluation (TUT Rare Sound 2017 and TAU Spatial Sound 2019). Training with recordings as context outperforms training with only annotated segments. Mismatch-first farthest-traversal outperforms reference sample selection methods based on random sampling and uncertainty sampling. Remarkably, the required annotation effort can be greatly reduced on the dataset where target sound events are rare: by annotating only 2{\%} of the training data, the achieved SED performance is similar to annotating all the training data.},
archivePrefix = {arXiv},
arxivId = {2002.05033},
author = {Zhao, Shuyang and Heittola, Toni and Virtanen, Tuomas},
eprint = {2002.05033},
file = {::},
pages = {1--11},
title = {{Active Learning for Sound Event Detection}},
xurl = {http://arxiv.org/abs/2002.05033},
year = {2020}
}

@inproceedings{Dwibedi:2020:RepNet:CVPR,
abstract = {We present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called Repnet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix ({\~{}}90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos. Project webpage: https://sites.google.com/view/repnet .},
archivePrefix = {arXiv},
arxivId = {2006.15418},
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {2006.15418},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwibedi et al. - 2020 - Counting Out Time Class Agnostic Video Repetition Counting in the Wild.pdf:pdf},
title = {{Counting Out Time: Class Agnostic Video Repetition Counting in the Wild}},
xurl = {http://arxiv.org/abs/2006.15418},
year = {2020}
}

@inproceedings{Grollmisch:2019:EnsembleSize:CMMR,
address = {Marseille, France},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Mora-{\'{A}}ngel, Fernando and Gil, Gustavo L{\'{o}}pez},
booktitle = {Proceedings of the 14th International Symposium of Computer Music Multidisciplinary Research (CMMR)},
file = {::},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Ensemble size classification in Colombian Andean string music recordings}},
year = {2019}
}

@inproceedings{Kareer2018,
abstract = {Knowing the number of sources present in a mixture is useful for many computer audition problems such as polyphonic music transcription, source separation, and speech enhancement. Most existing algorithms for these applications require the user to provide this number thereby limiting the possibility of complete automatization. In this paper we explore a few probabilistic and machine learning approaches for an autonomous source number estimation. We then propose an implementation of a multi-class classification method using convolutional neural networks for musical polyphony estimation. In addition, we use these results to improve the performance of an instrument classifier based on the same dataset. Our final classification results for both the networks, prove that this method is a promising starting point for further advancements in unsupervised source counting and separation algorithms for music and speech.},
address = {Milan, Italy},
author = {Kareer, Saarish and Basu, Sattwik},
booktitle = {Audio Engineering Society Convention 144},
file = {::},
title = {{Musical Polyphony Estimation}},
year = {2018}
}

@inproceedings{engel2020self,
  title={Self-supervised Pitch Detection by Inverse Audio Synthesis},
  author={Engel, Jesse and Swavely, Rigel and Hantrakul, Lamtharn Hanoi and Roberts, Adam and Hawthorne, Curtis},
  xbooktitle={International Conference on Machine Learning (ICML'20)},
  year={2020}
}

@article{chakrabarty2019gestalt,
  title={A Gestalt inference model for auditory scene segregation},
  author={Chakrabarty, Debmalya and Elhilali, Mounya},
  journal={PLoS computational biology},
  volume={15},
  number={1},
  pages={e1006711},
  year={2019},
  publisher={Public Library of Science}
}


@ARTICLE{bellur2020taslp,
  author={A. {Bellur} and M. {Elhilali}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title={Audio object classification using distributed beliefs and attention},
  year={2020},
  volume={},
  number={},
  pages={1-1},
  xdoi={10.1109/TASLP.2020.2966867}
}

@inproceedings{Kim:2019:PointWise:WASPAA,
address = {New Paltz, NY, USA},
author = {Kim, Bongjun and Pardo, Bryan},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {:C$\backslash$:/Users/abr/Downloads/08937213.pdf:pdf},
isbn = {9781728111230},
pages = {1--5},
title = {{Sound Event Detection using Point-Labeled Data}},
year = {2019}
}

@article{Zhang:2020:FrameLevelAttention:ARXIV,
abstract = {Environmental sound classification (ESC) is a challenging problem due to the complexity of sounds. The classification performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. However, ESC often suffers from the semantically irrelevant frames and silent frames. In order to deal with this, we employ a frame-level attention model to focus on the semantically relevant frames and salient frames. Specifically, we first propose a convolutional recurrent neural network to learn spectro-temporal features and temporal correlations. Then, we extend our convolutional RNN model with a frame-level attention mechanism to learn discriminative feature representations for ESC. We investigated the classification performance when using different attention scaling function and applying different layers. Experiments were conducted on ESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness of the proposed method and our method achieved the state-of-the-art or competitive classification accuracy with lower computational complexity. We also visualized our attention results and observed that the proposed attention mechanism was able to lead the network tofocus on the semantically relevant parts of environmental sounds.},
archivePrefix = {arXiv},
arxivId = {2007.07241},
author = {Zhang, Zhichao and Xu, Shugong and Zhang, Shunqing and Qiao, Tianhao and Cao, Shan},
eprint = {2007.07241},
pages = {1--8},
title = {{Learning Frame Level Attention for Environmental Sound Classification}},
xurl = {http://arxiv.org/abs/2007.07241},
year = {2020}
}

@article{Ding:2020:AdaMD:IEEE_TASLP,
abstract = {The goal of acoustic (or sound) events detection (AED or SED) is to predict the temporal position of target events in given audio segments. This task plays a significant role in safety monitoring, acoustic early warning and other scenarios. However, the deficiency of data and diversity of acoustic event sources make the AED task a tough issue, especially for prevalent data-driven methods. In this article, we start from analyzing acoustic events according to their time-frequency domain properties, showing that different acoustic events have different time-frequency scale characteristics. Inspired by the analysis, we propose an adaptive multi-scale detection (AdaMD) method. By taking advantage of hourglass neural network and gated recurrent unit (GRU) module, our AdaMD produces multiple predictions at different temporal and frequency resolutions. An adaptive training algorithm is subsequently adopted to combine multi-scale predictions to enhance the overall capability. Experimental results on Detection and Classification of Acoustic Scenes and Events 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3 demonstrate that the AdaMD outperforms published state-of-the-art competitors in terms of the metrics of event error rate (ER) and F1-score. The verification experiment on our collected factory mechanical dataset also proves the noise-resistant capability of the AdaMD, providing the possibility for it to be deployed in the complex environment.},
archivePrefix = {arXiv},
arxivId = {1911.06878},
author = {Ding, Wenhao and He, Liang},
xdoi = {10.1109/TASLP.2019.2953350},
eprint = {1911.06878},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, He - 2020 - Adaptive multi-scale detection of acoustic events.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Rare acoustic event detection,adaptive multi-scale,hourglass network},
number = {X},
pages = {294--306},
title = {{Adaptive multi-scale detection of acoustic events}},
volume = {28},
year = {2020}
}

@article{Pankajakshan:2020:MSSSelfAttention:ARXIV,
abstract = {In this paper we investigate the importance of the extent of memory in sequential self attention for sound recognition. We propose to use a memory controlled sequential self attention mechanism on top of a convolutional recurrent neural network (CRNN) model for polyphonic sound event detection (SED). Experiments on the URBAN-SED dataset demonstrate the impact of the extent of memory on sound recognition performance with the self attention induced SED model. We extend the proposed idea with a multi-head self attention mechanism where each attention head processes the audio embedding with explicit attention width values. The proposed use of memory controlled sequential self attention offers a way to induce relations among frames of sound event tokens. We show that our memory controlled self attention model achieves an event based F -score of 33.92{\%} on the URBAN-SED dataset, outperforming the F -score of 20.10{\%} reported by the model without self attention.},
archivePrefix = {arXiv},
arxivId = {2005.06650},
author = {Pankajakshan, Arjun and Bear, Helen L. and Subramanian, Vinod and Benetos, Emmanouil},
eprint = {2005.06650},
file = {::},
title = {{Memory Controlled Sequential Self Attention for Sound Recognition}},
xurl = {http://arxiv.org/abs/2005.06650},
year = {2020}
}

@book{Lostanlen:2019:EventDetection:PLOS,
abstract = {Bioacoustic sensors, sometimes known as autonomous recording units (ARUs), can record sounds of wildlife over long periods of time in scalable and minimally invasive ways. Deriving per-species abundance estimates from these sensors requires detection, classification, and quantification of animal vocalizations as individual acoustic events. Yet, variability in ambient noise, both over time and across sensors, hinders the reliability of current automated systems for sound event detection (SED), such as convolutional neural networks (CNN) in the time-frequency domain. In this article, we develop, benchmark, and combine several machine listening techniques to improve the generalizability of SED models across heterogeneous acoustic environments. As a case study, we consider the problem of detecting avian flight calls from a ten-hour recording of nocturnal bird migration, recorded by a network of six ARUs in the presence of heterogeneous background noise. Starting from a CNN yielding state-of-the-art accuracy on this task, we introduce two noise adaptation techniques, respectively integrating short-term (60-millisecond) and long-term (30-minute) context. First, we apply per-channel energy normalization (PCEN) in the time-frequency domain, which applies short-term automatic gain control to every subband in the mel-frequency spectrogram. Secondly, we replace the last dense layer in the network by a context-adaptive neural network (CA-NN) layer, i.e. an affine layer whose weights are dynamically adapted at prediction time by an auxiliary network taking long-term summary statistics of spectrotemporal features as input. We show that both techniques are helpful and complementary. [...] We release a pre-trained version of our best performing system under the name of BirdVoxDetect, a ready-to-use detector of avian flight calls in field recordings.},
archivePrefix = {arXiv},
arxivId = {1905.08352},
author = {Lostanlen, Vincent and Salamon, Justin and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
booktitle = {PLoS ONE},
xdoi = {10.1371/journal.pone.0214168},
eprint = {1905.08352},
file = {::},
isbn = {1111111111},
issn = {19326203},
number = {10},
pages = {1--31},
publisher = {Public Library of Science},
title = {{Robust sound event detection in bioacoustic sensor networks}},
xurl = {http://arxiv.org/abs/1905.08352},
volume = {14},
year = {2019}
}

@InProceedings{ofner2018hpc,
  author    = {André Ofner and Sebastian Stober},
  title     = {Towards Bridging Human and Artificial Cognition: Hybrid Variational Predictive Coding of the Physical World, the Body and the Brain},
  booktitle = {NeurIPS 2018 Workshop on Modeling the Physical World},
  year      = {2018},
  abstract  = {Predictive coding and its generalization to active inference offer a unified theory of brain function. The underlying predictive processing paradigm has gained significant attention within the machine learning community for its representation learning and predictive capacity. Here, we suggest that it is possible to integrate human and artificial predictive models with an artificial neural network that learns to predict sensations simultaneously with their representation in the brain. Guided by the principles of active inference, we propose a recurrent hierarchical predictive coding model that jointly predicts stimuli, electroencephalogram and physiological signals under variational inference. We suggest that in a shared environment, the artificial inference process can learn to predict and exploit the human generative model. We evaluate the model on a publicly available dataset of subjects watching one-minute long video excerpts and show that the model can be trained to predict physical properties such as the amount, distance and motion of human subjects in future frames of the videos. Our results hint at the possibility of bi-directional active inference across human and machine.},
  pdf       = {ofner2018hpc.pdf},
  poster    = {ofner2018hpc-poster.pdf},
  sortkey   = {an6},
  timestamp = {2018.11.22},
}

@InProceedings{ofner2020ismir,
  author    = {André Ofner and Sebastian Stober},
  booktitle = {21st International Society for Music Information Retrieval Conference (ISMIR'20)},
  title     = {Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in {EEG}},
  year      = {2020},
  timestamp = {2020.10.30},
}

@InProceedings{krug2018irasl,
  author    = {Andreas Krug and René Knaebel and Sebastian Stober},
  title     = {Neuron Activation Profiles for Interpreting Convolutional Speech Recognition Models},
  booktitle = {NeurIPS 2018 Interpretability and Robustness for Audio, Speech and Language Workshop (IRASL'18)},
  year      = {2018},
  abstract  = {The increasing complexity of deep Artificial Neural Networks (ANNs) allows to solve complex tasks in various applications. This comes with less understanding of decision processes in ANNs. Therefore, introspection techniques have been proposed to interpret how the network accomplishes its task. Those methods mostly visualize their results in the input domain and often only process single samples. For images, highlighting important features or creating inputs which activate certain neurons is intuitively interpretable. The same introspection for speech is much harder to interpret. In this paper, we propose an alternative method which analyzes neuron activations for whole data sets. Its generality allows application to complex data like speech. We introduce time-independent Neuron Activation Profiles (NAPs) as characteristic network responses to certain groups of inputs. By clustering those time-independent NAPs, we reveal that layers are specific to certain groups. We demonstrate our method for a fully-convolutional speech recognizer. There, we investigate whether phonemes are implicitly learned as an intermediate representation for predicting graphemes. We show that our method reveals, which layers encode phonemes and graphemes and that similarities between phonetic categories are reflected in the clustering of time-independent NAPs.},
  pdf       = {krug2018irasl.pdf},
  slides    = {krug2018irasl-slides.pdf},
  sortkey   = {an5},
  timestamp = {2018.11.21},
}

@InProceedings{krug2019blackboxnlp,
  author    = {Andreas Krug and Sebastian Stober},
  title     = {Visualizing Deep Neural Networks for Speech Recognition with Learned Topographic Filter Maps},
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  year      = {2019},
  abstract  = {The uninformative ordering of artificial neurons in Deep Neural Networks complicates visualizing activations in deeper layers. This is one reason why the internal structure of such models is very unintuitive. In neuroscience, activity of real brains can be visualized by highlighting active regions. Inspired by those techniques, we train a convolutional speech recognition model, where filters are arranged in a 2D grid and neighboring filters are similar to each other. We show, how those topographic filter maps visualize artificial neuron activations more intuitively. Moreover, we investigate, whether this causes phoneme-responsive neurons to be grouped in certain regions of the topographic map.},
  pdf       = {krug2019blackboxnlp.pdf},
  sortkey   = {ao5},
  timestamp = {2019.11.18},
}

@inproceedings{krug2017ccn,
  author = {Andreas Krug and Sebastian Stober},
  title = {Adaptation of the Event-Related Potential Technique for Analyzing Artificial Neural Nets},
  booktitle = {Conference on Cognitive Computational Neuroscience (CCN'17)},
  year = {2017}
}

@phdthesis{Abesser:2014:BassGuitar:PHD,
abstract = {Music recordings most often consist of multiple instrument signals, which overlap in time and frequency. In the field of Music Information Retrieval (MIR), existing algorithms for the automatic transcription and analysis of music recordings aim to extract semantic information from mixed audio signals. In the last years, it was frequently observed that the algorithm performance is limited due to the signal interference and the resulting loss of information. One common approach to solve this problem is to first apply source separation algorithms to isolate the present musical instrument signals before analyzing them individually. The performance of source separation algorithms strongly depends on the number of instruments as well as on the amount of spectral overlap.},
author = {Abe{\ss}er, Jakob},
file = {::},
school = {Technische Universit{\"{a}}t Ilmenau},
title = {{Automatic Transcription of Bass Guitar Tracks applied for Music Genre Classification and Sound Synthesis}},
year = {2014}
}

@inproceedings{Abesser:2018:Stadtlaerm:FICLOUD,
abstract = {Smart city applications for acoustic monitoring become essential to cope with the overall increasing noise pollution in urban environments. This paper gives an overview over a distributed sensor network for noise monitoring in the German city of Jena. Several acoustic sensor units allow for classifying among various acoustic scenes and events using Convolutional Neural Networks (CNN) and measuring different noise level parameters. Connected by a communication system based on MQTT (Message Queue Telemetry Transport), these sensors communicate measurement data to a central server for data postprocessing and storage. Finally, a web-based application allows for various real-time visualizations of noise exposure distributed over the city.},
address = {Barcelona, Spain},
author = {Abe{\ss}er, Jakob and Gr{\"{a}}fe, Robert and K{\"{u}}hn, Christian and Clau{\ss}, Tobias and Lukashevich, Hanna and G{\"{o}}tze, Marco and K{\"{u}}hnlenz, Stephanie},
booktitle = {Proceedings of the 6th IEEE International Conference on Future Internet of Things and Cloud (FiCLOUD)},
xdoi = {10.1109/FiCloud.2018.00053},
file = {::},
isbn = {9781538675038},
keywords = {TA L{\"{a}}rm,acoustic scene classification,event detection,machine{\_}learning,noise level measurement,sensor network,smart city},
mendeley-tags = {machine{\_}learning},
pages = {318--324},
title = {{A Distributed Sensor Network for Monitoring Noise Level and Noise Sources in Urban Environments}},
year = {2018}
}

@inproceedings{Abesser:2017:ASC:DCASE,
abstract = {Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban en-vironments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-the-art CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configura-tion leads to a classification accuracy of 75{\%} for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.},
address = {Munich, Germany},
author = {Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Gr{\"{a}}fe, Robert and Lukashevich, Hanna},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - 2017 - Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Ne.pdf:pdf},
title = {{Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks}},
year = {2017}
}