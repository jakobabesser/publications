@InProceedings{ofner2020ismir,
  author    = {André Ofner and Sebastian Stober},
  booktitle = {21st International Society for Music Information Retrieval Conference (ISMIR'20)},
  title     = {Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in {EEG}},
  year      = {2020},
  timestamp = {2020.10.30},
}


@InProceedings{rane2020icmr,
  author    = {Rane, Roshan Prakash and Sz\"{u}gyi, Edit and Saxena, Vageesh and Ofner, Andr\'{e} and Stober, Sebastian},
  booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
  title     = {PredNet and Predictive Coding: A Critical Review},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {233–241},
  publisher = {Association for Computing Machinery},
  series    = {ICMR ’20},
  xdoi       = {10.1145/3372278.3390694},
  isbn      = {9781450370875},
  keywords  = {semi-supervised, convolutional neural networks, predictive coding, video prediction, deep learning, video classification},
  location  = {Dublin, Ireland},
  numpages  = {9},
  timestamp = {2020.07.28},
  xurl       = {https://xdoi.org/10.1145/3372278.3390694},
}

@InProceedings{ofner2020smc,
  author    = {André Ofner and Sebastian Stober},
  booktitle = {IEEE International Conference on Systems, Man, and Cybernetics (SMC 2020)},
  title     = {Balancing Active Inference and Active Learning with Deep Variational Predictive Coding for {EEG}},
  year      = {2020},
  timestamp = {2020.07.28},
}

@Article{ofner2019alife,
  author    = {Ofner, André and Stober, Sebastian},
  journal   = {The 2019 Conference on Artificial Life},
  title     = {Hybrid Variational Predictive Coding as a Bridge between Human and Artificial Cognition},
  year      = {2019},
  number    = {31},
  pages     = {68-69},
  abstract  = {Predictive coding and its generalization to active inference offer a unified theory of brain function. The underlying predictive processing paradigmhas gained significant attention in artificial intelligence research for its representation learning and predictive capacity. Here, we suggest that it is possible to integrate human and artificial generative models with a predictive coding network that processes sensations simultaneously with the signature of predictive coding found in human neuroimaging data. We propose a recurrent hierarchical predictive coding model that predicts low-dimensional representations of stimuli, electroencephalogram and physiological signals with variational inference. We suggest that in a shared environment, such hybrid predictive coding networks learn to incorporate the human predictive model in order to reduce prediction error. We evaluate the model on a publicly available EEG dataset of subjects watching one-minute long video excerpts. Our initial results indicate that the model can be trained to predict visual properties such as the amount, distance and motion of human subjects in videos.},
  xdoi       = {10.1162/isal\_a\_00142},
  eprint    = {https://www.mitpressjournals.org/xdoi/pdf/10.1162/isal_a_00142},
  pdf       = {ofner2019alife.pdf},
  poster    = {ofner2019alife-slides.pdf},
  sortkey   = {ao2},
  timestamp = {2019.11.18},
  xurl       = {https://www.mitpressjournals.org/xdoi/abs/10.1162/isal_a_00142},
}

@InProceedings{ofner2018hpc,
  author    = {André Ofner and Sebastian Stober},
  title     = {Towards Bridging Human and Artificial Cognition: Hybrid Variational Predictive Coding of the Physical World, the Body and the Brain},
  booktitle = {NeurIPS 2018 Workshop on Modeling the Physical World},
  year      = {2018},
  abstract  = {Predictive coding and its generalization to active inference offer a unified theory of brain function. The underlying predictive processing paradigm has gained significant attention within the machine learning community for its representation learning and predictive capacity. Here, we suggest that it is possible to integrate human and artificial predictive models with an artificial neural network that learns to predict sensations simultaneously with their representation in the brain. Guided by the principles of active inference, we propose a recurrent hierarchical predictive coding model that jointly predicts stimuli, electroencephalogram and physiological signals under variational inference. We suggest that in a shared environment, the artificial inference process can learn to predict and exploit the human generative model. We evaluate the model on a publicly available dataset of subjects watching one-minute long video excerpts and show that the model can be trained to predict physical properties such as the amount, distance and motion of human subjects in future frames of the videos. Our results hint at the possibility of bi-directional active inference across human and machine.},
  pdf       = {ofner2018hpc.pdf},
  poster    = {ofner2018hpc-poster.pdf},
  sortkey   = {an6},
  timestamp = {2018.11.22},
}

@InProceedings{rane2019comco,
  author    = {Roshan Prakash Rane and André Ofner and Shreyas Gite and Sebastian Stober},
  booktitle = {Computational Cognition 2019 Workshop},
  title     = {Predictive Coding Based Vision For Autonomous Cars},
  year      = {2019},
  abstract  = {In recent decades, Predictive Coding has emerged as a unifying theory of human cognition. Related theories in cognitive neuroscience, such as Active Inference and Free Energy Minimization, have demonstrated that Predictive Coding can account for many aspects of human perception and action. However, little work has been done to explore the Predictive Coding framework in the practical domains like computer vision or robotics.
A popular implementation in the field of computer vision that is inspired by Predictive Coding is called the ‘PredNet’. PredNet is trained on videos to perform future frame prediction. In a purely perceptual setup like this, Predictive Coding is defined as a hierarchical generative model that dynamically infers low-dimensional causes from high-dimensional perceptual stimuli. The architecture is trained at each level of it’s hierarchy to learn low-dimensional causal factors from temporal visual data by actively generating top-down predictions or hypotheses and testing them against bottom-up incoming frames or sensory evidence. In our recent work, we inspected the PredNet architecture and found that it fails to emulate and therefore benefit from many core ideas of Predictive Coding. We will highlight these conceptual limitations of PredNet and present preliminary results from our improved Predictive Coding architecture.
Even though our architecture is inspired by PredNet, it differs from it in three main ways: (1) It is designed to perform semantic segmentation which is an important vision task for autonomous driving. The task is to classify pixels of an image as belonging to a semantic category like drivable road, pedestrian or car (2) The top-down predictions represent semantic class maps and not pixel values and (3) It performs not just short-term but also long-term predictions along its hierarchy.
Finally, we compare our architecture’s performance against contemporary deep learning methods for the autonomous driving vision task. We access the semantic segmentation accuracy with an emphasis on the computational efficiency. This includes the model size, amount of training data it needs and the run-time. We also inspect the ability of the model to adjust to differing visual contexts like day time, night time and different weather conditions like rain or snow.},
  poster    = {rane2019comco-poster.pdf},
  sortkey   = {ao7},
  timestamp = {2019.11.19},
  xurl       = {http://www.comco2019.com/abstracts/day1_rane.pdf},
}

@InProceedings{krug2018irasl,
  author    = {Andreas Krug and René Knaebel and Sebastian Stober},
  title     = {Neuron Activation Profiles for Interpreting Convolutional Speech Recognition Models},
  booktitle = {NeurIPS 2018 Interpretability and Robustness for Audio, Speech and Language Workshop (IRASL'18)},
  year      = {2018},
  abstract  = {The increasing complexity of deep Artificial Neural Networks (ANNs) allows to solve complex tasks in various applications. This comes with less understanding of decision processes in ANNs. Therefore, introspection techniques have been proposed to interpret how the network accomplishes its task. Those methods mostly visualize their results in the input domain and often only process single samples. For images, highlighting important features or creating inputs which activate certain neurons is intuitively interpretable. The same introspection for speech is much harder to interpret. In this paper, we propose an alternative method which analyzes neuron activations for whole data sets. Its generality allows application to complex data like speech. We introduce time-independent Neuron Activation Profiles (NAPs) as characteristic network responses to certain groups of inputs. By clustering those time-independent NAPs, we reveal that layers are specific to certain groups. We demonstrate our method for a fully-convolutional speech recognizer. There, we investigate whether phonemes are implicitly learned as an intermediate representation for predicting graphemes. We show that our method reveals, which layers encode phonemes and graphemes and that similarities between phonetic categories are reflected in the clustering of time-independent NAPs.},
  pdf       = {krug2018irasl.pdf},
  slides    = {krug2018irasl-slides.pdf},
  sortkey   = {an5},
  timestamp = {2018.11.21},
}

@InProceedings{krug2019blackboxnlp,
  author    = {Andreas Krug and Sebastian Stober},
  title     = {Visualizing Deep Neural Networks for Speech Recognition with Learned Topographic Filter Maps},
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  year      = {2019},
  abstract  = {The uninformative ordering of artificial neurons in Deep Neural Networks complicates visualizing activations in deeper layers. This is one reason why the internal structure of such models is very unintuitive. In neuroscience, activity of real brains can be visualized by highlighting active regions. Inspired by those techniques, we train a convolutional speech recognition model, where filters are arranged in a 2D grid and neighboring filters are similar to each other. We show, how those topographic filter maps visualize artificial neuron activations more intuitively. Moreover, we investigate, whether this causes phoneme-responsive neurons to be grouped in certain regions of the topographic map.},
  pdf       = {krug2019blackboxnlp.pdf},
  sortkey   = {ao5},
  timestamp = {2019.11.18},
}

@inproceedings{krug2017ccn,
  author = {Andreas Krug and Sebastian Stober},
  title = {Adaptation of the Event-Related Potential Technique for Analyzing Artificial Neural Nets},
  booktitle = {Conference on Cognitive Computational Neuroscience (CCN'17)},
  year = {2017}
}

@article{krug2020gradient,
  title={Gradient-Adjusted Neuron Activation Profiles for Comprehensive Introspection of Convolutional Speech Recognition Models},
  author={Krug, Andreas and Stober, Sebastian},
  journal={arXiv preprint arXiv:2002.08125},
  year={2020}
}





@inproceedings{engel2020ddsp,
  title={DDSP: Differentiable Digital Signal Processing},
  author={Jesse Engel and Lamtharn Hantrakul and Chenjie Gu and Adam Roberts},
  booktitle={ICLR},
  year={2020}
}

@ARTICLE{8022871,
  author={M. {Kahng} and P. Y. {Andrews} and A. {Kalro} and D. H. {Chau}},
  journal={IEEE Trans. on Visualization and Computer Graphics},
  title={ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models},
  year={2018},
  volume={24},
  number={1},
  pages={88-97},
  abstract={While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.},
  keywords={data visualisation;learning (artificial intelligence);neural nets;visual tools;large-scale datasets;participatory design sessions;interactive visualization system;large-scale deep learning models;model architecture;complex deep neural network models;visual exploration;industry-scale deep neural network models;ActiVis system;machine learning platform;Computational modeling;Tools;Machine learning;Data models;Neurons;Facebook;Data visualization;Visual analytics;deep learning;machine learning;information visualization},
  xdoi={10.1109/TVCG.2017.2744718},
  ISSN={1941-0506},
  month={Jan},}


@ARTICLE{8957135,
  author={A. {Bichicchi} and R. {Belaroussi} and A. {Simone} and V. {Vignali} and C. {Lantieri} and X. {Li}},
  journal={IEEE Access},
  title={Analysis of Road-User Interaction by Extraction of Driver Behavior Features Using Deep Learning},
  year={2020},
  volume={8},
  number={},
  pages={19638-19645},
  abstract={In this study, an improved deep learning model is proposed to explore the complex interactions between the road environment and driver's behaviour throughout the generation of a graphical representation. The proposed model consists of an unsupervised Denoising Stacked Autoencoder (SDAE) able to provide output layers in RGB colors. The dataset comes from an experimental driving test where kinematic measures were tracked with an in-vehicle GPS device. The graphical outcomes reveal the method ability to efficiently detect patterns of simple driving behaviors, as well as the road environment complexity and some events encountered along the path.},
  keywords={behavioural sciences computing;Global Positioning System;learning (artificial intelligence);neural nets;traffic engineering computing;pattern detection;driver behavior feature extraction;unsupervised denoising stacked autoencoder;road environment complexity;driving behaviors;graphical outcomes;in-vehicle GPS device;kinematic measures;experimental driving test;RGB colors;output layers;graphical representation;complex interactions;improved deep learning model;road-user interaction;Feature extraction;Vehicles;Roads;Image color analysis;Principal component analysis;Data models;Data mining;Deep learning;driver behavior;event detection;road safety;workload},
  xdoi={10.1109/ACCESS.2020.2965940},
  ISSN={2169-3536},
  month={},}


@inproceedings{10.1145/3109859.3109877,
author = {Donkers, Tim and Loepp, Benedikt and Ziegler, J\"{u}rgen},
title = {Sequential User-Based Recurrent Neural Network Recommendations},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
xurl = {https://xdoi.org/10.1145/3109859.3109877},
xdoi = {10.1145/3109859.3109877},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {152–160},
numpages = {9},
keywords = {recurrent neural networks, recommender systems, neural networks, deep learning, sequential recommendations},
location = {Como, Italy},
series = {RecSys ’17}
}

@inproceedings{10.1145/2766462.2767745,
author = {Lagun, Dmitry and Agichtein, Eugene},
title = {Inferring Searcher Attention by Jointly Modeling User Interactions and Content Salience},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
xurl = {https://xdoi.org/10.1145/2766462.2767745},
xdoi = {10.1145/2766462.2767745},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {483–492},
numpages = {10},
keywords = {web page layout, mouse cursor tracking, searcher attention, eye tracking, content salience, search behavior},
location = {Santiago, Chile},
series = {SIGIR ’15}
}

@inproceedings{10.1145/2647868.2654945,
author = {Lin, Huijie and Jia, Jia and Guo, Quan and Xue, Yuanyuan and Li, Qi and Huang, Jie and Cai, Lianhong and Feng, Ling},
title = {User-Level Psychological Stress Detection from Social Media Using Deep Neural Network},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
xurl = {https://xdoi.org/10.1145/2647868.2654945},
xdoi = {10.1145/2647868.2654945},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {507–516},
numpages = {10},
keywords = {stress detection, cross auto encoders, convolutional neural network, micro-blog, social media, deep learning},
location = {Orlando, Florida, USA},
series = {MM ’14}
}

@inproceedings{swearngin2019modeling,
  title={Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning},
  author={Swearngin, Amanda and Li, Yang},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--11},
  year={2019}
}

@article{budd2019survey,
  title={A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image Analysis},
  author={Budd, Samuel and Robinson, Emma C and Kainz, Bernhard},
  journal={arXiv preprint arXiv:1910.02923},
  year={2019}
}



@article{hinton2006autoencoders,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}

@inproceedings{baldi2012autoencoders,
  title={Autoencoders, unsupervised learning, and deep architectures},
  author={Baldi, Pierre},
  booktitle={Proceedings of ICML workshop on unsupervised and transfer learning},
  pages={37--49},
  year={2012}
}

@article{salakhutdinov2015learning,
  title={Learning deep generative models},
  author={Salakhutdinov, Ruslan},
  journal={ANNU REV STAT APPL},
  volume={2},
  pages={361--385},
  year={2015},
  publisher={Annual Reviews}
}

@inproceedings{kingma2013vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  xjournal={International Conference on Learning Representations (ICLR'14)},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{rezende2014vae,
  title={Stochastic backpropagation and variational inference in deep latent gaussian models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  xbooktitle={International Conference on Machine Learning (ICML'14)},
  booktitle={ICML},
  year={2014}
}

@inproceedings{vandenoord2017vqvae,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and others},
  booktitle={NeurIPS},
  pages={6306--6315},
  year={2017}
}

@inproceedings{gregor2018tdvae,
  title={Temporal Difference Variational Auto-Encoder},
  author={Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  zbooktitle={International Conference on Learning Representations (ICLR'18)},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{razavi2019vqvae2,
  title={Generating diverse high-fidelity images with {VQ-VAE-2}},
  author={Razavi, Ali and v d Oord, Aaron and Vinyals, Oriol},
  xbooktitle={Advances in Neural Information Processing Systems (NeurIPS'19)},
  booktitle={NeurIPS},
  pages={14866--76},
  year={2019}
}

@article{tsai2020demystifying,
  title={Demystifying Self-Supervised Learning: An Information-Theoretical Framework},
  author={Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2006.05576},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{tung2017self,
  title={Self-supervised learning of motion capture},
  author={Tung, Hsiao-Yu and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina},
  xbooktitle={Advances in Neural Information Processing Systems (NeurIPS'17)},
  booktitle={NeurIPS},
  pages={5236--5246},
  year={2017}
}

@inproceedings{zhai2019s4l,
  title={S4l: Self-supervised semi-supervised learning},
  author={Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1476--1485},
  year={2019}
}

@article{jing2020self,
  title={Self-supervised visual feature learning with deep neural networks: A survey},
  author={Jing, Longlong and Tian, Yingli},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@INPROCEEDINGS{sermanet2018timecontrastive,
  author={P. {Sermanet} and C. {Lynch} and Y. {Chebotar} and J. {Hsu} and E. {Jang} and S. {Schaal} and S. {Levine} and G. {Brain}},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Time-Contrastive Networks: Self-Supervised Learning from Video}, 
  year={2018},
  volume={},
  number={},
  pages={1134-1141},
  abstract={We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.},
  keywords={image representation;learning (artificial intelligence);pose estimation;robot programming;robot vision;video signal processing;time-contrastive networks;robotic behaviors;robotic imitation settings;human poses;viewpoint-invariant representation;end-effectors;reinforcement learning algorithm;self-supervised learning;robotic systems;Robots;Task analysis;Visualization;Learning (artificial intelligence);Training;Liquids;Lighting},
  xdoi={10.1109/ICRA.2018.8462891},
  ISSN={2577-087X},
  month={05},}

@inproceedings{lee2019making,
  title={Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
  author={Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8943--8950},
  year={2019},
  organization={IEEE}
}



@InProceedings{dwibedi2019temporal,
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
title = {Temporal Cycle-Consistency Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {06},
year = {2019}
}



@InProceedings{saunshi19contrastive,
  title =    {A Theoretical Analysis of Contrastive Unsupervised Representation Learning},
  author =   {Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {5628--5637},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  address =    {Long Beach, California, USA},
  month =    {06},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v97/saunshi19a/saunshi19a.pdf},
  xurl =    {http://proceedings.mlr.press/v97/saunshi19a.html},
  abstract =   {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically “similar" data points and “negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term \emph{contrastive learning} for such algorithms and presents a theoretical framework for analyzing them by introducing \emph{latent classes} and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.}
}

@article{huang2011predictive,
  title={Predictive coding},
  author={Huang, Yanping and Rao, Rajesh PN},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={2},
  number={5},
  pages={580--593},
  year={2011},
  publisher={Wiley Online Library}
}


@article{bongjun2018soundevent,
author = {Kim, Bongjun and Pardo, Bryan},
title = {A Human-in-the-Loop System for Sound Event Detection and Annotation},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
xurl = {https://xdoi.org/10.1145/3214366},
xdoi = {10.1145/3214366},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {13},
numpages = {23},
keywords = {human-in-the-loop system, sound event detection, Interactive machine learning}
}

@article{lorbach2019interactive,
  title={Interactive rodent behavior annotation in video using active learning},
  author={Lorbach, Malte and Poppe, Ronald and Veltkamp, Remco C},
  journal={Multimedia Tools and Applications},
  volume={78},
  number={14},
  pages={19787--19806},
  year={2019},
  publisher={Springer}
}

@book{lang2017handbook,
  title={Handbook of learning analytics},
  author={Lang, Charles and Siemens, George and Wise, Alyssa and Gasevic, Dragan},
  year={2017},
  publisher={SOLAR, Society for Learning Analytics and Research}
}

@article{liu2017going,
  title={Going beyond better data prediction to create explanatory models of educational data},
  author={Liu, Ran and Koedinger, Kenneth R},
  journal={The Handbook of learning analytics},
  pages={69--76},
  year={2017}
}

@article{rose2019explanatory,
  title={Explanatory learner models: Why machine learning (alone) is not the answer},
  author={Ros{\'e}, Carolyn P and McLaughlin, Elizabeth A and Liu, Ran and Koedinger, Kenneth R},
  journal={British Journal of Educational Technology},
  volume={50},
  number={6},
  pages={2943--2958},
  year={2019},
  publisher={Wiley Online Library}
}

@article{du2020integrated,
  title={An integrated framework based on latent variational autoencoder for providing early warning of at-risk students},
  author={Du, Xu and Yang, Juan and Hung, Jui-Long},
  journal={IEEE Access},
  volume={8},
  pages={10110--10122},
  year={2020},
  publisher={IEEE}
}

@article{yang2020using,
  title={Using Convolutional Neural Network to Recognize Learning Images for Early Warning of At-risk Students},
  author={Yang, Zongkai and Yang, Juan and Rice, Kerry and Hung, Jui-Long and Du, Xu},
  journal={IEEE Transactions on Learning Technologies},
  year={2020},
  publisher={IEEE}
}

@inproceedings{cen2006learning,
  title={Learning factors analysis--a general method for cognitive model evaluation and improvement},
  author={Cen, Hao and Koedinger, Kenneth and Junker, Brian},
  booktitle={International Conference on Intelligent Tutoring Systems},
  pages={164--175},
  year={2006},
  organization={Springer}
}

@inproceedings{EditBlog,
   author =       {MIke Jeffs},
   title =        {OK Google, Siri, Alexa, Cortana; Can you tell me some stats on voice search?},
   publisher =    {The Editr Blog},
   month =        {01},
   year =         {20178},
   note =         {[Online; posted 8th-Jan-2018]},
 }xurl =          {https://edit.co.uk/blog/google-voice-search-stats-growth-trends/},

@inproceedings{eMarketer2019,
   author =       {V. Petrock},
   title =        {US Voice Assistant Users 2019 -- Who, What, Where and Why},
   publisher =    {eMarketer},
   month =        {09},
   year =         {2019},
   note =         {[Online; 15-Jul-2019]},
 }xurl =          {https://www.emarketer.com/content/us-voice-assistant-users-2019#page-report},
 
 @inproceedings{voicebot.ai2019,
   author =       {Bret Kinsella},
   title =        {Nearly 90 Million U.S. Adults Have Smart Speakers, Adoption Now Exceeds One-Third of Consumers},
   publisher =    {voicebot.ai},
   month =        {04},
   year =         {2020},
   note =         {[Online;  April 28, 2020 at 12:19 pm]},
 }xurl =          {https://voicebot.ai/2020/04/28/nearly-90-million-u-s-adults-have-smart-speakers-adoption-now-exceeds-one-third-of-consumers/},
 
 @Article{AI_55,
  Title                    = {Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence},
  Author                   = {A. Kaplan and M. Haenlein},
  Journal                  = {Business Horizons},
  Volume                   = {62},
  Year                     = {2019},
  pages                    = {15--25},
}

@techreport{Valli_NotesOnNI,
     title = {Notes on Natural Interaction},
     author = {Valli, Alessandro},
     year = {2007},
     institution = {University of Florence, Italy},
     month = {09},
}

@Inbook{Companion2017,
author="Biundo, Susanne
and Wendemuth, Andreas",
title="An introduction to companion-technology",
bookTitle="Companion Technology: A Paradigm Shift in Human-Technology Interaction",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="1--15",
isbn="978-3-319-43665-4",
}

@article{Alepis2017MonkeySM,
  title={Monkey Says, Monkey Does: Security and Privacy on Voice Assistants},
  author={E. Alepis and C. Patsakis},
  journal={IEEE Access},
  year={2017},
  volume={5},
  pages={17841--17851}
}

@article{UBICOMM34,
  title={A study of security and privacy issues associated with the Amazon Echo},
  author={C. Jackson and A. Orebaugh},
  journal={International Journal of Internet of Things and Cyber-Assurance},
  year={2018},
  volume={1}, 
  number={1}, 
  pages={91--100}
}


@article{NAUTSCH2019441,
title = {Preserving privacy in speaker and speech characterisation},
journal = {COMPUT SPEECH LANG},
pages = {441-80},
year = {2019},
issn = {0885-2308},
xdoi = {https://xdoi.org/10.1016/j.csl.2019.06.001},
xurl = {http://www.sciencedirect.com/science/article/pii/S0885230818303875},
author = {Andreas Nautsch and Abelino Jiménez and Amos Treiber and Jascha Kolberg and Catherine Jasserand and Els Kindt and Héctor Delgado and Massimiliano Todisco and Mohamed Amine Hmani and Aymen Mtibaa and Mohammed Ahmed Abdelraheem and Alberto Abad and Francisco Teixeira and Driss Matrouf and Marta Gomez-Barrero and Dijana Petrovska-Delacrétaz and Gérard Chollet and Nicholas Evans and Thomas Schneider and Jean-François Bonastre and Bhiksha Raj and Isabel Trancoso and Christoph Busch},
}

@article{tomashenko2020introducing,
  author    = {Natalia Tomashenko and Brij Mohan Lal Srivastava and Xin Wang and Emmanuel Vincent and Andreas Nautsch and Junichi Yamagishi and Nicholas Evans and Jose Patino and Jean-François Bonastre and Paul-Gauthier Noé and Massimiliano Todisco},
  title     = {Introducing the VoicePrivacy Initiative},
  year      = {2020},
  xurl       = {https://arxiv.org/pdf/2005.01387v2.pdf},
  journal={arXiv preprint 2005.01387},
}

@ARTICLE{5353689,
  author={D. {Erro} and A. {Moreno} and A. {Bonafonte}},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={INCA Algorithm for Training Voice Conversion Systems From Nonparallel Corpora}, 
  year={2010},
  volume={18},
  number={5},
  pages={944-953},
}

@INPROCEEDINGS{7552917,  
author={L. {Sun} and K. {Li} and H. {Wang} and S. {Kang} and H. {Meng}},  
booktitle={2016 IEEE International Conference on Multimedia and Expo (ICME)},  
title={Phonetic posteriorgrams for many-to-one voice conversion without parallel data training},   year={2016}, 
volume={},  
number={},  
pages={1-6},
}

@inproceedings{XieSL16,
  author    = {Feng{-}Long Xie and
               Frank K. Soong and
               Haifeng Li},
  title     = {A {KL} Divergence and DNN-Based Approach to Voice Conversion without
               Parallel Training Sentences},
  booktitle = {Proc. of Interspeech 2016},
  pages     = {287--291},
  publisher = {{ISCA}},
  year      = {2016},
}

@INPROCEEDINGS{7820786,
  author={C. {Hsu} and H. {Hwang} and Y. {Wu} and Y. {Tsao} and H. {Wang}},
  booktitle={2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)}, 
  title={Voice conversion from non-parallel corpora using variational auto-encoder}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},}

@INPROCEEDINGS{7820901,
  author={J. {Wu} and Z. {Wu} and L. {Xie}},
  booktitle={APSIPA'16}, 
  title={On the use of I-vectors and average voice model for voice conversion without parallel data}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},}
  
  
  @article{HsuHWTW17,
  author    = {Chin{-}Cheng Hsu and
               Hsin{-}Te Hwang and
               Yi{-}Chiao Wu and
               Yu Tsao and
               Hsin{-}Min Wang},
  title     = {Voice Conversion from Unaligned Corpora using Variational Autoencoding
               Wasserstein Generative Adversarial Networks},
  year      = {2017},
  xurl       = {http://arxiv.org/abs/1704.00849},
  journal={arXiv preprint 1704.00849},
}

@inproceedings{Lorenzo-TruebaF18,
  author    = {Jaime Lorenzo{-}Trueba and
               Fuming Fang and
               Xin Wang and
               Isao Echizen and
               Junichi Yamagishi and
               Tomi Kinnunen},
  title     = {Can we steal your vocal identity from the Internet?: Initial investigation
               of cloning Obama's voice using GAN, WaveNet and low-quality found
               data},
  booktitle = {Odyssey'18: The Speaker and Language Recognition Workshop},
  pages     = {240--47},
  year      = {2018},
  xurl       = {https://xdoi.org/10.21437/Odyssey.2018-34},
  xdoi       = {10.21437/Odyssey.2018-34},
  timestamp = {Mon, 15 Apr 2019 10:35:34 +0200},
  bibxurl    = {https://dblp.org/rec/conf/odyssey/Lorenzo-TruebaF18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

  @article{Hirokazu18,
  author    = {Hirokazu Kameoka and Takuhiro Kaneko and Kou Tanaka and Nobukatsu Hojo},
  title     = {StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks},
  journal={arXiv preprint 1806.02169},
  year      = {2018},
  xurl       = {https://arxiv.org/abs/1806.02169},
}

@article{Oord2016WaveNetAG,
  title={WaveNet: A Generative Model for Raw Audio},
  author={A. Oord and S. Dieleman and H. Zen and K. Simonyan and Oriol Vinyals and A. Graves and Nal Kalchbrenner and A. Senior and K. Kavukcuoglu},
  year={2016},
  journal={arXiv preprint 1609.03499}
}



@inproceedings{HuangWHTHKTTW19,
  author    = {Wen{-}Chin Huang and
               Yi{-}Chiao Wu and
               Hsin{-}Te Hwang and
               Patrick Lumban Tobing and
               Tomoki Hayashi and
               Kazuhiro Kobayashi and
               Tomoki Toda and
               Yu Tsao and
               Hsin{-}Min Wang},
  title     = {Refined WaveNet Vocoder for Variational Autoencoder Based Voice Conversion},
  booktitle = {27th European Signal Processing Conference, {EUSIPCO} 2019, {A} Coru{\~{n}}a,
               Spain, September 2-6, 2019},
  pages     = {1--5},
  publisher = {{IEEE}},
  year      = {2019},
  xurl       = {https://xdoi.org/10.23919/EUSIPCO.2019.8902651},
  xdoi       = {10.23919/EUSIPCO.2019.8902651},
  timestamp = {Fri, 20 Dec 2019 16:00:32 +0100},
  bibxurl    = {https://dblp.org/rec/conf/eusipco/HuangWHTHKTTW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{8683143,
  author={R. {Prenger} and R. {Valle} and B. {Catanzaro}},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Waveglow: A Flow-based Generative Network for Speech Synthesis}, 
  year={2019},
  volume={},
  number={},
  pages={3617-3621},}

@INPROCEEDINGS{8639507,
  author={B. {Sisman} and M. {Zhang} and S. {Sakti} and H. {Li} and S. {Nakamura}},
  booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Adaptive Wavenet Vocoder for Residual Compensation in GAN-Based Voice Conversion}, 
  year={2018},
  volume={},
  number={},
  pages={282-289},}

@article{ZHANG202031,
title = "DeepConversion: Voice conversion with limited parallel training data",
journal = "Speech Communication",
volume = "122",
pages = "31 - 43",
year = "2020",
issn = "0167-6393",
xdoi = "https://xdoi.org/10.1016/j.specom.2020.05.004",
xurl = "http://www.sciencedirect.com/science/article/pii/S0167639320302296",
author = "Mingyang Zhang and Berrak Sisman and Li Zhao and Haizhou Li",
}


@article{pal2019study,
  title={A study of semi-supervised speaker diarization system using gan mixture model},
  author={Monisankha Pal and Manoj Kumar and Raghuveer Peri and Shrikanth Narayanan},
  journal={arXiv:1910.11416},
  year={2019}
}

@INPROCEEDINGS{8682064,
  author={G. {Bhattacharya} and J. {Monteiro} and J. {Alam} and P. {Kenny}},
  booktitle={Proc. of IEEE ICASSP'19}, 
  title={Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-end Speaker Verification}, 
  year={2019},
  volume={},
  number={},
  pages={6226-6230},}

@article{chung2018voxceleb2,
    title={VoxCeleb2: Deep Speaker Recognition},
    author={Joon Son Chung and Arsha Nagrani and Andrew Zisserman},
    year={2018},
    journal={arXiv preprint 1806.05622},
}

@article{ali2018DBN,
author = {Ali, Hazrat and Tran, Son and Benetos, Emmanouil and Garcez, Artur},
year = {2018},
month = {03},
pages = {},
title = {Speaker Recognition with Hybrid Features from a Deep Belief Network},
journal = {Neural Computing and Applications},
xdoi = {10.1007/s00521-016-2501-7}
}

@inproceedings{snyder2018xvec,
author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
year = {2018},
month = {04},
pages = {5329-5333},
title = {X-Vectors: Robust DNN Embeddings for Speaker Recognition},
xdoi = {10.1109/ICASSP.2018.8461375}
}

@article{lanckerVan1986,
author = {Van Lancker Sidtis, Diana},
year = {1986},
month = {05},
pages = {},
title = {Familiar voice recognition and unfamiliar voice discrimination are independent and unordered abilities},
volume = {79},
journal = {Journal of The Acoustical Society of America - J ACOUST SOC AMER},
xdoi = {10.1121/1.2023449}
}

@article{dolmenVan1990,
author = {Van Dommelen, Wim},
year = {1990},
pages = {259-72},
title = {Acoustic parameters in human speaker recognition},
volume = {33},
journal = {Lang. Speech},
xdoi = {10.1177/002383099003300302}
}

@article{kreiman1992,
author = {Kreiman, Jody and Gerratt, Bruce and Precoda, Kristin and Berke, Gerald},
year = {1992},
month = {07},
pages = {512-20},
title = {Individual Differences in Voice Quality Perception},
volume = {35},
journal = {Journal of speech and hearing research},
xdoi = {10.1044/jshr.3503.512}
}

@article{baumann2008,
author = {Baumann, Oliver and Belin, Pascal},
year = {2008},
month = {12},
pages = {110-20},
title = {Perceptual scaling of voice identity: Common dimensions for different vowels and speakers},
volume = {74},
journal = {Psychological research},
xdoi = {10.1007/s00426-008-0185-z}
}

@inproceedings{boem2007,
author = {Bőhm, Tamás and Shattuck-Hufnagel, Stefanie},
year = {2007},
month = {01},
pages = {2657-2660},
title = {Utterance-final glottalization as a cue for familiar speaker recognition},
volume = {3}
}

@article{alexander2005,
author = {Alexander, A and Botti, F and Dessimoz, Damien and Drygajlo, Andrzej},
year = {2005},
month = {01},
pages = {S95-9},
title = {The effect of mismatched recording conditions on human and automatic speaker recognition in forensic applications},
volume = {146 Suppl},
journal = {Forensic science international},
xdoi = {10.1016/j.forsciint.2004.09.078}
}

@article{boyle2012,
author = {Boyle, Gregory J.},
year = {2012},
month = {11},
pages = {56-66},
title = {Factor Structure of the Differential Emotions Scale and the Eight State Questionnaire Revisited},
volume = {10},
journal = {The Irish Journal of Psychology},
xdoi = {10.1080/03033910.1989.10557734}
}

@book{janke1978eigenschaftswörterliste,
  title={Die Eigenschaftsw{\"o}rterliste: EWL ; eine mehrdimensionale Methode zur Beschreibung von Aspekten des Befindens},
  author={Janke, W. and Debus, G.},
  number={Bd. 1},
  xurl={https://books.google.de/books?id=BCA-GwAACAAJ},
  year={1978},
  publisher={Verlag f{\"u}r Psychologie Hogrefe}
}

@Inbook{cattell2001,
	author="Cattell, Heather E. P.",
	editor="Dorfman, William I.
	and Hersen, Michel",
	title="The Sixteen Personality Factor (16PF) Questionnaire",
	bookTitle="Understanding Psychological Assessment",
	year="2001",
	publisher="Springer US",
	address="Boston, MA",
	pages="187--215",
	abstract="The Sixteen Personality Factor Questionnaire is a comprehensive measure of normal range personality. Although it was not developed to identify psychopathology, it has been used extensively and productively in clinical settings due to its ability to give a deep, integrated picture of the whole person, including both personal strengths and weaknesses. The 16PF questionnaire can be used to identify patterns of behavior in a wide variety of real-life circumstances. For example, it can be used to understand a person's self-esteem, coping patterns, capacity for empathy, interpersonal needs, likely attitude toward power and authority, cognitive processing style, internalization of societal rules or standards, and likely occupational preferences. Because of this comprehensive scope, 16PF results are useful in a wide variety of settings, including clinical, counseling, industrial, career development, and research.",
	isbn="978-1-4615-1185-4",
	xdoi="10.1007/978-1-4615-1185-4_10",
	xurl="https://xdoi.org/10.1007/978-1-4615-1185-4_10"
	}

@article{zhang2017,
    author = {Zhang, Zixing and Geiger, Jürgen and Pohjalainen, Jouni and Mousa, Amr and Schuller, Björn},
    year = {2017},
    month = {05},
    pages = {},
    title = {Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments},
    volume = {9},
    journal = {ACM T INTEL SYST TEC},
    xdoi = {10.1145/3178115}
}

@article{nassif2019,
    author = {Nassif, Ali and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
    year = {2019},
    month = {02},
    title = {Speech Recognition Using Deep Neural Networks: A Systematic Review},
    volume = {PP},
    journal = {IEEE Access},
    xdoi = {10.1109/ACCESS.2019.2896880}
}

@article{errattahi2018,
author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
year = {2018},
month = {01},
pages = {32-37},
title = {Automatic Speech Recognition Errors Detection and Correction: A Review},
volume = {128},
journal = {Procedia Computer Science},
xdoi = {10.1016/j.procs.2018.03.005}
}

@inproceedings{fernandez2017,
author = {Fernández Gallardo, Laura and Möller, Sebastian and Beerends, John},
year = {2017},
month = {08},
booktitle={Proc. Interspeech 2018},
pages = {2939-2943},
title = {Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores},
xdoi = {10.21437/Interspeech.2017-36}
}




@Article{Schuller2011b,
  Title                    = {Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge},
  Author                   = {Schuller, B. and Batliner, A. and Steidl, S. and Seppi, D.},
  Journal                  = {Speech Commun},
  Pages                    = {1062--1087},
  Volume                   = {53},
  Year                     = {2011},
  Month                    = {11},
  Issue                   = {9-10},
}

@Article{ERinthewild,
  Title                    = {Emotion recognition in the wild},
  Author                   = {Dhall, A. and Goecke, R. and Gedeon T. and Sebe, N.},
  Journal                  = {JMUI},
  Pages                    = {95--97},
  Volume                   = {10},
  Year                     = {2016},
  Issue                   = {2},
}

@Article{Ververidis2006,
  Title                    = {Emotional speech recognition: Resources, features, and methods},
  Author                   = {Dimitrios Ververidis and Constantine Kotropoulos},
  Journal                  = {Speech Commun},
  Pages                    = {1162--1181},
  Volume                   = {48},
  Year                     = {2006},
  Issue                   = {9},
}

@ARTICLE{Tahon2015,
author={M. Tahon and L. Devillers},
journal={EEE/ACM Trans. Audio, Speech, Language Process.},
title={Towards a Small Set of Robust Acoustic Features for Emotion Recognition: Challenges},
year={2016},
volume={24},
number={1},
pages={16-28},
}

@article{AKCAY202056,
title = {Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers},
journal = {Speech Communication},
volume = {116},
pages = {56 - 76},
year = {2020},
issn = {0167-6393},
xdoi = {https://xdoi.org/10.1016/j.specom.2019.12.001},
xurl = {http://www.sciencedirect.com/science/article/pii/S0167639319302262},
author = {Mehmet Berkehan Akçay and Kaya Oğuz},
keywords = {Speech emotion recognition, Survey, Speech features, Classification, Speech databases},
abstract = {Speech is the most natural way of expressing ourselves as humans. It is only natural then to extend this communication medium to computer applications. We define speech emotion recognition (SER) systems as a collection of methodologies that process and classify speech signals to detect the embedded emotions. SER is not a new field, it has been around for over two decades, and has regained attention thanks to the recent advancements. These novel studies make use of the advances in all fields of computing and technology, making it necessary to have an update on the current methodologies and techniques that make SER possible. We have identified and discussed distinct areas of SER, provided a detailed survey of current literature of each, and also listed the current challenges.}
}

@INPROCEEDINGS{8733432,
  author={S. K. {Pandey} and H. S. {Shekhawat} and S. R. M. {Prasanna}},
  xbooktitle={2019 29th International Conference Radioelektronika (RADIOELEKTRONIKA)}, 
  booktitle={Int. Conf. Radioelektronika}, 
  title={Deep Learning Techniques for Speech Emotion Recognition: A Review}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
}


@inproceedings{Cho2018,
  author={Jaejin Cho and Raghavendra Pappagari and Purva Kulkarni and Jesús Villalba and Yishay Carmiel and Najim Dehak},
  title={Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts},
  year=2018,
  booktitle={Interspeech},
  pages={247--51},
  xdoi={10.21437/Interspeech.2018-2466},
  xurl={http://dx.xdoi.org/10.21437/Interspeech.2018-2466}
}

@misc{itut1996,
  title={ITU-T Recommendation P.800. Methods for Subjective Determination of Transmission Quality},
  year=1996,
  booktitle={International Telecommunication Union}
}

@misc{itut2019,
  title={ITU-T Recommendation P.808. Subjective evaluation of speech quality with a crowdsourcing approach},
  year=2019,
  booktitle={International Telecommunication Union}
}

@book{waeltermann2013a,
Title = {Dimension-based Quality Modeling of Transmitted Speech},
Author = {Wältermann, Marcel},
Year = {2013},
Address = {Berlin, Heidelberg},
Publisher = {Springer},
Series = {T-Labs Series in Telecommunication Services},
Howpublished = {full},
Toappear = {published},
Categories = {tlabs_yes}
}

@article{wältermann2012direct,			  		  
author={Wältermann, Marcel and Raake, Alexander and Möller, Sebastian},
journal={Journal of the Audio Engineering Society}, 
title={Direct Quantification of Latent Speech Quality Dimensions},
year={2012},
volume={60},
number={4},
pages={246-254},
xdoi={},
month={april}
}	

@article{weiss2018,
author = {Weiss, Benjamin and Estival, Dominique and Stiefelhagen, Ulrike},
year = {2018},
month = {01},
pages = {174-184},
title = {Non-Experts' Perceptual Dimensions of Voice Assessed by Using Direct Comparisons},
volume = {104},
journal = {Acta Acustica united with Acustica},
xdoi = {10.3813/AAA.919157}
}

@inproceedings{Mittag2019,
  author={Gabriel Mittag and Sebastian Möller},
  title={{Quality Degradation Diagnosis for Voice Networks — Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={3426--3430},
  xdoi={10.21437/Interspeech.2019-2636},
  xurl={http://dx.xdoi.org/10.21437/Interspeech.2019-2636}
}

@inproceedings{pub10309,
    author = {Schwarzenberg, Robert and Harbecke, David and Macketanz, Vivien and Avramidis, Eleftherios and Möller, Sebastian},
    title = {Train, Sort, Explain: Learning to Diagnose Translation Models},
    booktitle = {Proc. NAACL-HLT'19},
    year = {2019},
    publisher = {ACL}
}
@inproceedings{pub9237,
    author = {Zeman, Daniel and Popel, Martin and Straka, Milan and Hajic, Jan and Nivre, Joakim and Ginter, Filip and Luotolahti, Juhani and Pyysalo, Sampo and Petrov, Slav and Potthast, Martin and Tyers, Francis and Badmaeva, Elena and Gokirmak, Memduh and Nedoluzhko, Anna and Cinkova, Silvie and jr., Jan Hajic and Hlavacova, Jaroslava and Kettnerová, Václava and Uresova, Zdenka and Kanerva, Jenna and Ojala, Stina and Missilä, Anna and Manning, Christopher D. and Schuster, Sebastian and Reddy, Dima Taji Siva and Habash, Nizar and Leung, Herman and Marneffe, Marie-Catherine de and Sanguinetti, Manuela and Simi, Maria and Kanayama, Hiroshi and dePaiva, Valeria and Droganova, Kira and Alonso, Héctor Martínez and Çöltekin, Çağrı and Sulubacak, Umut and Uszkoreit, Hans and Macketanz, Vivien and Burchardt, Aljoscha and Harris, Kim and Marheinecke, Katrin and Rehm, Georg and Kayadelen, Tolga and Attia, Mohammed and Elkahky, Ali and Yu, Zhuoran and Pitler, Emily and Lertpradit, Saran and Mandl, Michael and Kirchner, Jesse and Alcalde, Hector Fernandez and Strnadová, Jana and Banerjee, Esha and Manurung, Ruli and Stella, Antonio and Shimada, Atsuko and Kwak, Sookyoung and Mendonca, Gustavo and Lando, Tatiana and Nitisaroj, Rattima and Li, Josie},
    title = {CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
    booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Conference on Computational Natural Language Learning (CoNLL-2017), The SIGNLL Conference on Computational Natural Language Learning, August 3-4, Vancouver, BC, Canada},
    year = {2017},
    pages = {1--19},
    publisher = {Association for Computational Linguistics}
}

@article{pub9094,
    author = {Macketanz, Vivien and Avramidis, Eleftherios and Burchardt, Aljoscha and Helcl, Jindrich and Srivastava, Ankit},
    title = {Machine Translation: Phrase-Based, Rule-Based and Neural Approaches with Linguistic Evaluation},
    year = {2017},
    month = {6},
    volume = {17},
    number = {2},
    pages = {28--43},
    journal = {Cybernetics and Information Technologies (cait)},
    publisher = {De Gruyter}
}

@inproceedings{pub9563,
    author = {Avramidis, Eleftherios and Macketanz, Vivien and Lommel, Arle and Uszkoreit, Hans},
    title = {Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite},
    booktitle = {Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing. Conference of the Association for Machine Translation in the Americas (AMTA-2018), March 21-21, Boston, United States},
    year = {2018},
    month = {3},
    pages = {243--248},
    publisher = {Association for Machine Translation in the Americas}
}

@book{moeller2014t,
Title = {Quality of Experience: Advanced Concepts, Applications and Methods},
Author = {Möller, Sebastian and Raake, Alexander},
Year = {2014},
Isbn = {978-3-319-02681-7},
xdoi = {10.1007/978-3-319-02681-7},
Address = {Heidelberg},
Publisher = {Springer},
Abstract = {This pioneering book develops definitions and concepts related to Quality of Experience in the context of multimedia- and telecommunications-related applications, systems and services and applies these to various fields of communication and media technologies. The editors bring together numerous key-protagonists of the new discipline ``Quality of Experience'' and combine the state-of-the-art knowledge in one single volume.},
xurl = {https://www.qu.tu-berlin.de/fileadmin/fg41/publications-restricted/moeller_2014_quality-of-experience.-advanced-concepts.-applications-and-methods.pdf}
}

@book{polzehl2014a,
Title = {Personality in Speech - Assessment and Automatic Classification},
Author = {Polzehl, Tim},
Booktitle = {T-Labs Series in Telecommunication Services},
Year = {2014},
Isbn = {978-3-319-09516-5},
Publisher = {Springer},
Series = {XIV},
Toappear = {published}
}

@book{fernandez2016,
title = {Human and Automatic Speaker Recognition over Telecommunication Channels},
author = {F Gallardo, Laura},
Booktitle = {T-Labs Series in Telecommunication Services},
Year = {2016},
Publisher = {Springer},
}


@article{moeller2017j,
author = {Möller, Sebastian and Köster, Friedemann},
year = {2017},
month = {12},
pages = {},
title = {Review of recent standardization activities in speech quality of experience},
volume = {2},
journal = {Quality and User Experience},
xdoi = {10.1007/s41233-017-0012-7}
}

@TechReport{GEW,
  Title                    = {{Geneva Emotion Wheel rating study}},
  Author                   = {Sacharin, V. and Schlegel, K. and and Scherer, K. R.},
  Institution              = {Center for Person, Kommunikation, Aalborg University},
  Year                     = {2012},
  Address                  = {NCCR Affective Sciences},
}


@article{moeller2013ii,
author = {Möller, Sebastian and Heusdens, R.},
year = {2013},
month = {09},
pages = {1955-1967},
title = {Objective Estimation of Speech Quality for Communication Systems},
volume = {101},
journal = {Proceedings of the IEEE},
xdoi = {10.1109/JPROC.2013.2241374}
}
@inproceedings{burkhardt2017a,
Title = {Complex Emotions -- The Simultaneous Simulation of Emotion-Related States In Synthesized Speech},
Author = {Burkhardt, Felix and Weiss, Benjamin},
Booktitle = {28\textsuperscript{th} Konferenz Elektronische Sprachsignalverarbeitung (ESSV)},
Pages = {67--74},
Year = {2017},
Isbn = {978-3-95908-094-1},
xAddress = {Dresden},
xEditor = {Möbius, B. and Steiner, I. and Trouvain, J.},
xPublisher = {TUDpress},
Howpublished = {full},
xurl = {https://www.qu.tu-berlin.de/fileadmin/fg41/publications-restricted/burkhardt_2017_complex-emotions-..-the-simultaneous-simulation-of-emotion.related-states-in-synthesized-speech.pdf},
xurl2 = {http://essv2017.coli.uni-saarland.de/pdfs/Burkhardt.pdf},
Reviewed = {no},
Toappear = {published}
}

@INPROCEEDINGS{7351631,
  author={G. {Letournel} and A. {Bugeau} and V. -. {Ta} and J. -. {Domenger}},
  booktitle={2015 IEEE ICIP'15}, 
  title={Face de-identification with expressions preservation}, 
  year={2015},
  volume={},
  number={},
  pages={4366-4370},
  }


@Inbook{Aggarwal2008,
author="Aggarwal, Charu C.
and Yu, Philip S.",
editor="Aggarwal, Charu C.
and Yu, Philip S.",
title="A General Survey of Privacy-Preserving Data Mining Models and Algorithms",
bookTitle="Privacy-Preserving Data Mining: Models and Algorithms",
year="2008",
publisher="Springer US",
address="Boston, MA",
pages="11--52",
abstract="In recent years, privacy-preserving data mining has been studied extensively, because of the wide proliferation of sensitive information on the internet. A number of algorithmic techniques have been designed for privacy-preserving data mining. In this paper, we provide a review of the state-of-the-art methods for privacy. We discuss methods for randomization, k-anonymization, and distributed privacy-preserving data mining. We also discuss cases in which the output of data mining applications needs to be sanitized for privacy-preservation purposes. We discuss the computational and theoretical limits associated with privacy-preservation over high dimensional data sets.",
isbn="978-0-387-70992-5",
xdoi="10.1007/978-0-387-70992-5_2",
xurl="https://xdoi.org/10.1007/978-0-387-70992-5_2"
}

@Incollection{BremNiebuhr2020,
  Title                    = {Dress to impress? On the interaction of attire with prosody and gender in the perception of speaker charisma},
  Address                  = {New York},
  Author                   = {A. Brem and O. Niebuhr},
  Booktitle                = {Voice attractiveness: Concepts, methods, and data},
  Pages                    = {301--309},
  Year                     = {2020},
}%Editor                   = {B. Weiß and J. Trouvain and J. Ohala},

@inproceedings{pub11064,
    author = {Schwarzenberg, Robert and Castle, Steffen},
    title = {Pattern-Guided Integrated Gradients},
    booktitle = {Proceedings of the ICML 2020 Workshop on Human Interpretability in Machine Learning (WHI)},
    year = {2020},
    publisher = {ICML}
}
@inproceedings{pub9964,
    author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
    title = {Learning Explanations From Language Data},
    booktitle = {EMNLP Workshop on Interpreting and Analysing Neural Networks for NLP (BlackboxNLP). Conference on Emperical Methods in Natural Language Processing (EMNLP-2018), October 31-November 4, Brussels, Belgium},
    year = {2018},
    publisher = {EMNLP}
}

@Article{SiegertJMUI:2013,
  Title                    = {{Inter-Rater Reliability for Emotion Annotation in Human-Computer Interaction -- Comparison and Methodological Improvements}},
  Author                   = {Ingo Siegert and Ronald Böck and Andreas Wendemuth},
  Journal                  = {Journal of Multimodal User Interfaces},
  Pages                    = {17--28},
  Volume                   = {8},
  Year                     = {2014},
  Issue                   = {1},

  Keyword                  = {pubown},
  
  Owner                    = {papaingo},
  Timestamp                = {2014.08.10}
}

@inproceedings{engel2020self,
  title={Self-supervised Pitch Detection by Inverse Audio Synthesis},
  author={Engel, Jesse and Swavely, Rigel and Hantrakul, Lamtharn Hanoi and Roberts, Adam and Hawthorne, Curtis},
  xbooktitle={International Conference on Machine Learning (ICML'20)},
  year={2020}
}

@ARTICLE{bellur2020taslp,
  author={A. {Bellur} and M. {Elhilali}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title={Audio object classification using distributed beliefs and attention},
  year={2020},
  volume={},
  number={},
  pages={1-1},
  xdoi={10.1109/TASLP.2020.2966867}
}

@inproceedings{kothinti2019joint,
  title={Joint acoustic and class inference for weakly supervised sound event detection},
  author={Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={36--40},
  year={2019},
  organization={IEEE}
}

@article{chakrabarty2019gestalt,
  title={A Gestalt inference model for auditory scene segregation},
  author={Chakrabarty, Debmalya and Elhilali, Mounya},
  journal={PLoS computational biology},
  volume={15},
  number={1},
  pages={e1006711},
  year={2019},
  publisher={Public Library of Science}
}

@article{parkjoint2020dcase,
  title={Joint Acoustic and Supervised Inference for Sound Event Detection},
  author={Park, Sangwook and Bellur, Ashwin and Kothinti, Sandeep and Kapourchali, Masoumeh Heidari and Elhilali, Mounya}
}

@incollection{elhilali2019modulation,
  title={Modulation representations for speech and music},
  author={Elhilali, Mounya},
  booktitle={Timbre: Acoustics, perception, and cognition},
  pages={335--359},
  year={2019},
  publisher={Springer}
}

@inproceedings{bellur2020bio,
  title={Bio-Mimetic Attentional Feedback in Music Source Separation},
  author={Bellur, Ashwin and Elhilali, Mounya},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8718--8722},
  year={2020},
  organization={IEEE}
}

@article{stober2015arXiv:1511.04306,
  author = {Sebastian Stober and Avital Sternin and Adrian M. Owen and Jessica A. Grahn},
  title = {Deep Feature Learning for {EEG} Recordings},
  journal = {arXiv preprint arXiv:1511.04306},
  year = {2015},
  note = {submitted as conference paper for ICLR 2016},
  xurl = {http://arxiv.org/abs/1511.04306}
}

@article{Parisi:2018:LLL:NN,
  author    = {German Ignacio Parisi and
               Ronald Kemker and
               Jose L. Part and
               Christopher Kanan and
               Stefan Wermter},
  title     = {Continual Lifelong Learning with Neural Networks: {A} Review},
  journal   = {Neural Networks},
  volume    = {113},
  year      = {2019},
  pages     = {54-71}
}

@inproceedings{Dang:2017:SurveyAED:ICOT,
abstract = {Deep learning has achieved state of the art in various machine learning problems, such as computer vision, speech recognition, and natural language processing. Sound event detection (SED), which is about recognizing audio events in real-life environments, has attracted a lot of attention recently. Many works have been successful when applying deep learning techniques for the SED problem as can be seen in Detection and Classification of Acoustic Scenes and Events (DCASE) challenge 2016-2017. In this paper, we present a review of the SED problem and discuss different deep learning approaches for the problem.},
address = {Singapore, Singapore},
author = {Dang, An and Vu, Toan H. and Wang, Jia Ching},
booktitle = {Proceedings of the International Conference on Orange Technologies (ICOT)},
xdoi = {10.1109/ICOT.2017.8336092},
file = {:C$\backslash$:/Users/abr/Downloads/08336092.pdf:pdf},
keywords = {Convolutional neural networks,Deep learning,Neural networks,Recurrent neural networks,Sound event detection},
pages = {75--78},
title = {{A survey of Deep Learning for Polyphonic Sound Event Detection}},
year = {2017},
note = {8 - 10 December}
}


@article{Barchiesi:2015:ASC:SPM,
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {Barchiesi, Daniele and Giannoulis, D. Dimitrios and Stowell, Dan and Plumbley, Mark D.},
xdoi = {10.1109/MSP.2014.2326181},
file = {:C$\backslash$:/Users/abr/Downloads/BarchiesiGiannoulisStowellP15-asc{\_}accepted.pdf:pdf},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {machine{\_}listening,acoustic{\_}scene{\_}classification},
number = {3},
pages = {16--34},
title = {{Acoustic Scene Classification: Classifying environments from the sounds they produce}},
volume = {32},
year = {2015}
}


@InProceedings{Abesser:2019:Stadtlaerm:DCASE,
  author =        {Abe{\ss}er, Jakob and G{\"{o}}tze, Marco and Clau{\ss}, Tobias and Zapf, Dominik and K{\"{u}}hn, Christian and Lukashevich, Hanna and K{\"{u}}hnlenz, Stephanie and Mimilakis, Stylianos},
  title =         {{Urban Noise Monitoring in the Stadtl{\"{a}}rm Project - A Field Report}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Abesser{\_}2019{\_}DCASE.pdf:pdf},
  keywords =      {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
  mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city}
}

@InProceedings{Abesser:2017:ASC:DCASE,
  author =    {Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Gr{\"{a}}fe, Robert and Lukashevich, Hanna},
  title =     {{Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2017},
  organization =   {Munich, Germany},
  note =     {16-17 November},
  abstract =  {Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban en-vironments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-the-art CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configura-tion leads to a classification accuracy of 75{\%} for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.},
  file =      {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Abesser{\_}165.pdf:pdf}
}

@InProceedings{Abidin:2017:LBP:ICASSP,
  author =        {Abidin, Shamsiah and Togneri, Roberto and Sohel, Ferdous},
  title =         {{Enhanced LBP Texture Features from Time Frequency Representations for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {626--630},
  organization =       {New Orleans, LA, USA},
  note =         {5-9 March},
  file =          {:Users/jakobabeer/Downloads/07952231.pdf:pdf},
  isbn =          {9781509041176},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Amiriparian:2018:GANASC:EUSIPCO,
  author =    {Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Gerczuk, Maurice and Pugachevskiy, Sergey and Schuller, Bj{\"{o}}rn},
  title =     {{A Fusion of Deep Convolutional Generative Adversarial Networks and Sequence to Sequence Autoencoders for Acoustic Scene Classification}},
  booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
  year =      {2018},
  pages =     {977--981},
  organization =   {Rome, Italy},
  note =     {3-7 September},
  abstract =  {Unsupervised representation learning shows high promise for generating robust features for acoustic scene analysis. In this regard, we propose and investigate a novel combination of features learnt using both a deep convolutional generative adversarial network (DCGAN) and a recurrent sequence to sequence autoencoder (S2SAE). Each of the representation learning algorithms is trained individually on spectral features extracted from audio instances. The learnt representations are: (i) the activations of the discriminator in case of the DCGAN, and (ii) the activations of a fully connected layer between the decoder and encoder units in case of the S2SAE. We then train two multilayer perceptron neural networks on the DCGAN and S2SAE feature vectors to predict the class labels. The individual predicted labels are combined in a weighted decision-level fusion to achieve the final prediction. The system is evaluated on the development partition of the acoustic scene classification data set of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). In comparison to the baseline, the accuracy is increased from 74.8 {\%} to 86.4 {\%} using only the DCGAN, to 88.5 {\%} on the development set using only the S2SAE, and to 91.1 {\%} after fusion of the individual predictions.},
  xdoi =       {10.23919/EUSIPCO.2018.8553225},
  file =      {:Users/jakobabeer/Downloads/08553225.pdf:pdf},
  isbn =      {9789082797015},
  issn =      {22195491},
  keywords =  {Acoustic scene classification,Generative adversarial networks,Sequence to sequence autoencoders,Unsupervised feature learning}
}

@Article{Aytar:2016:SoundNet:NIPS,
  author =        {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  title =         {{SoundNet: Learning Sound Representations from Unlabeled Video}},
  journal =       {Advances in Neural Information Processing Systems (NIPS)},
  year =          {2016},
  publisher = {Curran Associates, Inc., NY, USA},
  number =        {Nips},
  pages =         {892--900},
  abstract =      {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
  archiveprefix = {arXiv},
  arxivid =       {1610.09001},
  eprint =        {1610.09001},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Aytar, Vondrick, Torralba - 2016 - SoundNet Learning Sound Representations from Unlabeled Video.pdf:pdf},
  issn =          {10495258}
}

@Article{Bach:2015:LRP:PLOS,
  author =   {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
  title =    {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
  journal =  {PLoS ONE},
  year =     {2015},
  volume =   {10},
  number =   {7},
  pages =    {1--46},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest.We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  xdoi =      {10.1371/journal.pone.0130140},
  file =     {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/73bbd4448083b01b5a9389b3c37f5425aac0.pdf:pdf},
  issn =     {19326203},
  pmid =     {26161953}
}

@InProceedings{Bae:2016:LSTMCNN:DCASE,
  author =    {Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo},
  title =     {{Acoustic Scene Classification using Parallel Combination of LSTM and CNN}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2016},
  organization =   {Budapest, Hungary},
  note =     {3 September},
  file =      {:Users/jakobabeer/Downloads/Bae-DCASE2016workshop.pdf:pdf}
}

@InProceedings{Basbug:2019:SpatialPyramidPoolingASC:ICSC,
  author =        {Basbug, Ahmet Melih and Sert, Mustafa},
  title =         {{Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks}},
  booktitle =     {Proceedings of the 13th IEEE International Conference on Semantic Computing (ICSC),},
  year =          {2019},
  pages =         {128--131},
  organization =       {Newport, CA, USA, 30 January - 1 February},
  note =         {18-20 November},
  xdoi =           {10.1109/ICOSC.2019.8665547},
  file =          {:Users/jakobabeer/Downloads/08665547.pdf:pdf},
  isbn =          {9781538667835},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Bear:2019:JointASCAED:INTERSPEECH,
  author =        {Bear, Helen L. and Nolasco, In{\^{e}}s and Benetos, Emmanouil},
  title =         {{Towards joint sound scene and polyphonic sound event recognition}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2019},
  volume =        {2019-Septe},
  pages =         {4594--4598},
  organization =       {Graz, Austria},
  note =         {2-15 November},
  abstract =      {Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two separate tasks in the field of computational sound scene analysis. In this work, we present a new dataset with both sound scene and sound event labels and use this to demonstrate a novel method for jointly classifying sound scenes and recognizing sound events. We show that by taking a joint approach, learning is more efficient and whilst improvements are still needed for sound event detection, SED results are robust in a dataset where the sample distribution is skewed towards sound scenes.},
  archiveprefix = {arXiv},
  arxivid =       {1904.10408},
  xdoi =           {10.21437/Interspeech.2019-2169},
  eprint =        {1904.10408},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Bear, Nolasco, Benetos - 2019 - Towards joint sound scene and polyphonic sound event recognition.pdf:pdf},
  issn =          {19909772},
  journal =       {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  keywords =      {Acoustic scene classification,CRNN,Computational sound scene analysis,Sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
  mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification}
}

@Article{Bello:2018:SONYC:CACM,
  author =        {Bello, Juan Pablo and Silva, Claudio and Nov, Oded and DuBois, R. Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish},
  title =         {{SONYC: A System for the Monitoring, Analysis and Mitigation of Urban Noise Pollution}},
  journal =       {Communications of the ACM (CACM)},
  year =          {2019},
  volume =        {62},
  number =        {2},
  abstract =      {We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs. SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment. SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.},
  archiveprefix = {arXiv},
  arxivid =       {1805.00889},
  eprint =        {1805.00889},
  file =          {:Users/jakobabeer/Downloads/bello{\_}sonyc{\_}cacm{\_}2018.pdf:pdf},
  isbn =          {1234567245},
  keywords =      {acm reference format,citizen science,cyber-physical systems,machine listening,noise pollution,sensor networks,smart cities,visualization},
  xurl =           {http://arxiv.org/abs/1805.00889}
}

@InProceedings{Benetos:2012:ASC:DAFX,
  author =        {Benetos, Emmanouil and Lagrange, Mathieu and Dixon, Simon},
  title =         {{Characterisation of Acoustic Scenes using a Temporally-Constrained Shift-Invariant Model}},
  booktitle =     {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
  year =          {2012},
  pages =         {1--7},
  organization =       {York, UK},
  note =         {17-21 September},
  abstract =      {In this paper, we propose a method for modeling and classifying acoustic scenes using temporally-constrained shift-invariant probabilistic latent component analysis (SIPLCA). SIPLCA can be used for extracting time-frequency patches from spectrograms in an unsupervised manner. Component-wise hidden Markov models are incorporated to the SIPLCA formulation for enforcing temporal constraints on the activation of each acoustic component. The time-frequency patches are converted to cepstral coefficients in order to provide a compact representation of acoustic events within a scene. Experiments are made using a corpus of train station recordings, classified into 6 scene classes. Results show that the proposed model is able to model salient events within a scene and outperforms the non-negative matrix factorization algorithm for the same task. In addition, it is demonstrated that the use of temporal constraints can lead to improved performance.},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/dafx12{\_}submission{\_}30.pdf:pdf},
  journal =       {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Bisot:2015:ASC:EUSIPCO,
  author =        {Bisot, Victor and Essid, Slim and Richard, Gael},
  title =         {{HOG and Subband Power Distribution Image Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the 23rd European Signal Processing Conference (EUSIPCO)},
  year =          {2015},
  pages =         {719--723},
  organization =       {Nice, France},
  note =         {31 August - 4 September},
  abstract =      {Acoustic scene classification is a difficult problem mostly due to the high density of events concurrently occurring in audio scenes. In order to capture the occurrences of these events we propose to use the Subband Power Distribution (SPD) as a feature. We extract it by computing the histogram of amplitude values in each frequency band of a spectrogram image. The SPD allows us to model the density of events in each frequency band. Our method is evaluated on a large acoustic scene dataset using support vector machines. We outperform the previous methods when using the SPD in conjunction with the histogram of gradients. To reach further improvement, we also consider the use of an approximation of the earth mover's distance kernel to compare histograms in a more suitable way. Using the so-called Sinkhorn kernel improves the results on most of the feature configurations. Best performances reach a 92.8{\%} F1 score.},
  xdoi =           {10.1109/EUSIPCO.2015.7362477},
  file =          {:Users/jakobabeer/Downloads/07362477.pdf:pdf},
  isbn =          {9780992862633},
  keywords =      {Acoustic scene classification,Sinkhorn distance,acoustic{\_}scene{\_}classification,machine{\_}listening,subband power distribution image,support vector machine},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Bisot:2017:ASC:TASLP,
  author =   {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Ga{\"{e}}l},
  title =    {{Feature Learning with Matrix Factorization Applied to Acoustic Scene Classification}},
  journal =  {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =     {2017},
  volume =   {25},
  number =   {6},
  pages =    {1216--1229},
  abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
  xdoi =      {10.1109/TASLP.2017.2690570},
  file =     {:Users/jakobabeer/Downloads/main{\_}bisot2016.pdf:pdf},
  issn =     {23299290},
  keywords = {Acoustic scene classification,feature learning,matrix factorization}
}

@InProceedings{Bisot:2017:NFASC:DCASE,
  author =        {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael},
  title =         {{Nonnegative Feature Learning Methods for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Bisot{\_}194.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Boddapati:2017:ASC:PCS,
  author =    {Boddapati, Venkatesh and Petef, Andrej and Rasmusson, Jim and Lundberg, Lars},
  title =     {{Classifying environmental sounds using image recognition networks}},
  journal =   {Procedia Computer Science},
  year =      {2017},
  volume =    {112},
  pages =     {2048--2056},
  abstract =  {Automatic classification of environmental sounds, such as dog barking and glass breaking, is becoming increasingly interesting, especially for mobile devices. Most mobile devices contain both cameras and microphones, and companies that develop mobile devices would like to provide functionality for classifying both videos/images and sounds. In order to reduce the development costs one would like to use the same technology for both of these classification tasks. One way of achieving this is to represent environmental sounds as images, and use an image classification neural network when classifying images as well as sounds. In this paper we consider the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds. We evaluate the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition (AlexNet and GoogLeNet). Our experiments show that we obtain good classification accuracy for the three datasets.},
  xdoi =       {10.1016/j.procs.2017.08.250},
  file =      {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Boddapati et al. - 2017 - Classifying environmental sounds using image recognition networks.pdf:pdf},
  issn =      {18770509},
  keywords =  {Convolutional Neural Networks,Deep Learning,Environmental Sound Classification,GPU Processing,Image Classification},
  publisher = {Elsevier B.V.},
  xurl =       {http://dx.xdoi.org/10.1016/j.procs.2017.08.250}
}

@InProceedings{Chen:2019:ASC:DCASE,
  author =    {Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong},
  title =     {{Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  organization =   {New York, NY, USA},
  note =     {25-26 October},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Zhang{\_}34.pdf:pdf}
}

@InProceedings{Chen:2018:Scalogram:INTERSPEECH,
  author =        {Chen, Hangting and Zhang, Pengyuan and Bai, Haichuan and Yuan, Qingsheng and Bao, Xiuguo and Yan, Yonghong},
  title =         {{Deep convolutional neural network with scalogram for audio scene modeling}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2018},
  pages =         {3304--3308},
  organization =       {Hyderabad, India},
  note =         {2-6 September},
  abstract =      {Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5{\%} in the evaluation step in DCASE2016.},
  xdoi =           {10.21437/Interspeech.2018-1524},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1524.pdf:pdf},
  issn =          {19909772},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Chen:2019:ASCFilters:ICASSP,
  author =        {Chen, Hangting and Zhang, Pengyuan and Yan, Yonghong},
  title =         {{An Audio Scene Classification Framework with Embedded Filters and a DCT-Based Temporal Module}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {835--839},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0000835.pdf:pdf},
  isbn =          {9781538646588},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Cho:DCASE:LargeMarginCNN:DCASE,
  author =        {Cho, Janghoon and Yun, Sungrack and Park, Hyoungwoo and Eum, Jungyun and Hwang, Kyuwoong},
  title =         {{Acoustic Scene Classification Based on a Large-Margin Factorized CNN}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {45--49},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  archiveprefix = {arXiv},
  arxivid =       {1910.06784},
  xdoi =           {10.33682/8xh4-jm46},
  eprint =        {1910.06784},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2019 - Acoustic Scene Classification Based on a Large-Margin Factorized CNN.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Dang:2018:ASCMulti:ICCE,
  author =        {Dang, An and Vu, Toan H. and Wang, Jia-Ching},
  title =         {{Acoustic Scene Classification using Convolutional Neural Networks and Multi-Scale Multi-Feature Extraction}},
  booktitle =     {Proceedings of the IEEE International Conference on Consumer Electronics (ICCE)},
  year =          {2018},
  organization =       {Hue City, Vietnam},
  note =         {18-20 July},
  xdoi =           {10.1109/ICCE.2018.8326315},
  file =          {:Users/jakobabeer/Downloads/08326315.pdf:pdf},
  isbn =          {9781538630259},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Drossos:2019:DomainAdaptation:WASPAA,
  author =        {Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas},
  title =         {{Unsupervised Adversarial Domain Adaptation based on the Wasserstein Distance for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year =          {2019},
  pages =         {259--263},
  organization =       {New Paltz, NY, USA},
  note =         {20-23 October},
  publisher =     {IEEE},
  abstract =      {A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of $\Delta$-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\%} to 45{\%}, using the TUT Acoustic Scenes dataset.},
  file =          {::},
  keywords =      {Acoustic scene classification,Wasserstein distance,acoustic{\_}scene{\_}classificaiton,adversarial training,domain{\_}adaptation,machine{\_}listening,unsupervised domain adaptation},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,domain{\_}adaptation,machine{\_}listening}
}

@Article{Drossos:2020:SED:ARXIV,
  author =        {Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas},
  title =         {{Sound Event Detection with Depthwise Separable and Dilated Convolutions}},
  journal =       {ArXiv pre-prints},
  year =          {2020},
  abstract =      {State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85{\%} and average training time per epoch by 78{\%}, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6{\%} and 3.8{\%}, respectively.},
  archiveprefix = {arXiv},
  arxivid =       {2002.00476},
  eprint =        {2002.00476},
  file =          {::},
  xurl =           {http://arxiv.org/abs/2002.00476}
}

@InProceedings{Fonseca:2017:ASC:DACSE,
  author =        {Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier},
  title =         {{Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Fonseca{\_}181.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Gemmeke:2017:Audioset:ICASSP,
  author =    {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  title =     {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2017},
  pages =     {776--780},
  organization =   {New Orleans, LA, USA},
  note =     {5-9 March},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45857.pdf:pdf}
}

@InProceedings{Gharib:2018:DomainAdaptationASC:DCASE,
  author =        {Gharib, Shayan and Drossos, Konstantinos and Emre, Cakir and Serdyuk, Dmitriy and Virtanen, Tuomas},
  title =         {{Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Gharib et al. - 2018 - Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
  mendeley-tags = {acoustic{\_}scene{\_}classification,domain{\_}adaptation}
}

@InProceedings{Goodfellow:2014:GAN:NIPS,
  author =    {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and {Yoshua Bengio}},
  title =     {{Generative Adversarial Nets}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year =      {2014},
  editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  publisher = {Curran Associates, Inc., Red Hook, NY, USA},
  pages =     {2672--2680},
  abstract =  {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/5423-generative-adversarial-nets.pdf:pdf},
  issn =      {10495258}
}

@InProceedings{Gordon:2018:MorphNet:CVPR,
  author =        {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien Ju and Choi, Edward},
  title =         {{MorphNet: Fast {\&} Simple Resource-Constrained Structure Learning of Deep Networks}},
  booktitle =     {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year =          {2018},
  pages =         {1586--1595},
  organization =       {Salt Lake City, UT, USA},
  note =         {18-23 June},
  abstract =      {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
  archiveprefix = {arXiv},
  arxivid =       {1711.06798},
  xdoi =           {10.1109/CVPR.2018.00171},
  eprint =        {1711.06798},
  file =          {::},
  isbn =          {9781538664209},
  issn =          {10636919},
  journal =       {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@InProceedings{Green:2017:SpatialFeaturesASC:DCASE,
  author =        {Green, Marc C. and Murphy, Damian},
  title =         {{Acoustic Scene Classification using Spatial Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  abstract =      {Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or bin-aural datasets. This paper introduces EigenScape -a new dataset of 4th-order Ambisonic acoustic scene recordings -and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient -Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Direc-tional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The dif-ferences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Green{\_}126.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Grollmisch:2019:ISA:EUSIPCO,
  author =        {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
  title =         {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis (ISA)}},
  booktitle =     {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
  year =          {2019},
  pages =         {1--5},
  organization =       {A Coruna, Spain},
  note =         {2-6 September},
  file =          {:Users/jakobabeer/Sync/Jakob/Programming/Repositories/publications/pdf/Grollmisch{\_}2019{\_}EUSIPCO.pdf:pdf},
  keywords =      {idmt},
  mendeley-tags = {idmt}
}

@InProceedings{Han:2017:BinauralASC:DCASE,
  author =        {Han, Yoonchang and Park, Jeongsoo and Lee, Kyogu},
  title =         {{Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Han{\_}206.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Huang:2019:ASCEnsemble:DCASE,
  author =        {Huang, Jonathan and Lu, Hong and Lopez-Meyer, Paulo and Maruri, Hector A. Cordourier and Ontiveros, Juan A. del Hoyo},
  title =         {{Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {94--98},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2019 - Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Jati:2020:ASC:ICASSP,
  author =        {Jati, Arindam and Nadarajan, Amrutha and Mundnich, Karel and Narayanan, Shrikanth},
  title =         {{Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting}},
  booktitle =     {submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2020},
  note = {4-8 May},
  organization =       {Barcelona, Spain},
  abstract =      {Devices capable of detecting and categorizing acoustic scenes have numerous applications such as providing context-aware user experiences. In this paper, we address the task of characterizing acoustic scenes in a workplace setting from audio recordings collected with wearable microphones. The acoustic scenes, tracked with Bluetooth transceivers, vary dynamically with time from the egocentric perspective of a mobile user. Our dataset contains experience sampled long audio recordings collected from clinical providers in a hospital, who wore the audio badges during multiple work shifts. To handle the long egocentric recordings, we propose a Time Delay Neural Network{\~{}}(TDNN)-based segment-level modeling. The experiments show that TDNN outperforms other models in the acoustic scene classification task. We investigate the effect of primary speaker's speech in determining acoustic scenes from audio badges, and provide a comparison between performance of different models. Moreover, we explore the relationship between the sequence of acoustic scenes experienced by the users and the nature of their jobs, and find that the scene sequence predicted by our model tend to possess similar relationship. The initial promising results reveal numerous research directions for acoustic scene classification via wearable devices as well as egocentric analysis of dynamic acoustic scenes encountered by the users.},
  archiveprefix = {arXiv},
  arxivid =       {1911.03843},
  eprint =        {1911.03843},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Jati et al. - 2020 - Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting(2).pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  xurl =           {http://arxiv.org/abs/1911.03843}
}

@InProceedings{Jimenez:2017:ShiftInvariantASC:DCASE,
  author =        {Jim{\'{e}}nez, Abelino and Elizalde, Benjam{\'{i}}n and Raj, Bhiksha},
  title =         {{DCASE 2017 Task 1: Acoustic Scene Classification using Shift-Invariant Kernels and Random Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Jimenez{\_}195.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Kong:2019:SceneGeneration:ICASSP,
  author =    {Kong, Qiuqiang and Xu, Yong and Iqbal, Turab and Cao, Yin and Wang, Wenwu and Plumbley, Mark D},
  title =     {{Acoustic Scene Generation with Conditional SampleRNN}},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2019},
  pages =     {925--929},
  organization =   {Brighton, UK},
  note =     {12-17 May},
  file =      {::},
  isbn =      {9781538646588}
}

@InProceedings{Kosmider:2019:DeviceCalibration:DCASE,
  author =    {Kosmider, Michal},
  title =     {{Calibrating Neural Networks for Secondary Recording Devices}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  organization =   {New York, NY, USA},
  note =     {25-26 October},
  file =      {::}
}

@InProceedings{Koutini:2019:ReceptiveField:DCASE,
  author =        {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
  title =         {{Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {124--128},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Koutini, Eghbal-zadeh, Widmer - 2019 - Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Koutini:2019:ASC:DCASE,
  author =    {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard and Kepler, Johannes},
  title =     {{CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  pages =     {1--5},
  organization =   {New York, NY, USA},
  note =     {25-26 October},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Koutini{\_}99.pdf:pdf}
}

@InProceedings{Kumar:2018:ASC:ICASSP,
  author =        {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
  title =         {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2018},
  pages =         {326--330},
  organization =       {Alberta, Canada},
  note =         {15-20 April},
  abstract =      {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
  annote =        {- for weakly labeled data - CNN - audio of variable length - usable for transfer learning for domain and task adaptation - problem on scene classification: -- few datasets -- labeling expensive/difficult -- begin and end of event subjective - audioset with weak labels (DCASE2017) - "Soundnet" transfer visual model to audio data - train on "Audioset"(weakly labeled example from Youtube with 527 classes) - tested on ESC-50 (same task but different domain) and DCASE2016 (task adaption) - SLAT -{\textgreater} strong label assumption training (class correct for all patches) - logmel spec as input, 44.1kHz, 128 mel bands, winsize 23ms, overlap 11.5 - global pooling at the end -{\textgreater} fully conv for variable length of input - ESC-50: outperform sota by 9.3{\%} - DCASE2016: absolute improvement of 4.1{\%} - Audioset: sota results},
  xdoi =           {10.1109/ICASSP.2018.8462200},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Khadkevich, Fugen - 2018 - Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events an(3).pdf:pdf},
  isbn =          {978-1-5386-4658-8},
  issn =          {15206149},
  keywords =      {Audio Event Classification,Learning Representations,Transfer Learning,Weak Label Learning,acmus,dnn better},
  mendeley-tags = {acmus,dnn better},
  xurl =           {https://ieeexplore.ieee.org/document/8462200/}
}

@InProceedings{Lassseck:2018:Bird:DCASE,
  author =        {Lasseck, Mario},
  title =         {{Acoustic bird detection with deep convolutional neural networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE)},
  year =          {2018},
  pages =         {143--147},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  abstract =      {This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 $\backslash$$\backslash${\%} on the public challenge leaderboard.},
  file =          {::},
  keywords =      {Bird Detection,Data Augmentation,Deep Convolutional Neural Networks,Deep Learning,bird{\_}recognition,machine{\_}listening},
  mendeley-tags = {bird{\_}recognition,machine{\_}listening}
}

@InProceedings{Lehner:2019:ASCReject:DCASE,
  author =        {Lehner, Bernhard and Koutini, Khaled and Schwarzlm{\"{u}}ller, Christopher and Gallien, Thomas and Widmer, Gerhard},
  title =         {{Acoustic Scene Classification with Reject Option based on Resnets}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Downloads/DCASE20191.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening}
}

@InProceedings{Li:2018:ASC:ICALIP,
  author =        {Li, Yanxiong and Li, Xianku and Zhang, Yuhan and Wang, Wucheng and Liu, Mingle and Feng, Xiaohui},
  title =         {{Acoustic Scene Classification Using Deep Audio Feature and BLSTM Network}},
  booktitle =     {Proceedings of the 6th International Conference on Audio, Language and Image Processing (ICALIP)},
  year =          {2018},
  pages =         {371--374},
  organization =       {Shanghai, China},
  note =         {16-17 July},
  abstract =      {Although acoustic scene classification has been received great attention from researchers in the field of audio signal processing, it is still a challenging and unsolved task to date. In this paper, we present our work of acoustic scene classification for the challenge of the Detection and Classification of Acoustic Scenes and Events 2017, i.e., DCASE2017 challenge, using a feature of Deep Audio Feature (DAF) for acoustic scene representation and a classifier of Bidirectional Long Short Term Memory (BLSTM) network for acoustic scene classification. We first use a deep neural network to generate the DAF from Mel frequency cepstral coefficients, and then adopt a network of BLSTM fed by the DAF for acoustic scene classification. When evaluated on the official datasets of the DCASE2017 challenge, the proposed system outperforms the baseline system in terms of classification accuracy.},
  xdoi =           {10.1109/ICALIP.2018.8455765},
  file =          {:Users/jakobabeer/Downloads/08455765.pdf:pdf},
  isbn =          {9781538651957},
  keywords =      {acoustic scene classification,acoustic{\_}scene{\_}classification,bidirectional long short term memory network,deep audio feature,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Li:2019:MultilevelAttention:ICMEW,
  author =        {Li, Zhitong and Hou, Yuanbo and Xie, Xiang and Li, Shengchen and Zhang, Liqiang and Du, Shixuan and Liu, Wei},
  title =         {{Multi-Level Attention Model with Deep Scattering Spectrum for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  year =          {2019},
  pages =         {396--401},
  organization =       {Shanghai, China},
  note =         {8-12 July},
  abstract =      {Acoustic scene classification (ASC) refers to the classification of audio into one of predefined classes that characterize the environment. People are used to combine log-mel filterbank features with convolutional neural network (CNN) to build ASC system. In this paper, we explore the use of deep scattering spectrum (DSS) features combined with a multi-level attention model based on CNN for ASC tasks. First, the time scatter and frequency scatter coefficients of DSS with different resolutions are explored as ASC features. Second, we incorporate a multi-level attention model into CNN to build the classification system. We then evaluate the proposed approach on the IEEE challenge of detection and classification of acoustic scenes and events 2018 (DCASE 2018) dataset. Results show that the DSS features provide between a 11{\%}-14{\%} relative improvement in accuracy over log-mel features, within a state-of-the-art framework. The application of multilevel attention model on CNN can improve the accuracy by nearly 5{\%}. The highest accuracy of our proposed system is 78.3{\%} on the development set.},
  xdoi =           {10.1109/ICMEW.2019.00074},
  file =          {:Users/jakobabeer/Downloads/08794892.pdf:pdf},
  isbn =          {9781538692141},
  keywords =      {Acoustic scene classification,DCASE 2018,Deep scattering spectrum,Multi-level attention mechanism,acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Lostanlen:2018:PCEN:SPL,
  author =        {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
  title =         {{Per-channel energy normalization: Why and how}},
  journal =       {IEEE Signal Processing Letters},
  year =          {2019},
  volume =        {26},
  number =        {1},
  pages =         {39--43},
  abstract =      {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named per-channel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This letter investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Second, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Third, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise, PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
  xdoi =           {10.1109/LSP.2018.2878620},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Lostanlen et al. - 2019 - Per-channel energy normalization Why and how(2).pdf:pdf},
  issn =          {10709908},
  keywords =      {Acoustic noise,acoustic sensors,acoustic signal detection,machine{\_}listening,signal classification,spectrogram},
  mendeley-tags = {machine{\_}listening}
}

@InProceedings{Maka:2018:FeatureSpaceASC:DCASE,
  author =        {Maka, Tomasz},
  title =         {{Audio Feature Space Analysis for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Maka - 2018 - Audio Feature Space Analysis for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Marchi:2016:MKSL:DCASE,
  author =        {Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
  title =         {{Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  organization =       {Budapest, Hungary},
  note =         {3 September},
  abstract =      {We propose a system for acoustic scene classification using pair-wise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contri-bution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the 'one-against-all' paradigm to perform a pair-wise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4{\%} is achieved.},
  file =          {:Users/jakobabeer/Downloads/Marchi-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mariotti:2018:DeepVisionASC:DCASE,
  author =        {Mariotti, Octave and Cord, Matthieu and Schwander, Olivier},
  title =         {{Exploring Deep Vision Models for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mariotti, Cord, Schwander - 2018 - Exploring Deep Vision Models for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mars:2019:BinauralASC:DCASE,
  author =        {Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chongsoon},
  title =         {{Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {149--153},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  xdoi =           {10.33682/6c9z-gd15},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mars et al. - 2019 - Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mcdonnell:2019:AcousticScenes:DCASE,
  author =        {Mcdonnell, Mark D and Gao, Wei},
  title =         {{Acoustic Scene Classification Using Deep Residual Networks With Late Fusion of Separated High and Low Frequency Paths}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  abstract =      {This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output.},
  file =          {::},
  keywords =      {acou,machine{\_}listening},
  mendeley-tags = {acou,machine{\_}listening},
  xurl =           {https://github.com/McDonnell-Lab/DCASE2019-Task1}
}

@Article{Mesaros:2016:ASC:IEEE_TASLP,
  author =    {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
  title =     {{Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge}},
  journal =   {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =      {2018},
  volume =    {26},
  number =    {2},
  pages =     {379--393},
  abstract =  {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events DCASE 2016 has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: Acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
  xdoi =       {10.1109/TASLP.2017.2778423},
  file =      {:Users/jakobabeer/Downloads/08123864.pdf:pdf},
  issn =      {23299290},
  keywords =  {Acoustic scene classification,audio datasets,pattern recognition,sound event detection},
  publisher = {IEEE}
}

@InProceedings{Mesaros:2018:MultiDeviceDataset:DCASE,
  author =        {Mesaros, Annemaria and Heittola, Toni and {Tuomas Virtanen}},
  title =         {{A Multi-Device Dataset for Urban Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Heittola, Tuomas Virtanen - 2018 - A Multi-Device Dataset for Urban Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mesaros:2017:HumanASC:WASPAA,
  author =    {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title =     {{Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study}},
  booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  year =      {2017},
  pages =     {319--323},
  organization =   {New Paltz, NY, USA},
  note =     {15-18 October}
}

@InProceedings{Mesaros:2019:ClosedOpenSet:DCASE,
  author =        {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  title =         {{Acoustic Scene Classification in DCASE 2019 Challenge:Closed and Open Set Classification and Data Mismatch Setups}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {164--168},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  xdoi =           {10.33682/m5kp-fa97},
  file =          {::},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Moritz:2016:TDNN:DCASE,
  author =        {Moritz, Niko and Schr{\"{o}}der, Jens and Goetze, Stefan and Anem{\"{u}}ller, J{\"{o}}rn and Kollmeier, Birger},
  title =         {{Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  organization =       {Budapest, Hungary},
  note =         {3 September},
  abstract =      {This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual infor-mation similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 76.5 {\%}, which is 4.0 {\%} higher compared to the DCASE'16 baseline system.},
  file =          {:Users/jakobabeer/Downloads/Moritz-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2017:GANASC:DCASE,
  author =        {Mun, Seongkyu and Park, Sangwook and Han, David K. and Ko, Hanseok},
  title =         {{Generative Adversarial Networks based Acoustic Scene Training Set Augmentation and Selection using SVM Hyperplane}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Mun{\_}215.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2019:DomainMismatch:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon},
  title =         {{Domain Mismatch Robust Acoustic Scene Classification Using Channel Information Conversion}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {845--849},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  abstract =      {In recent acoustic scene classification (ASC) research field, training and test device channel mismatch have become an issue for the real world implementation. To address the issue, this paper proposes a channel domain conversion using factor-ized hierarchical variational autoencoder. Proposed method adapts both the source and target domain to a pre-defined specific domain. Unlike the conventional approach, the relationship between the target and source domain and information of each domain are not required in the adaptation process. Based on the experimental results using the IEEE Detection and Classification of Acoustic Scenes and Event 2018 task 1-B dataset and the baseline system, it is shown that the proposed approach can mitigate the channel mismatching issue of different recording devices.},
  xdoi =           {10.1109/ICASSP.2019.8683514},
  file =          {:Users/jakobabeer/Downloads/08683514.pdf:pdf},
  isbn =          {9781479981311},
  issn =          {15206149},
  keywords =      {acoustic scene classification,acoustic{\_}scene{\_}classification,domain adaptation,factorized hierarchical variational autoencoder,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Mun:2017:ASC:ICASSP,
  author =        {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Han, David K. and Ko, Hanseok},
  title =         {{Deep Neural Network Based Learning and Transferring Mid-Level Audio Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {796--800},
  organization =       {New Orleans, LA, USA},
  note =         {5-9 March},
  abstract =      {Deep Neural Network (DNN) based transfer learning has been shown to be effective in Visual Object Classification (VOC) for complementing the deficit of target domain training samples by adapting classifiers that have been pre- trained for other large-scaled DataBase (DB). Although there exists an abundance of acoustic data, it can also be said that datasets of specific acoustic scenes are sparse for training Acoustic Scene Classification (ASC) models. By exploiting VOC DNN‟s ability of learning beyond its pre- trained environments, this paper proposes DNN based transfer learning for ASC. Effectiveness of the proposed method is demonstrated on the database of IEEE DCASE Challenge 2016 Task 1 and home surveillance environment via representative experiments. Its improved performance is verified by comparing it to prominent conventional methods.},
  xdoi =           {10.1097/IOP.0000000000000348},
  file =          {:Users/jakobabeer/Downloads/07952265.pdf:pdf},
  isbn =          {9781509041176},
  issn =          {15372677},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Nguyen:2018:ASCEnsemble:DCASE,
  author =        {Nguyen, Truc and Pernkopf, Franz},
  title =         {{Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Pernkopf - 2018 - Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Nwe:2017:MultiTaskASC:APSIPA,
  author =    {Nwe, Tin Lay and Dat, Tran Huy and Ma, Bin},
  title =     {{Convolutional Neural Network with Multi-Task Learning Scheme for Acoustic Scene Classification}},
  booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =      {2018},
  pages =     {1347--1350},
  organization =   {Honolulu, Hawaii, USA},
  note =     {2-15 November},
  abstract =  {Deep Neural Network (DNN) with Multi-Task Learning (MTL) methods have recently demonstrated significant performance gains on a number of classification, detection, recognition tasks compared to conventional DNN. DNN with MTL framework involves cross-task and within-task knowledge sharing layers. MTL methods have benefit for regularization effect from the cross-task knowledge sharing layers. And, within- task knowledge sharing layers allow MTL based DNN to learn information to optimize the performance for individual task. We formulate our acoustic scene classification in MTL framework using Convolutional Neural Network to learn information specific to different types of environment. We conduct experiments using DCASE2016 dataset. Proposed approach achieves 83.8{\%} accuracy to classify 15 acoustic scene classes.},
  xdoi =       {10.1109/APSIPA.2017.8282241},
  file =      {:Users/jakobabeer/Downloads/08282241.pdf:pdf},
  isbn =      {9781538615423}
}

@InProceedings{Park:2019:SpecAugment:INTERSPEECH,
  author =        {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  title =         {{Specaugment: A simple data augmentation method for automatic speech recognition}},
  booktitle =     {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year =          {2019},
  volume =        {2019-Septe},
  pages =         {2613--2617},
  organization =       {Graz, Austria},
  note =         {2-15 November},
  abstract =      {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
  archiveprefix = {arXiv},
  arxivid =       {1904.08779},
  xdoi =           {10.21437/Interspeech.2019-2680},
  eprint =        {1904.08779},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2019 - Specaugment A simple data augmentation method for automatic speech recognition.pdf:pdf},
  issn =          {19909772},
  journal =       {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  keywords =      {Data augmentation,End-to-end speech recognition}
}

@InProceedings{Park:2017:DoubleImageASC:DCASE,
  author =        {Park, Sangwook and Mun, Seonkyu and Lee, Younglo and Ko, Hanseok},
  title =         {{Acoustic Scene Classification Based on Convolutional Neural Network using Double Image Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Park{\_}214.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Phaye:2019:Subspectralnet:ICASSP,
  author =        {Phaye, Sai Samarth R and Benetos, Emmanouil and Wang, Ye},
  title =         {{Subspectralnet - Using Sub-Spectrogram based Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {825--829},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  file =          {:Users/jakobabeer/Downloads/08683288 (1).pdf:pdf},
  isbn =          {9781538646588},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Qian:2017:WaveletASC:DCASE,
  author =        {Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Wavelets Revisited for the Classification of Acoustic Scenes}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Qian{\_}132.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Rafii:2012:REPET:ISMIR,
  author =    {Rafii, Zafar and Pardo, Bryan},
  title =     {{Music/Voice Separation using the Similarity Matrix}},
  booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
  year =      {2012},
  pages =     {583--588},
  organization =   {Porto, Portugal},
  note =     {8-12 October},
  abstract =  {Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. {\textcopyright} 2012 International Society for Music Information Retrieval.},
  file =      {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/0740488f2e930fa1b0a0ec546b876cf00ecb.pdf:pdf},
  isbn =      {9789727521449}
}

@InProceedings{Ren:2019:AttrousCNNAttention:ICASSP,
  author =        {Ren, Zhao and Kong, Qiuqiang and Han, Jing and Plumbley, Mark D and Schuller, Bjorn W.},
  title =         {{Attention-based Atrous Convolutional Neural Networks: Visualisation and Understanding Perspectives of Acoustic Scenes}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {56--60},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  xdoi =           {10.1109/ICASSP.2019.8683434},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2019 - Attention-based Atrous Convolutional Neural Networks Visualisation and Understanding Perspectives of Acoustic Scenes.pdf:pdf},
  keywords =      {machine{\_}listening},
  mendeley-tags = {machine{\_}listening},
  xurl =           {https://ieeexplore.ieee.org/document/8683434/}
}

@InProceedings{Ren:2018:AttentionASC:DCASE,
  author =        {Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark D. and Schuller, Bj{\"{o}}rn W.},
  title =         {{Attention-based Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2018 - Attention-based Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Ren:2017:DeepSequentialASC:DCASE,
  author =        {Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
  title =         {{Deep Sequential Image Features for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Ren{\_}133.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Roletscheck:2019:EvolutionaryASC:DCASE,
  author =        {Roletscheck, Christian and Watzka, Tobias and Seiderer, Andreas and Schiller, Dominik and Andr{\'{e}}, Elisabeth},
  title =         {{Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Roletscheck et al. - 2019 - Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Ruder:2017:MultitaskLearning:ARXIV,
  author =        {Ruder, Sebastian},
  title =         {{An Overview of Multi-Task Learning in Deep Neural Networks}},
  journal =       {ArXiv pre-prints},
  year =          {2017},
  abstract =      {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archiveprefix = {arXiv},
  arxivid =       {1706.05098},
  eprint =        {1706.05098},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1706.05098.pdf:pdf},
  xurl =           {http://arxiv.org/abs/1706.05098}
}

@Article{Russakovsky:2015:ImageNet:IJCV,
  author =        {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title =         {{ImageNet Large Scale Visual Recognition Challenge}},
  journal =       {International Journal of Computer Vision},
  year =          {2015},
  volume =        {115},
  number =        {3},
  pages =         {211--252},
  abstract =      {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  arxivid =       {1409.0575},
  xdoi =           {10.1007/s11263-015-0816-y},
  eprint =        {1409.0575},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/10.1.1.876.2726.pdf:pdf},
  issn =          {15731405},
  keywords =      {Benchmark,Dataset,Large-scale,Object detection,Object recognition}
}

@InProceedings{Saki:2019:OpenSetASC:DCASE,
  author =        {Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu},
  title =         {{Open-Set Evolving Acoustic Scene Classification System}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {219--223},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Saki, Guo, Hung - 2019 - Open-Set Evolving Acoustic Scene Classification System.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Salamon:2017:ASC:SPL,
  author =        {Salamon, Justin and Bello, Juan Pablo},
  title =         {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
  journal =       {IEEE Signal Processing Letters},
  year =          {2017},
  volume =        {24},
  number =        {3},
  pages =         {279--283},
  abstract =      {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a 'shallow' dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
  archiveprefix = {arXiv},
  arxivid =       {1608.04363},
  xdoi =           {10.1109/LSP.2017.2657381},
  eprint =        {1608.04363},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1608.04363.pdf:pdf},
  issn =          {10709908},
  keywords =      {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset}
}

@InProceedings{Sandler:2018:MobileNet:CVPR,
  author =        {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
  title =         {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
  booktitle =     {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year =          {2018},
  pages =         {4510--4520},
  organization =       {Salt Lake City, UT, USA},
  note =         {18-23 June},
  abstract =      {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
  archiveprefix = {arXiv},
  arxivid =       {1801.04381},
  xdoi =           {10.1109/CVPR.2018.00474},
  eprint =        {1801.04381},
  file =          {::},
  isbn =          {9781538664209},
  issn =          {10636919}
}

@InProceedings{Seo:2019:ASC:DCASE,
  author =    {Seo, Hyeji and Park, Jihwan and Park, Yongjin},
  title =     {{Acoustic Scene Classification using Various Pre-Processed Features and Convolutional Neural Networks}},
  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =      {2019},
  pages =     {3--6},
  organization =   {New York, NY, USA},
  note =     {25-26 October},
  annote =    {Ensemble approach, computationally too expensive},
  file =      {:Users/jakobabeer/Downloads/DCASE2019{\_}Seo{\_}72.pdf:pdf}
}

@Article{Sharma:2019:SoundClassification:ARXIV,
  author =        {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
  title =         {{Environment Sound Classification using Multiple Feature Channels and Deep Convolutional Neural Networks}},
  journal =       {ArXiv pre-prints},
  year =          {2019},
  volume =        {14},
  number =        {8},
  pages =         {1--11},
  abstract =      {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN). The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. Also, we employ a deeper CNN (DCNN) compared to previous models, consisting of 2D separable convolutions working on time and feature domain separately. The model also consists of max pooling layers that downsample time and feature domain separately. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (98.60{\%}), ESC-10 (97.25{\%}) and ESC-50 (95.50{\%}). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets and by a considerable margin over the previous models. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7{\%} and 81.3{\%} respectively.},
  archiveprefix = {arXiv},
  arxivid =       {1908.11219},
  eprint =        {1908.11219},
  file =          {:Users/jakobabeer/Downloads/1908.11219.pdf:pdf},
  xurl =           {http://arxiv.org/abs/1908.11219}
}

@Article{Sigtia:2016:PerformanceCost:IEEE_TASLP,
  author =        {Sigtia, Siddharth and Stark, Adam M. and Krstulovi{\'{c}}, Sacha and Plumbley, Mark D.},
  title =         {{Automatic Environmental Sound Recognition: Performance Versus Computational Cost}},
  journal =       {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year =          {2016},
  volume =        {24},
  number =        {11},
  pages =         {2096--2107},
  abstract =      {In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.},
  xdoi =           {10.1109/TASLP.2016.2592698},
  file =          {::},
  issn =          {23299290},
  keywords =      {Automatic environmental sound recognition,computational auditory scene analysis,deep learning,machine learning,machine{\_}listening},
  mendeley-tags = {machine{\_}listening},
  publisher =     {IEEE}
}

@InProceedings{Singh:2019:MultiViewFeatures:DCASE,
  author =        {Singh, Arshdeep and Rajan, Padmanabhan and Bhavsar, Arnav},
  title =         {{Deep Multi-View Features from Raw Audio for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {229--233},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Rajan, Bhavsar - 2019 - Deep Multi-View Features from Raw Audio for Acoustic Scene Classification.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Singh:2018:EnsembleASC:EUSIPCO,
  author =        {Singh, Arshdeep and Thakur, Anshul and Rajan, Padmanabhan and Bhavsar, Arnav},
  title =         {{A Layer-Wise Score Level Ensemble Framework for Acoustic Scene Detection}},
  booktitle =     {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
  year =          {2018},
  pages =         {837--841},
  organization =       {Rome, Italy},
  note =         {3-7 September},
  abstract =      {Scene classification based on acoustic information is a challenging task due to various factors such as the non-stationary nature of the environment and multiple overlapping acoustic events. In this paper, we address the acoustic scene classification problem using SoundNet, a deep convolution neural network, pre-trained on raw audio signals. We propose a classification strategy by combining scores from each layer. This is based on the hypothesis that layers of the deep convolutional network learn complementary information and combining this layer-wise information provides better classification than the features extracted from an individual layer. In addition, we also propose a pooling strategy to reduce the dimensionality of features extracted from different layers of SoundNet. Our experiments on DCASE 2016 acoustic scene classification dataset reveals the effectiveness of this layer-wise ensemble approach. The proposed approach provides a relative improvement of approx. 30.85{\%} over the classification accuracy provided by the best individual layer of SoundNet.},
  xdoi =           {10.23919/EUSIPCO.2018.8553052},
  file =          {:Users/jakobabeer/Downloads/08553052.pdf:pdf},
  isbn =          {9789082797015},
  issn =          {22195491},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Takahashi:2017:ASC:APSIPA,
  author =        {Takahashi, Gen and Yamada, Takeshi and Ono, Nobutaka and Makino, Shoji},
  title =         {{Performance Evaluation of Acoustic Scene Classification using DNN-GMM and Frame-Concatenated Acoustic Features}},
  booktitle =     {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =          {2018},
  pages =         {1739--1743},
  organization =       {Honolulu, Hawaii, USA},
  note =         {2-15 November},
  abstract =      {We previously proposed a method of acoustic scene classification using a deep neural network-Gaussian mixture model (DNN-GMM) and frame-concatenated acoustic features. It was submitted to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 Challenge and was ranked eighth among 49 algorithms. In the proposed method, acoustic features in temporally distant frames were concatenated to capture their temporal relationship. The experimental results indicated that the classification accuracy is improved by increasing the number of concatenated frames. On the other hand, the frame concatenation interval, which is the interval with which the frames used for frame concatenation are selected, is another important parameter. In our previous method, the frame concatenation interval was fixed to 100 ms. In this paper, we optimize the number of concatenated frames and the frame concatenation interval for the previously proposed method. As a result, it was confirmed that the classification accuracy of the method was improved by 2.61{\%} in comparison with the result submitted to the DCASE 2016.},
  xdoi =           {10.1109/APSIPA.2017.8282314},
  file =          {:Users/jakobabeer/Downloads/08282314.pdf:pdf},
  isbn =          {9781538615423},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Tan:2019:EfficientNet:ICML,
  author =        {Tan, Mingxing and Le, Quoc V.},
  title =         {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
  booktitle =     {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year =          {2019},
  organization =       {Long Beach, CA, USA},
  note =         {9-15 June},
  abstract =      {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  arxivid =       {1905.11946},
  eprint =        {1905.11946},
  file =          {::},
  isbn =          {9781510886988},
  xurl =           {http://arxiv.org/abs/1905.11946}
}

@InProceedings{Wang:2018:SelfDeterminationASC:APSIPA,
  author =        {Wang, Chien-Yao and Santoso, Andri and Wang, Jia-Ching},
  title =         {{Acoustic Scene Classification using Self-Determination Convolutional Neural Network}},
  booktitle =     {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year =          {2018},
  pages =         {19--22},
  organization =       {Honolulu, Hawaii, USA},
  note =         {2-15 November},
  xdoi =           {10.1109/APSIPA.2017.8281995},
  file =          {:Users/jakobabeer/Downloads/08281995.pdf:pdf},
  isbn =          {9781538615423},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wang:2017:ASC:ISCE,
  author =        {Wang, Chien-Yao and Wang, Jia-Ching and Wu, Yu-Chi and Chang, Pao-Chi},
  title =         {{Asymmetric Kernel Convolution Neural Networks for Acoustic Scenes Classification}},
  booktitle =     {Proceedings of the IEEE International Symposium on Consumer Electronics (ISCE)},
  year =          {2017},
  pages =         {11--12},
  organization =       {Kuala Lumpur, Malaysia},
  note =         {14-15 November},
  file =          {:Users/jakobabeer/Downloads/08355533.pdf:pdf},
  isbn =          {9781538621899},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wang:2017:PCEN:ICASSP,
  author =        {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
  title =         {{Trainable Frontend for Robust and Far-Field Keyword Spotting}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2017},
  pages =         {5670--5674},
  organization =       {New Orleans, LA, USA},
  note =         {5-9 March},
  abstract =      {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
  archiveprefix = {arXiv},
  arxivid =       {1607.05666},
  xdoi =           {10.1109/ICASSP.2017.7953242},
  eprint =        {1607.05666},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/45911.pdf:pdf},
  isbn =          {9781509041176},
  issn =          {15206149},
  keywords =      {automatic gain control,deep neural networks,robust and far-field speech recognition}
}

@InProceedings{Weiping:2017:SpectrogramFusion:DCASE,
  author =        {Weiping, Zheng and Jiantao, Yi and Xiaotao, Xing and Xiangtao, Liu and Shaohu, Peng},
  title =         {{Acoustic Scene Classification using Deep Convolutional Neural Networks and Multiple Spectrogram Fusions}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2017},
  organization =       {Munich, Germany},
  note =         {16-17 November},
  file =          {:Users/jakobabeer/Downloads/DCASE2017Workshop{\_}Zheng{\_}159.pdf:pdf},
  isbn =          {1299670261},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wilkinghoff:2019:OpenSetASC:DCASE,
  author =        {Wilkinghoff, Kevin and {Frank Kurth}},
  title =         {{Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2019},
  pages =         {258--262},
  organization =       {New York, NY, USA},
  note =         {25-26 October},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinghoff, Frank Kurth - 2019 - Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Wu:2019:SoundTexture:ICASSP,
  author =        {Wu, Yuzhong and Lee, Tan},
  title =         {{Enhancing Sound Texture in CNN-based Acoustic Scene Classification}},
  booktitle =     {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =          {2019},
  pages =         {815--819},
  organization =       {Brighton, UK},
  note =         {12-17 May},
  archiveprefix = {arXiv},
  arxivid =       {1901.01502},
  xdoi =           {10.1109/ICASSP.2019.8683490},
  eprint =        {1901.01502},
  file =          {:Users/jakobabeer/Downloads/08683490 (1).pdf:pdf},
  isbn =          {9781479981311},
  issn =          {15206149},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Xia:2019:EventDetection:CSSR,
  author =   {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
  title =    {{A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection}},
  journal =  {Circuits, Systems, and Signal Processing},
  year =     {2019},
  number =   {38},
  pages =    {3433?3453},
  abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
  xdoi =      {10.1007/s00034-019-01094-1},
  issn =     {15315878},
  keywords = {Acoustic event detection,Deep learning,Strongly labeled,Weakly labeled}
}

@InProceedings{Xu:2018:ASCMobileNet:ISM,
  author =        {Xu, Jun-Xiang and Lin, Tzu-Ching and Yu, Tsai-Ching and Tai, Tzu-Chiang and Chang, Pao-Chi},
  title =         {{Acoustic Scene Classification Using Reduced MobileNet Architecture}},
  booktitle =     {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
  year =          {2018},
  pages =         {267--270},
  organization =       {Taichung, Taiwan},
  note =         {10-12 December},
  xdoi =           {10.1109/ISM.2018.00038},
  file =          {:Users/jakobabeer/Downloads/08603300.pdf:pdf},
  isbn =          {9781538668573},
  journal =       {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Xu:2016:HierarchicalASC:DCASE,
  author =        {Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.},
  title =         {{Hierarchical Learning for DNN-Based Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  organization =       {Budapest, Hungary},
  note =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Xu-a-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Yang:2018:MultiScaleFeatures:DCASE,
  author =        {Yang, Liping and Chen, Xinxing and Tao, Lianjie},
  title =         {{Acoustic Scene Classification using Multi-Scale Features}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Chen, Tao - 2018 - Acoustic Scene Classification using Multi-Scale Features.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Article{Ye:2018:ASC:AS,
  author =   {Ye, Jiaxing and Kobayashi, Takumi and Toyama, Nobuyuki and Tsuda, Hiroshi and Murakawa, Masahiro},
  title =    {{Acoustic scene classification using efficient summary statistics and multiple spectro-temporal descriptor fusion}},
  journal =  {Applied Sciences},
  year =     {2018},
  volume =   {8},
  number =   {8},
  pages =    {1--12},
  abstract = {This paper presents a novel approach for acoustic scene classification based on efficient acoustic feature extraction using spectro-temporal descriptors fusion. Grounded on the finding in neuroscience-"auditory system summarizes the temporal details of sounds using time-averaged statistics to understand acoustic scenes", we devise an efficient computational framework for sound scene classification by using multipe time-frequency descriptors fusion with discriminant information enhancement. To characterize rich information of sound, i.e., local structures on the time-frequency plane, we adopt 2-dimensional local descriptors. A more critical issue raised in how to logically 'summarize' those local details into a compact feature vector for scene classification. Although 'time-averaged statistics' is suggested by the psychological investigation, directly computing time average of local acoustic features is not a logical way, since arithmetic mean is vulnerable to extreme values which are anticipated to be generated by interference sounds which are irrelevant to the scene category. To tackle this problem, we develop time-frame weighting approach to enhance sound textures as well as to suppress scene-irrelevant events. Subsequently, robust acoustic feature for scene classification can be efficiently characterized. The proposed method had been validated by using Rouen dataset which consists of 19 acoustic scene categories with 3029 real samples. Extensive results demonstrated the effectiveness of the proposed scheme.},
  xdoi =      {10.3390/app8081363},
  file =     {:Users/jakobabeer/Downloads/applsci-08-01363.pdf:pdf},
  issn =     {20763417},
  keywords = {Acoustic scene classification,Convex combination,Local descriptor,Summary statistics,Time-frequency analysis}
}

@InProceedings{Zoehrer:2016:GRN_ASC:DCASE,
  author =        {Z{\"{o}}hrer, Matthias and Pernkopf, Franz},
  title =         {{Gated Recurrent Networks Applied to Acoustic Scene Classification and Acoustic Event Detection}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2016},
  organization =       {Budapest, Hungary},
  note =         {3 September},
  file =          {:Users/jakobabeer/Downloads/Zohrer-DCASE2016workshop.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Zeinali:2018:XVektorEmbeddings:DCASE,
  author =        {Zeinali, Hossein and Burget, Luk{\'{a}}s and Cernocky, Jan},
  title =         {{Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Challenge}},
  booktitle =     {Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)},
  year =          {2018},
  organization =       {Surrey, UK},
  note =         {19-20 November},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zeinali, Burget, Cernocky - 2018 - Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Cha.pdf:pdf},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@InProceedings{Zhang:2018:Mixup:ICLR,
  author =        {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  title =         {{mixup: Beyond Empirical Risk Minimization}},
  booktitle =     {Proceedings of the International Conference on Learning Representations (ICLR)},
  year =          {2018},
  organization =       {Vancouver, Canada},
  note =         {30 April - 3 May},
  abstract =      {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By xdoing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  arxivid =       {1710.09412},
  eprint =        {1710.09412},
  file =          {:Users/jakobabeer/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:pdf}
}

@Article{Zhong:2017:RandomErasing:ARXIV,
  author =        {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  title =         {{Random Erasing Data Augmentation}},
  journal =       {ArXiv pre-prints},
  year =          {2017},
  abstract =      {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  archiveprefix = {arXiv},
  arxivid =       {1708.04896},
  eprint =        {1708.04896},
  file =          {:Users/jakobabeer/Desktop/{\_}NEW{\_}Papers/1708.04896.pdf:pdf},
  keywords =      {data{\_}augmentation},
  mendeley-tags = {data{\_}augmentation},
  xurl =           {http://arxiv.org/abs/1708.04896}
}

@InProceedings{Zielinski:2018:BinauralASC:FEDCSIS,
  author =        {Zieli{\'{n}}ski, S{\l}awomir K. and Lee, Hyunkook},
  title =         {{Feature Extraction of Binaural Recordings for Acoustic Scene Classification}},
  booktitle =     {Proceedings of the Federated Conference on Computer Science and Information Systems (FedCSIS)},
  year =          {2018},
  pages =         {585--588},
  organization =       {Pozna{\'{n}}, Poland},
  note =         {9-12 September},
  abstract =      {Binaural technology becomes increasingly popular in the multimedia systems. This paper identifies a set of features of binaural recordings suitable for the automatic classification of the four basic spatial audio scenes representing the most typical patterns of audio content distribution around a listener. Moreover, it compares the five artificial-intelligence-based methods applied to the classification of binaural recordings. The results show that both the spatial and the spectro-temporal features are essential to accurate classification of binaurally rendered acoustic scenes. The spectro-temporal features appear to have a stronger influence on the classification results than the spatial metrics. According to the obtained results, the method based on the support vector machine, exploiting the features identified in the study, yields the classification accuracy approaching 84{\%}.},
  xdoi =           {10.15439/2018F182},
  file =          {:Users/jakobabeer/Downloads/08511268.pdf:pdf},
  isbn =          {9788394941970},
  keywords =      {acoustic{\_}scene{\_}classification,machine{\_}listening},
  mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening}
}

@Book{Virtanen:2018:SoundSceneBook:BOOK,
  title =     {{Computational Analysis of Sound Scenes and Events}},
  publisher = {Springer International Publishing},
  year =      {2018},
  editor =    {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
  organization =   {Cham, Switzerland},
  xdoi =       {10.1007/978-3-319-63450-0},
  file =      {:Users/jakobabeer/Sync/Jakob/Knowhow/Books/Ellis, Dan{\_} Plumbley, Mark D.{\_} Virtanen, Tuomas-Computational analysis of sound scenes and events-Springer (2018).pdf:pdf},
  xurl =       {http://link.springer.com/10.1007/978-3-319-63450-0}
}

@inproceedings{Johnson2020:isa_normalization,
abstract = {The field of Industrial Sound Analysis (ISA) aims to automatically identify faults in production machinery or manufactured goods by analyzing audio signals. Publications in this field have shown that the surface condition of metal balls and different types of bulk materials (screws, nuts, etc.) sliding down a tube can be classified with a high accuracy using audio signals and deep neural networks. However, these systems suffer from domain shift, or dataset bias, due to minor changes in the recording setup which may easily happen in real-world production lines. This paper aims at finding methods to increase robustness of existing detection systems to domain shift, ideally without the need to record new data or retrain the models. Through five experiments, we implement a convolutional neural network (CNN) for two publicly available ISA datasets and evaluate transfer learning, data normalization and data augmentation as approaches to deal with domain shift. Our results show that while supervised methods with additional labeled data are the best approach, an unsupervised method that implements data augmentation with adaptive normalization is able to improve the performance by a large margin without the need of retraining neural networks.},
address = {Amsterdam, The Netherlands},
author = {Johnson, David and Grollmisch, Sascha},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/goh/Desktop/eusipco{\_}2020{\_}normalization{\_}for{\_}isa.pdf:pdf},
title = {{Techniques Improving the Robustness of Deep Learning Models for Industrial Sound Analysis}},
year = {2020}
}


@inproceedings{Grollmisch2020:embeddings,
abstract = {In the context of deep learning, the availability of large amounts of training data can play a critical role in a model's performance. Recently, several models for audio classification have been pre-trained in a supervised or self-supervised fashion on large datasets to learn complex feature representations, so- called embeddings. These embeddings can then be extracted from smaller datasets and used to train subsequent classifiers. In the field of audio event detection (AED) for example, classifiers using these features have achieved high accuracy without the need of additional domain knowledge. This paper evaluates three state-of-the-art embeddings on six audio classification tasks from the fields of music information retrieval and industrial sound analysis. The embeddings are systematically evaluated by analyzing the influence on classification accuracy of classifier architecture, fusion methods for file-wise predictions, amount of training data, and initial training domain of the embeddings. To better understand the impact of the pre-training step, results are also compared with those acquired with models trained from scratch. On average, the OpenL3 embeddings performed best with a linear SVM classifier. For a reduced amount of training examples, OpenL3 outperforms the initial baseline.},
address = {Amsterdam, The Netherlands},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Kehling, Christian and Taenzer, Michael},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/goh/Desktop/eusipco{\_}2020{\_}embeddings.pdf:pdf},
title = {{Analyzing the Potential of Pre-Trained Embeddings for Audio Classification Tasks}},
year = {2020}
}
@inproceedings{Grollmisch:2019:EnsembleSize:CMMR,
address = {Marseille, France},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Mora-{\'{A}}ngel, Fernando and Gil, Gustavo L{\'{o}}pez},
booktitle = {International Symposium of Computer Music Multidisciplinary Research (CMMR)},
file = {:I$\backslash$:/IMA/goh/Documents/conferences/cmmr2019/ACMus{\_}CMMR{\_}2019{\_}Paper.pdf:pdf},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Ensemble size classification in Colombian Andean string music recordings}},
year = {2019}
}
@inproceedings{grollmisch2020:isa_visualization,
abstract = {Recent research has shown acoustic quality control using audio signal processing and neural networks to be a viable solution for detecting product faults in noisy factory environments. For industrial partners, it is important to be able to explain the network's decision making, however, there is limited research on this area in the field of industrial sound analysis (ISA). In this work, we visualize learned patterns of an existing network to gain insights about the decision making process. We show that unwanted biases can be discovered, and thus avoided, using this technique when validating acoustic quality control systems.},
address = {N{\"{u}}rnberg, Germany},
author = {Grollmisch, Sascha and Johnson, David and Liebetrau, Judith},
booktitle = {Sensor and Measurement Science International (SMSI)},
file = {:C$\backslash$:/Users/goh/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grollmisch, Johnson, Liebetrau - 2020 - Visualizing Neural Network Decisions for Industrial Sound Analysis.pdf:pdf},
keywords = {acoustic quality control,background,classification,ima,industrial sound analysis,isa,layer-wise relevance propagation,machine learning,motivation and objective,neural networks,neural networks have improved,own,visualization},
mendeley-tags = {ima,isa,own,visualization},
title = {{Visualizing Neural Network Decisions for Industrial Sound Analysis}},
year = {2020}
}
@inproceedings{Grollmisch2020:iaeo3,
abstract = {In this technical report, we present our system for task 2 of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2020 Challenge): Unsupervised Detec- tion of Anomalous Sounds for Machine Condition Monitoring. The focus of this task is to detect anomalous industrial machine sounds using an acoustic quality control system, which is only trained with sound samples from the normal (machine) condition. The dataset covers a variety of machines ranging from stable sound sources such as car engines, to transient sounds such as opening and closing valves. Our proposed method combines pre-trained OpenL3 embeddings with the reconstruction error of an interpolation autoen- coder using a gaussian mixture model as the final predictor. The optimized model achieved 88.5{\%} AUC and 76.8{\%} pAUC on av- erage over all machines and types provided with the development dataset, and outperformed the published baseline by 14.9{\%} AUC and 17.2{\%} pAUC.},
author = {Grollmisch, Sascha and Johnson, David and Abe{\ss}er, Jakob and Lukashevich, Hanna},
booktitle = {Detection and Classification of Acoustic Scenes Events},
file = {:C$\backslash$:/Users/goh/Desktop/dcase{\_}submission/Grollmisch{\_}IDMT{\_}task2{\_}technical{\_}report.pdf:pdf},
keywords = {anomaly detection,isa,own},
mendeley-tags = {anomaly detection,isa,own},
title = {{IAEO3 - COMBINING OPENL3 EMBEDDINGS AND INTERPOLATION AUTOENCODER FOR ANOMALOUS SOUND DETECTION}},
year = {2020}
}


@inproceedings{Cramer2019:openl3,
address = {Brighton, United Kingdom},
author = {Cramer, Jason and Wu, Ho-hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {IEEE ICASSP},
xdoi = {10.1109/ICASSP.2019.8682475},
isbn = {978-1-4799-8131-1},
pages = {3852--3856},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}},
year = {2019}
}

@inproceedings{Kumar2018:embedding,
author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
booktitle = {IEEE ICASSP},
isbn = {978-1-5386-4658-8},
issn = {15206149},
pages = {326--330},
title = {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
year = {2018}
}

@article{Mcfee,
author = {Mcfee, Brian},
file = {::},
title = {{Metric Learning for MIR}}
}
@article{Kelz2019a,
author = {Kelz, Rainer and B{\"{o}}ck, Sebasian and Widmer, Gerhard},
xdoi = {10.1109/MMRP.2019.00023},
file = {::},
isbn = {978-1-7281-1649-5},
journal = {International Workshop on Multilayer Music Representation and Processing (MMRP)},
pages = {85--91},
title = {{Multitask Learning for Polyphonic Piano
Transcription, a Case Study}},
xurl = {https://ieeexplore.ieee.org/document/8665372},
year = {2019}
}
@article{Wang2020c,
abstract = {Regularization is commonly used in machine learning for alleviating overfitting. In convolutional neural networks, regularization methods, such as Dropout and Shake-Shake, have been proposed to improve the generalization performance. However, these methods are lack of self-adaption throughout training, i.e., the regularization strength is fixed to a predefined schedule, and manual adjustment has to be performed to adapt to various network architectures. In this paper, we propose a dynamic regularization method which can dynamically adjust the regularization strength in the training procedure. Specifically, we model the regularization strength as a backward difference of the training loss, which can be directly extracted in each training iteration. With dynamic regularization, the large model is regularized by the strong perturbation and vice versa. Experimental results show that the proposed method can improve the generalization capability of off-the-shelf network architectures and outperforms state-of-the-art regularization methods.},
archivePrefix = {arXiv},
arxivId = {1909.11862},
author = {Wang, Yi and Bian, Zhen-Peng and Hou, Junhui and Chau, Lap-Pui},
xdoi = {10.1109/tnnls.2020.2997044},
eprint = {1909.11862},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {jun},
pages = {1--6},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Convolutional Neural Networks With Dynamic Regularization}},
year = {2020}
}
@inproceedings{Mimilakis:2020:EUSIPCO,
abstract = {In this work, we present a method for learning interpretable music signal representations directly from waveform signals. Our method can be trained using unsupervised objectives and relies on the denoising auto-encoder model that uses a simple sinusoidal model as decoding functions to reconstruct the singing voice. To demonstrate the benefits of our method, we employ the obtained representations to the task of informed singing voice separation via binary masking, and measure the obtained separation quality by means of scale-invariant signal to distortion ratio. Our findings suggest that our method is capable of learning meaningful representations for singing voice separation, while preserving conveniences of the the short-time Fourier transform like non-negativity, smoothness, and reconstruction subject to time-frequency masking, that are desired in audio and music source separation.},
archivePrefix = {arXiv},
arxivId = {2003.01567},
author = {Mimilakis, Stylianos I. and Drossos, Konstantinos and Schuller, Gerald},
booktitle = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO 2020)},
eprint = {2003.01567},
file = {:X$\backslash$:/knowhow/publica{\_}exports/new{\_}publications/mis{\_}eusipco{\_}2020.pdf:pdf},
keywords = {abt-md},
mendeley-tags = {abt-md},
pages = {1--7},
title = {{Unsupervised Interpretable Representation Learning for Singing Voice Separation}},
xurl = {http://arxiv.org/abs/2003.01567},
year = {2020}
}
@article{Mimilakis2020,
abstract = {In this work, we present a method for learning interpretable music signal representations directly from waveform signals. Our method can be trained using unsupervised objectives and relies on the denoising auto-encoder model that uses a simple sinusoidal model as decoding functions to reconstruct the singing voice. To demonstrate the benefits of our method, we employ the obtained representations to the task of informed singing voice separation via binary masking, and measure the obtained separation quality by means of scale-invariant signal to distortion ratio. Our findings suggest that our method is capable of learning meaningful representations for singing voice separation, while preserving conveniences of the the short-time Fourier transform like non-negativity, smoothness, and reconstruction subject to time-frequency masking, that are desired in audio and music source separation.},
archivePrefix = {arXiv},
arxivId = {2007.02780},
author = {Mimilakis, Stylianos I. and Drossos, Konstantinos and Schuller, Gerald},
eprint = {2007.02780},
file = {:X$\backslash$:/knowhow/publica{\_}exports/new{\_}publications/mis{\_}sinkhorn{\_}2020.pdf:pdf},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Revisiting Representation Learning for Singing Voice Separation with Sinkhorn Distances}},
xurl = {http://arxiv.org/abs/2007.02780},
year = {2020}
}
@inproceedings{Pyykkoenen:2020:DSC:MMSP,
abstract = {Recent approaches for music source separation are almost exclusively based on deep neural networks, mostly employing recurrent neural networks (RNNs). Although RNNs are in many cases superior than other types of deep neural networks for sequence processing, they are known to have specific difficulties in training and parallelization, especially for the typically long sequences encountered in music source separation. In this paper we present a use-case of replacing RNNs with depth-wise separable (DWS) convolutions, which are a lightweight and faster variant of the typical convolutions. We focus on singing voice separation, employing an RNN architecture, and we replace the RNNs with DWS convolutions (DWS-CNNs). We conduct an ablation study and examine the effect of the number of channels and layers of DWS-CNNs on the source separation performance, by utilizing the standard metrics of signal-to-artifacts, signal-to-interference, and signal-to-distortion ratio. Our results show that by replacing RNNs with DWS-CNNs yields an improvement of 1.20, 0.06, 0.37 dB, respectively, while using only 20.57{\%} of the amount of parameters of the RNN architecture.},
archivePrefix = {arXiv},
arxivId = {2007.02683},
author = {Pyykk{\"{o}}nen, Pyry and Mimilakis, Styliannos I. and Drossos, Konstantinos and Virtanen, Tuomas},
booktitle = {Proceedings of the 22nd IEEE International Workshop on Multimedia Signal Processing (MMSP)},
eprint = {2007.02683},
file = {:X$\backslash$:/knowhow/publica{\_}exports/new{\_}publications/mis{\_}mmsp{\_}2020.pdf:pdf},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Depthwise Separable Convolutions Versus Recurrent Neural Networks for Monaural Singing Voice Separation}},
xurl = {http://arxiv.org/abs/2007.02683},
year = {2020}
}
@article{Chen2020,
abstract = {Deep learning algorithms are increasingly developed for learning to compose music in the form of MIDI files. However, whether such algorithms work well for composing guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with Transformer-XL, a neural sequence model architecture. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful note-string combinations, which is important for the guitar but not other instruments such as the piano. Second, whether it generates compositions with coherent rhythmic groove, crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence of the promise of deep learning for tab composition, and suggests areas for future study.},
archivePrefix = {arXiv},
arxivId = {2008.01431},
author = {Chen, Yu-Hua and Huang, Yu-Hsiang and Hsiao, Wen-Yi and Yang, Yi-Hsuan},
eprint = {2008.01431},
file = {::},
month = {aug},
title = {{Automatic Composition of Guitar Tabs by Transformers and Groove Modeling}},
xurl = {http://arxiv.org/abs/2008.01431},
year = {2020}
}
@article{Anastasopoulos2018,
abstract = {We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.},
archivePrefix = {arXiv},
arxivId = {1802.06655},
author = {Anastasopoulos, Antonios and Chiang, David},
eprint = {1802.06655},
file = {::},
journal = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
month = {feb},
pages = {82--91},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Tied Multitask Learning for Neural Speech Translation}},
xurl = {http://arxiv.org/abs/1802.06655},
volume = {1},
year = {2018}
}
@article{Wang2020a,
abstract = {Data-driven approaches to automatic drum transcription (ADT) are often limited to a predefined, small vocabulary of percussion instrument classes. Such models cannot recognize out-of-vocabulary classes nor are they able to adapt to finer-grained vocabularies. In this work, we address open vocabulary ADT by introducing few-shot learning to the task. We train a Prototypical Network on a synthetic dataset and evaluate the model on multiple real-world ADT datasets with polyphonic accompaniment. We show that, given just a handful of selected examples at inference time, we can match and in some cases outperform a state-of-the-art supervised ADT approach under a fixed vocabulary setting. At the same time, we show that our model can successfully generalize to finer-grained or extended vocabularies unseen during training, a scenario where supervised approaches cannot operate at all. We provide a detailed analysis of our experimental results, including a breakdown of performance by sound class and by polyphony.},
archivePrefix = {arXiv},
arxivId = {2008.02791},
author = {Wang, Yu and Salamon, Justin and Cartwright, Mark and Bryan, Nicholas J. and Bello, Juan Pablo},
eprint = {2008.02791},
file = {::},
month = {aug},
title = {{Few-Shot Drum Transcription in Polyphonic Music}},
xurl = {http://arxiv.org/abs/2008.02791},
year = {2020}
}
@article{Wang2017,
abstract = {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05666v1},
author = {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
xdoi = {10.1109/ICASSP.2017.7953242},
eprint = {arXiv:1607.05666v1},
file = {:C$\backslash$:/Users/abr/Downloads/1607.05666.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {automatic gain control,deep neural networks,robust and far-field speech recognition},
number = {1},
pages = {5670--5674},
title = {{Trainable frontend for robust and far-field keyword spotting}},
year = {2017}
}
@article{Deshmukh2020,
abstract = {Weakly Labelled learning has garnered lot of attention in recent years due to its potential to scale Sound Event Detection (SED). The paper proposes a Multi-Task Learning (MTL) framework for learning from Weakly Labelled Audio data which encompasses the traditional Multiple Instance Learning (MIL) setup. The MTL framework uses two-step attention mechanism and reconstructs Time Frequency (T-F) representation of audio as the auxiliary task. By breaking the attention into two steps, the network retains better time level information without compromising classification performance. The auxiliary task uses an auto-encoder structure to encourage the network for retaining source specific information. This indirectly de-noises internal T- F representation and improves classification performance under noisy recordings. For evaluation of proposed methodology, we remix the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 db SNR. The proposed network outperforms existing benchmark models over all SNRs, specifically 22.3 {\%}, 12.8 {\%}, 5.9 {\%} improvement over benchmark model on 0, 10 and 20 dB SNR respectively. The results and ablation study performed demonstrates the usefulness of auto-encoder for auxiliary task and verifies that the output of decoder portion provides a cleaned Time Frequency (T-F) representation of audio/sources which can be further used for source separation. The code is publicly released.},
archivePrefix = {arXiv},
arxivId = {2008.07085},
author = {Deshmukh, Soham and Raj, Bhiksha and Singh, Rita},
eprint = {2008.07085},
file = {::},
pages = {1--10},
title = {{Multi-Task Learning for Interpretable Weakly Labelled Sound Event Detection}},
xurl = {http://arxiv.org/abs/2008.07085},
year = {2020}
}
@article{Wang2020,
abstract = {The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation.},
archivePrefix = {arXiv},
arxivId = {2008.07118},
author = {Wang, Ziyu and Zhang, Yiyi and Zhang, Yixiao and Jiang, Junyan and Yang, Ruihan and Zhao, Junbo and Xia, Gus},
eprint = {2008.07118},
file = {::},
pages = {0--7},
title = {{PIANOTREE VAE: Structured Representation Learning for Polyphonic Music}},
xurl = {http://arxiv.org/abs/2008.07118},
year = {2020}
}
@article{Wang2015,
abstract = {Generally speaking, most systems of network traffic identification are based on features. The features may be port numbers, static signatures, statistic characteristics, and so on. The difficulty of the traffic identification is to find the features in the flow data. The process is very time‐consuming. Also, these approaches are invalid to unknown protocol. To solve these problems, we propose a method that is based on neural network and deep learning – a hotspot of research in machine learning. The results show that our approach works very well on the applications of feature learning, protocol identification and anomalous protocol detection.},
author = {Wang, Zhanyi},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/us-15-Wang-The-Applications-Of-Deep-Learning-On-Traffic-Identification-wp.pdf:pdf},
journal = {Black Hat USA},
keywords = {anomalous protocol detection,deep learning,feature learning,protocol classification,traffic identification},
title = {{The Applications of Deep Learning on Traffic Identification}},
year = {2015}
}
@article{SasiPriya2020,
abstract = {The traffic surveillance system is accumulated with an enormous amount of data regarding road traffic each and every second. Monitoring these data with the human eye is a tedious process and it also requires manpower for monitoring. Deep learning approach (Convolutional Neural Network) can be utilized for traffic monitoring and control. The traffic surveillance data are pre-processed to construct the training dataset. The Traffic net is constructed by transferring the network to traffic applications and retraining it with self-established data set. This Traffic net can be used for regional detection in large scale applications.Further, it can be implemented across-the-board. The efficiency is admirably verified through speedy discovery in the high accuracy in the case study. The tentative assessment could pull out to its successful application to a traffic surveillance system and has potential enrichment for the intelligent transport system in future.},
author = {{Sasi Priya}, S. and Rajarajeshwari and Sowmiya, K. and Vinesha, P.},
xdoi = {10.1109/ICICT48043.2020.9112408},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/09112408.pdf:pdf},
isbn = {9781728146850},
journal = {Proceedings of the 5th International Conference on Inventive Computation Technologies, ICICT 2020},
keywords = {Convolution Neural Network,Deep learning,Intelligent traffic system,Residual Learning,Traffic net},
pages = {330--335},
title = {{Road Traffic Condition Monitoring using Deep Learning}},
year = {2020}
}
@article{Li2017,
abstract = {In this work, we propose a system that detects approaching cars for smartphone users. In addition to detecting the presence of a vehicle, it can also estimate the vehicle's driving direction, as well as count the number of cars around the user. We achieve these goals by processing the acoustic signal captured by microphones embedded in the user's mobile phone. The largest challenge we faced involved addressing the fact that vehicular noise is predominantly due to tire-road friction, and therefore lacked strong (frequency) formant or temporal structure. Additionally, outdoor environments have complex acoustic noise characteristics, which are made worse when the signal is captured by non-professional grade microphones embedded in smartphones. We address these challenges by monitoring a new feature: maximal frequency component that crosses a threshold. We extract this feature with a blurred edge detector. Through detailed experiments, we found our system to be robust across different vehicles and environmental conditions, and thereby support unsupervised car detection and counting. We evaluated our system using audio tracks recorded from seven different models of cars, including SUVs, medium-sized sedans, compact cars, and electric cars. We also tested our system with the user walking in various outdoor environments including parking lots, campus roads, residential areas, and shopping centers. Our results show that we can accurately and robustly detect cars with low CPU and memory requirements.},
author = {Li, Sugang and Fan, Xiaoran and Zhang, Yanyong and Trappe, Wade and Lindqvist, Janne and Howard, Richard},
xdoi = {10.1145/3130938},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/audio{\_}traffic{\_}monitoring{\_}literature/auto++.pdf:pdf},
issn = {2474-9567},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
keywords = {Detector,Embedded system,Engineering,Formant,Mobile phone,Noise,Wearable computer},
number = {3},
pages = {1--20},
title = {{Auto++: Detecting Cars Using Embedded Microphones in Real-Time}},
volume = {1},
year = {2017}
}
@article{Alqudah2020,
abstract = {Traffic analysis has many purposes such as evaluating the performance and security of network operations and management. Therefore, network traffic analysis is considered vital for improving networks operation and security. This paper discusses different machine learning approaches for traffic analysis. Increased network traffic and the development of artificial intelligence require new ways to detect intrusions, analyze malware behavior, categorize Internet traffic and other security aspects. Machine learning (ML) shows effective capabilities in solving network problems. A review of the techniques used in the traffic analysis is presented in this paper.},
author = {Alqudah, Nour and Yaseen, Qussai},
xdoi = {10.1016/j.procs.2020.03.111},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/1-s2.0-S1877050920305494-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {machine learning,network security,traffic analysis},
pages = {911--916},
publisher = {Elsevier B.V.},
title = {{Machine Learning for Traffic Analysis: A Review}},
xurl = {https://xdoi.org/10.1016/j.procs.2020.03.111},
volume = {170},
year = {2020}
}
@book{Debashi2018,
abstract = {Maintaining situational awareness of what is happening within a computer network is challenging, not only because the behaviour happens within machines, but also because data traffic speeds and volumes are beyond human ability to process. Visualisation techniques are widely used to present information about network traffic dynamics. Although they provide operators with an overall view and specific information about particular traffic or attacks on the network, they often still fail to represent the events in an understandable way. Also, because they require visual attention they are not well suited to continuous monitoring scenarios in which network administrators must carry out other tasks. Here we present SoNSTAR (Sonification of Networks for SiTuational AwaReness), a real-time sonification system for monitoring computer networks to support network administrators' situational awareness. SoNSTAR provides an auditory representation of all the TCP/IP traffic within a network based on the different traffic flows between between network hosts. A user study showed that SoNSTAR raises situational awareness levels by enabling operators to understand network behaviour and with the benefit of lower workload demands (as measured by the NASA TLX method) than visual techniques. SoNSTAR identifies network traffic features by inspecting the status flags of TCP/IP packet headers. Combinations of these features define particular traffic events which are mapped to recorded sounds to generate a soundscape that represents the real-time status of the network traffic environment. The sequence, timing, and loudness of the different sounds allow the network to be monitored and anomalous behaviour to be detected without the need to continuously watch a monitor screen.},
author = {Debashi, Mohamed and Vickers, Paul},
booktitle = {PLoS ONE},
xdoi = {10.1371/journal.pone.0195948},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/audio{\_}traffic{\_}monitoring{\_}literature/journal.pone.0195948.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
number = {4},
pages = {1--31},
pmid = {29672543},
title = {{Sonification of network traffic flow for monitoring and situational awareness}},
volume = {13},
year = {2018}
}
@article{Foggia2016,
abstract = {In the last decades, several systems based on video analysis have been proposed for automatically detecting accidents on roads to ensure a quick intervention of emergency teams. However, in some situations, the visual information is not sufficient or sufficiently reliable, whereas the use of microphones and audio event detectors can significantly improve the overall reliability of surveillance systems. In this paper, we propose a novel method for detecting road accidents by analyzing audio streams to identify hazardous situations such as tire skidding and car crashes. Our method is based on a two-layer representation of an audio stream: at a low level, the system extracts a set of features that is able to capture the discriminant properties of the events of interest, and at a high level, a representation based on a bag-of-words approach is then exploited in order to detect both short and sustained events. The deployment architecture for using the system in real environments is discussed, together with an experimental analysis carried out on a data set made publicly available for benchmarking purposes. The obtained results confirm the effectiveness of the proposed approach.},
author = {Foggia, Pasquale and Petkov, Nicolai and Saggese, Alessia and Strisciuglio, Nicola and Vento, Mario},
xdoi = {10.1109/TITS.2015.2470216},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/audio{\_}traffic{\_}monitoring{\_}literature/1-s2.0-S1877050920305494-main.pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Accident detection,Audio detection,Audio events,Car crashes,Hazard detection,Tire skidding},
number = {1},
pages = {279--288},
title = {{Audio surveillance of roads: A system for detecting anomalous sounds}},
volume = {17},
year = {2016}
}
@article{Mohammed2019,
abstract = {The Internet is constantly growing in size and becoming more complex. The field of networking is thus continuously progressing to cope with this monumental growth of network traffic. While approaches such as Software Defined Networking (SDN) can provide a centralized control mechanism for network traffic measurement, control, and prediction, still the amount of data received by the SDN controller is huge. To process that data, it has recently been suggested to use Machine Learning (ML). In this paper, we review existing proposal for using ML in an SDN context for traffic measurement (specifically, classification) and traffic prediction. We will especially focus on approaches that use Deep learning (DL) in traffic prediction, which seems to have been mostly untapped by existing surveys. Furthermore, we discuss remaining challenges and suggest future research directions.},
author = {Mohammed, Aysse Rumeysa and Mohammed, Shady A. and Shirmohammadi, Shervin},
xdoi = {10.1109/IWMN.2019.8805044},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/08805044.pdf:pdf},
isbn = {9781728112732},
journal = {2019 IEEE International Symposium on Measurements and Networking, M and N 2019 - Proceedings},
keywords = {Network measurement,deep learning,machine learning,software defined networking,traffic classification,traffic prediction.},
title = {{Machine Learning and Deep Learning Based Traffic Classification and Prediction in Software Defined Networking}},
year = {2019}
}
@article{Won2019,
abstract = {A traffic monitoring system (TMS) is an integral part of Intelligent Transportation Systems (ITS). It is an essential tool for traffic analysis and planning. One of the biggest challenges is, however, the high cost especially in covering the huge rural road network. In this paper, we propose to address the problem by developing a novel TMS called DeepWiTraffic. DeepWiTraffic is a low-cost, portable, and non-intrusive solution that is built only with two WiFi transceivers. It exploits the unique WiFi Channel State Information (CSI) of passing vehicles to perform detection and classification of vehicles. Spatial and temporal correlations of CSI amplitude and phase data are identified and analyzed using a machine learning technique to classify vehicles into five different types: motorcycles, passenger vehicles, SUVs, pickup trucks, and large trucks. A large amount of CSI data and ground-truth video data are collected over a month period from a real-world two-lane rural roadway to validate the effectiveness of DeepWiTraffic. The results validate that DeepWiTraffic is an effective TMS with the average detection accuracy of 99.4{\%} and the average classification accuracy of 91.1{\%} in comparison with state-of-the-art non-intrusive TMSs.},
archivePrefix = {arXiv},
arxivId = {1812.08208},
author = {Won, Myounggyu and Sahu, Sayan and Park, Kyung Joon},
xdoi = {10.1109/MASS.2019.00062},
eprint = {1812.08208},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/1812.08208.pdf:pdf},
isbn = {9781728146010},
journal = {Proceedings - 2019 IEEE 16th International Conference on Mobile Ad Hoc and Smart Systems, MASS 2019},
keywords = {Traffic Monitoring System,Vehicle Classification,WiFi CSI},
pages = {476--484},
title = {{DeepWiTraffic: Low cost WiFi-based traffic monitoring system using deep learning}},
year = {2019}
}
@article{Na2016,
abstract = {Vehicle emits sounds as it travels along the road, which can be used for traffic monitoring. In this paper, an acoustic based traffic monitoring system is designed and implemented. The system utilizes a cross microphone array to collect road-side acoustic signals. Then, lane positions are automatically detected by the built-in lane detection module. Eventually, different measuring indices which reflect the road condition and traffic quality are derived according to the collected signals and the detected lanes. Since acoustic sensor is less expensive than other types of vehicle sensors, and acoustic features are robust against light, weather, and environmental variations, we expect that the proposed acoustic traffic monitoring system will have lower hardware cost, and become a good complement to the existing traffic monitoring techniques.},
author = {Na, Yueyue and Guo, Yanmeng and Fu, Qiang and Yan, Yonghong},
xdoi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.41},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/audio{\_}traffic{\_}monitoring{\_}literature/W020150928605950101461.pdf:pdf},
isbn = {9781467372114},
journal = {Proceedings - 2015 IEEE 12th International Conference on Ubiquitous Intelligence and Computing, 2015 IEEE 12th International Conference on Advanced and Trusted Computing, 2015 IEEE 15th International Conference on Scalable Computing and Communications, 20},
keywords = {Beamforming,Intelligent transportation system,Traffic monitoring,Vehicle counting,Vehicle speed estimation},
number = {2012},
pages = {119--126},
title = {{An acoustic traffic monitoring system: Design and implementation}},
year = {2016}
}
@article{Zhang:2020:FrameLevelAttention:ARXIV,
abstract = {Environmental sound classification (ESC) is a challenging problem due to the complexity of sounds. The classification performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. However, ESC often suffers from the semantically irrelevant frames and silent frames. In order to deal with this, we employ a frame-level attention model to focus on the semantically relevant frames and salient frames. Specifically, we first propose a convolutional recurrent neural network to learn spectro-temporal features and temporal correlations. Then, we extend our convolutional RNN model with a frame-level attention mechanism to learn discriminative feature representations for ESC. We investigated the classification performance when using different attention scaling function and applying different layers. Experiments were conducted on ESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness of the proposed method and our method achieved the state-of-the-art or competitive classification accuracy with lower computational complexity. We also visualized our attention results and observed that the proposed attention mechanism was able to lead the network tofocus on the semantically relevant parts of environmental sounds.},
archivePrefix = {arXiv},
arxivId = {2007.07241},
author = {Zhang, Zhichao and Xu, Shugong and Zhang, Shunqing and Qiao, Tianhao and Cao, Shan},
eprint = {2007.07241},
pages = {1--8},
title = {{Learning Frame Level Attention for Environmental Sound Classification}},
xurl = {http://arxiv.org/abs/2007.07241},
year = {2020}
}
@inproceedings{Chen:2020:Loops:ISMIR,
address = {Montre{\'{a}}l, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:2008.02011v1},
author = {Chen, Bo-yu and Smith, Jordan B L and Yang, Yi-hsuan},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {arXiv:2008.02011v1},
file = {:C$\backslash$:/Users/abr/Downloads/2008.02011.pdf:pdf},
pages = {1--8},
title = {{Neural Loop Combiner: Neural Network Models for ASsessing the Compatibility of Loops}}
}
@article{Pati:2020:dMelodies:ARXIV,
abstract = {Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.},
archivePrefix = {arXiv},
arxivId = {2007.15067},
author = {Pati, Ashis and Gururani, Siddharth and Lerch, Alexander},
eprint = {2007.15067},
file = {:C$\backslash$:/Users/abr/Downloads/d-melodies-a-music-dataset-for-disentanglement-learning.pdf:pdf},
title = {{dMelodies: A Music Dataset for Disentanglement Learning}},
xurl = {http://arxiv.org/abs/2007.15067},
year = {2020}
}
@inproceedings{Kim:2019:PointWise:WASPAA,
address = {New Paltz, NY, USA},
author = {Kim, Bongjun and Pardo, Bryan},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {:C$\backslash$:/Users/abr/Downloads/08937213.pdf:pdf},
isbn = {9781728111230},
pages = {1--5},
title = {{Sound Event Detection using Point-Labeled Data}},
year = {2019}
}
@article{Zhang:2020:ASC:IEEE_SPL,
author = {Zhang, Liwen and Han, Jiqing and Shi, Ziqiang},
xdoi = {10.1109/lsp.2020.2996085},
file = {:C$\backslash$:/Users/abr/Downloads/09097422.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
pages = {950--954},
title = {{Learning Temporal Relations from Semantic Neighbors for Acoustic Scene Classification}},
volume = {27},
year = {2020}
}
@inproceedings{Hu:2020:ASC:DCASE,
author = {Hu, Hu and Yang, Chao-han Huck and Xia, Xianjun and Bai, Xue and Tang, Xin and Wang, Yajian and Niu, Shutong and Chai, Li and Li, Juanjuan and Zhu, Hongning and Bao, Feng and Zhao, Yuanjun and Siniscalchi, Sabato Marco and Wang, Yannan and Du, Jun and Lee, Chin-hui},
booktitle = {Detection and Classification of Acoustic Scenes and Events 2020},
file = {::},
pages = {2--6},
title = {{Device-Robust Acoustic Scene Classification based on Two-Stage Categorization and Data Augmentation}},
year = {2020}
}
@article{Kong:2019:PANN:ARXIV,
abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification and sound event detection. Recently neural networks have been applied to solve audio pattern recognition problems. However, previous systems focus on small datasets, which limits the performance of audio pattern recognition systems. Recently in computer vision and natural language processing, systems pretrained on large datasets have generalized well to several tasks. However, there is limited research on pretraining neural networks on large datasets for audio pattern recognition. In this paper, we propose large-scale pretrained audio neural networks (PANNs) trained on AudioSet. We propose to use Wavegram, a feature learned from waveform, and the mel spectrogram as input. We investigate the performance and complexity of a variety of convolutional neural networks. Our proposed AudioSet tagging system achieves a state-of-the-art mean average precision (mAP) of 0.439, outperforming the best previous system of 0.392. We transferred a PANN to six audio pattern recognition tasks and achieve state-of-the-art performance in many tasks. Source code and pretrained models have been released.},
archivePrefix = {arXiv},
arxivId = {1912.10211},
author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
eprint = {1912.10211},
file = {::},
pages = {1--14},
title = {{PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition}},
xurl = {http://arxiv.org/abs/1912.10211},
year = {2019}
}
@techreport{Suh:2020:ASCCNN:DCASE,
author = {Suh, Sangwon and Park, Sooyoung and Jeong, Youngho and Lee, Taejin},
booktitle = {Detection and Classification of Acoustic Scenes and Events 2020 Challenge},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2020{\_}07{\_}17{\_}ASC{\_}ICASSP{\_}2020/DCASE2020{\_}Suh{\_}101 (1).pdf:pdf},
title = {{Designing Acoustic Scene Classification Models with CNN Variants}},
year = {2020}
}
@article{Zieger:2009:Surveillance:AVSS,
abstract = {This paper describes a surveillance system for intrusion detection which is based only on information derived from the processing of audio signals acquired by a Distributed Microphone Network (DMN). In particular the system exploits different acoustic features and estimates of acoustic event positions in order to detect intrusion and reject possible false alarms that may be generated by sound sources inside and outside the monitored room. An evaluation has been conducted in order to measure the performance in terms of false alarms and missed alarms in presence of acoustic events produced inside and outside a test room. The obtained results are very promising and encouraging for future works aimed at improving the actual system accuracy. {\textcopyright} 2009 IEEE.},
author = {Zieger, Christian and Brutti, Alessio and Svaizer, Piergiorgio},
xdoi = {10.1109/AVSS.2009.49},
file = {:C$\backslash$:/Users/abr/Downloads/05279765.pdf:pdf},
isbn = {9780769537184},
journal = {6th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2009},
keywords = {Audio,Distributed microphone network,Intrusion detection},
pages = {314--319},
title = {{Acoustic based surveillance system for intrusion detection}},
year = {2009}
}
@article{Workera2020,
abstract = {Workera is a deeplearning.ai company that helps data scientists, machine learning engineers, and software engineers meet their career goals by providing mentorship and top-quality job opportunities. Our mission is to make sure that every person, regardless of background, has the opportunity to achieve their fullest potential and fulfill their career goals in AI. WORKERA a deeplearning.ai company 2 The executive summary goes here Developing an AI project development life cycle involves five distinct tasks: • Data engineering: People responsible for data engineering prepare data and transform data into formats that other team members can use. • Modeling: People assigned to modeling look for patterns in data that can help a company predict outcomes of various decisions, identify business risks and opportunities, or determine cause-and-effect relationships.},
author = {Workera},
file = {::},
pages = {1--22},
title = {{AI Career Pathways : Put Yourself on the Right Track}},
year = {2020}
}
@inproceedings{Engel:2020:DDSP:ICLR,
address = {Addis Ababa, Ethopia},
author = {Engel, Jesse and Hantrakul, Lamtharn and Gu, Chenje and Roberts, Adam},
booktitle = {2Proceedings of the International Conference on Learning Representations (ICLR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Engel et al. - 2020 - DDSPDifferentiable Digital Signal Processing(2).pdf:pdf},
title = {{DDSP:Differentiable Digital Signal Processing}},
year = {2020}
}
@article{Cakir:2018:EndtoEndAED:IJCNN,
abstract = {Sound event detection systems typically consist of two stages: Extracting hand-crafted features from the raw audio waveform, and learning a mapping between these features and the target sound events using a classifier. Recently, the focus of sound event detection research has been mostly shifted to the latter stage using standard features such as mel spectrogram as the input for classifiers such as deep neural networks. In this work, we utilize end-to-end approach and propose to combine these two stages in a single deep neural network classifier. The feature extraction over the raw waveform is conducted by a feedforward layer block, whose parameters are initialized to extract the time-frequency representations. The feature extraction parameters are updated during training, resulting with a representation that is optimized for the specific task. This feature extraction block is followed by (and jointly trained with) a convolutional recurrent network, which has recently given state-of-the-art results in many sound recognition tasks. The proposed system does not outperform a convolutional recurrent network with fixed hand-crafted features. The final magnitude spectrum characteristics of the feature extraction block parameters indicate that the most relevant information for the given task is contained in 0 - 3 kHz frequency range, and this is also supported by the empirical results on the SED performance.},
archivePrefix = {arXiv},
arxivId = {1805.03647},
author = {Cakir, Emre and Virtanen, Tuomas},
xdoi = {10.1109/IJCNN.2018.8489470},
eprint = {1805.03647},
file = {::},
isbn = {9781509060146},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {convolutional recurrent neural networks,end-to-end,feature learning,neural networks},
title = {{End-to-End Polyphonic Sound Event Detection Using Convolutional Recurrent Neural Networks with Learned Time-Frequency Representation Input}},
volume = {2018-July},
year = {2018}
}
@article{Lee:2018:SampleCNN:AS,
abstract = {Convolutional Neural Networks (CNN) have been applied to diverse machine learning tasks for different modalities of raw data in an end-to-end fashion. In the audio domain, a raw waveform-based approach has been explored to directly learn hierarchical characteristics of audio. However, the majority of previous studies have limited their model capacity by taking a frame-level structure similar to short-time Fourier transforms. We previously proposed a CNN architecture which learns representations using sample-level filters beyond typical frame-level input representations. The architecture showed comparable performance to the spectrogram-based CNN model in music auto-tagging. In this paper, we extend the previous work in three ways. First, considering the sample-level model requires much longer training time, we progressively downsample the input signals and examine how it affects the performance. Second, we extend the model using multi-level and multi-scale feature aggregation technique and subsequently conduct transfer learning for several music classification tasks. Finally, we visualize filters learned by the sample-level CNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency.},
author = {Lee, Jongpil and Park, Jiyoung and Kim, Keunhyoung Luke and Nam, Juhan},
xdoi = {10.3390/APP8010150},
file = {::},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Convolutional neural networks,Downsampling,Filter visualization,Music classification,Raw waveforms,Sample-level filters,Transfer learning},
number = {1},
title = {{SampleCNN: End-to-end deep convolutional neural networks using very small filters for music classification}},
volume = {8},
year = {2018}
}
@article{Cella:2020:OrchidealSOL:ARXIV,
abstract = {This paper introduces OrchideaSOL, a free dataset of samples of extended instrumental playing techniques, designed to be used as default dataset for the Orchidea framework for target-based computer-aided orchestration. OrchideaSOL is a reduced and modified subset of Studio On Line, or SOL for short, a dataset developed at Ircam between 1996 and 1998. We motivate the reasons behind OrchideaSOL and describe the differences between the original SOL and our dataset. We will also show the work done in improving the dynamic ranges of orchestral families and other aspects of the data.},
archivePrefix = {arXiv},
arxivId = {2007.00763},
author = {Cella, Carmine Emanuele and Ghisi, Daniele and Lostanlen, Vincent and L{\'{e}}vy, Fabien and Fineberg, Joshua and Maresz, Yan},
eprint = {2007.00763},
file = {::},
journal = {ArXiV},
title = {{OrchideaSOL: a dataset of extended instrumental techniques for computer-aided orchestration}},
xurl = {http://arxiv.org/abs/2007.00763},
year = {2020}
}
@phdthesis{Abesser:2014:BassGuitar:PHD,
abstract = {Music recordings most often consist of multiple instrument signals, which overlap in time and frequency. In the field of Music Information Retrieval (MIR), existing algorithms for the automatic transcription and analysis of music recordings aim to extract semantic information from mixed audio signals. In the last years, it was frequently observed that the algorithm performance is limited due to the signal interference and the resulting loss of information. One common approach to solve this problem is to first apply source separation algorithms to isolate the present musical instrument signals before analyzing them individually. The performance of source separation algorithms strongly depends on the number of instruments as well as on the amount of spectral overlap.},
author = {Abe{\ss}er, Jakob},
file = {::},
school = {Technische Universit{\"{a}}t Ilmenau},
title = {{Automatic Transcription of Bass Guitar Tracks applied for Music Genre Classification and Sound Synthesis}},
year = {2014}
}
@inproceedings{Dwibedi:2020:RepNet:CVPR,
abstract = {We present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called Repnet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix ({\~{}}90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos. Project webpage: https://sites.google.com/view/repnet .},
archivePrefix = {arXiv},
arxivId = {2006.15418},
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {2006.15418},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwibedi et al. - 2020 - Counting Out Time Class Agnostic Video Repetition Counting in the Wild.pdf:pdf},
title = {{Counting Out Time: Class Agnostic Video Repetition Counting in the Wild}},
xurl = {http://arxiv.org/abs/2006.15418},
year = {2020}
}
@article{Zinemanas2019,
abstract = {We present a novel approach to tackle the problem of sound event detection (SED) in urban environments using end-to-end convolutional neural networks (CNN). It consists of a 1D CNN for extracting the energy on mel-frequency bands from the audio signal based on a simple filter bank, followed by a 2D CNN for the classification task. The main goal of this two-stage architecture is to bring more interpretability to the first layers of the network and to permit their reutilization in other problems of same the domain. We present a novel model to calculate the mel-spectrogam using a neural network that outperforms an existing work, both in its simplicity and its matching performance. Also, we implement a recently proposed approach to normalize the energy of the mel-spectrogram (per channel energy normalization' PCEN) as a layer of the neural network. We show how the parameters of this normalization can be learned by the network and why this is useful for SED on urban environments. We study how the training modifies the filter bank as well as the PCEN normalization parameters. The obtained system achieves classification results that are comparable to the state-of-the-Art, while decreasing the number of parameters involved.},
author = {Zinemanas, Pablo and Cancela, Pablo and Rocamora, Martin},
xdoi = {10.23919/FRUCT.2019.8711906},
file = {:C$\backslash$:/Users/abr/Downloads/ZCR19.pdf:pdf},
isbn = {9789526865386},
issn = {23057254},
journal = {Conference of Open Innovation Association, FRUCT},
number = {April},
pages = {533--539},
title = {{End-to-end convolutional neural networks for sound event detection in urban environments}},
volume = {2019-April},
year = {2019}
}
@article{Kim2020,
author = {Kim, Nam Kyun and Kim, Hong Kook},
file = {::},
title = {{POLYPHONIC SOUND EVENT DETECTION BASED ON CONVOLUTIONAL RECURRENT NEURAL NETWORKS WITH SEMI-SUPERVISED LOSS FUNCTION FOR DCASE CHALLENGE 2020 TASK 4 Technical Report School of Electrical Engineering and Computer Science , 2 AI Graduate School Gwangju Inst}},
year = {2020}
}
@article{Chan2020,
author = {Chan, Teck Kai and Chin, Cheng Siong},
file = {::},
title = {{SEMI-SUPERVISED NMF-CNN FOR SOUND EVENT DETECTION Technical Report Faculty of Science , Agriculture , and Engineering Xylem Water Solution Singapore Pte Ltd 3A International Business Park}},
volume = {2048},
year = {2020}
}
@article{Perez-Castanos2020,
abstract = {Anomalous sound detection (ASD) is, nowadays, one of the topical subjects in machine listening discipline. Unsupervised detection is attracting a lot of interest due to its immediate applicability in many fields. For example, related to industrial processes, the early detection of malfunctions or damage in machines can mean great savings and an improvement in the efficiency of industrial processes. This problem can be solved with an unsupervised ASD solution since industrial machines will not be damaged simply by having this audio data in the training stage. This paper proposes a novel framework based on convolutional autoencoders (both unsupervised and semi-supervised) and a Gammatone-based representation of the audio. The results obtained by these architectures substantially exceed the results presented as a baseline.},
archivePrefix = {arXiv},
arxivId = {2006.15321},
author = {Perez-Castanos, Sergi and Naranjo-Alcazar, Javier and Zuccarello, Pedro and Cobos, Maximo},
eprint = {2006.15321},
file = {::},
pages = {2--6},
title = {{Anomalous Sound Detection using unsupervised and semi-supervised autoencoders and gammatone audio representation}},
xurl = {http://arxiv.org/abs/2006.15321},
year = {2020}
}
@article{Kumar2020,
abstract = {An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learning sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data, and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9{\%} improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks.},
archivePrefix = {arXiv},
arxivId = {2007.00144},
author = {Kumar, Anurag and Ithapu, Vamsi Krishna},
eprint = {2007.00144},
file = {::},
title = {{A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition}},
xurl = {http://arxiv.org/abs/2007.00144},
year = {2020}
}
@article{Bones:2018:SoundCategories:FIP,
abstract = {Five evidence-based taxonomies of everyday sounds frequently reported in the soundscape literature have been generated. An online sorting and category-labeling method that elicits rather than prescribes descriptive words was used. A total of N = 242 participants took part. The main categories of the soundscape taxonomy were people, nature, and manmade, with each dividing into further categories. Sounds within the nature and manmade categories, and two further individual sound sources, dogs, and engines, were explored further by repeating the procedure using multiple exemplars. By generating multidimensional spaces containing both sounds and the spontaneously generated descriptive words the procedure allows for the interpretation of the psychological dimensions along which sounds are organized. This reveals how category formation is based upon different cues-sound source-event identification, subjective-states, and explicit assessment of the acoustic signal-in different contexts. At higher levels of the taxonomy the majority of words described sound source-events. In contrast, when categorizing dog sounds a greater proportion of the words described subjective-states, and valence and arousal scores of these words correlated with their coordinates along the first two dimensions of the data. This is consistent with valence and arousal judgments being the primary categorization strategy used for dog sounds. In contrast, when categorizing engine sounds a greater proportion of the words explicitly described the acoustic signal. The coordinates of sounds along the first two dimensions were found to correlate with fluctuation strength and sharpness, consistent with explicit assessment of acoustic signal features underlying category formation for engine sounds. By eliciting descriptive words the method makes explicit the subjective meaning of these judgments based upon valence and arousal and acoustic properties, and the results demonstrate distinct strategies being spontaneously used to categorize different types of sounds.},
author = {Bones, Oliver and Cox, Trevor J. and Davies, William J.},
xdoi = {10.3389/fpsyg.2018.01277},
file = {:C$\backslash$:/Users/abr/Downloads/fpsyg-09-01277.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {acoustic,acoustic correlates,arousal,categories,category formation,everyday sounds,soundscape,taxonomy,valence},
number = {July},
pages = {1--17},
title = {{Sound Categories: Category Formation and Evidence-Based Taxonomies}},
volume = {9},
year = {2018}
}
@inproceedings{Zinemanas:2019:MAVD:DCASE,
address = {New York, NY, USA},
author = {Zinemanas, Pablo and Cancela, Pablo and Rocamora, Mart{\'{i}}n},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.33682/kfmf-zv94},
file = {::},
pages = {263--267},
title = {{MAVD: A Dataset for Sound Event Detection in Urban Environments}},
year = {2019}
}
@article{Mesaros2016,
abstract = {This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.},
author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
xdoi = {10.3390/app6060162},
file = {::},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Audio content analysis,Audio signal processing,Computational auditory scene analysis,Evaluation of sound event detection,Everyday sounds,Pattern recognition,Polyphonic sound event detection,Sound events},
number = {6},
title = {{Metrics for polyphonic sound event detection}},
volume = {6},
year = {2016}
}
@techreport{Sakashita2018,
abstract = {Many classification tasks using deep learning have improved classification accuracy by using a large amount of training data. However , it is difficult to collect audio data and build a large database. Since training data is restricted in DCASE 2018 Task 1a, unknown acoustic scene must be predicted from less training data. From the results of DCASE 2017[1], we determine that using a convolu-tion neural network and ensemble multiple networks is an effective means for classifying acoustic scenes. In our method we generate mel-spectrogram from binaural audio, mono audio, Harmonic-percussive source separation (HPSS) audio, adaptively divide the spectrogram into multiple ways and learn 9 neural networks. We further improve ensemble accuracy by ensemble learning using these outputs. The classification result of the proposed system was 0.769 for Development dataset and 0.796 for Leaderboard dataset.},
address = {Sakashita:2018:ASC:DCASE},
author = {Sakashita, Yuma and Aono, Masaki},
booktitle = {Detection and Classification of Acoustic Scenes and Events},
xdoi = {10.1109/mra.2018.2802120},
file = {:C$\backslash$:/Users/abr/Downloads/4f976d9970aaec84f7d5379107f577743b13.pdf:pdf},
issn = {1070-9932},
title = {{Acoustic scene classification by ensemble of spectrograms based in adaptive temporal division}},
year = {2018}
}
@article{Fierro:2020:TSM:,
author = {Fierro, Leonardo and V{\"{a}}lim{\"{a}}ki, Vesa},
file = {:C$\backslash$:/Users/abr/Downloads/FierroVlimki-2020-TowardsObjectiveEvaluationofAudioTime-ScaleModificationMethods.pdf:pdf},
number = {June},
pages = {2--7},
title = {{Towards Objective Evaluation of Audio Time-Scale Modification Methods}},
year = {2020}
}
@inproceedings{Arora2017,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)},
xdoi = {10.1109/MMSP.2017.8122258},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario(4).pdf:pdf},
isbn = {978-1-5090-3649-3},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{A study on transfer learning for acoustic event detection in a real life scenario}},
xurl = {http://ieeexplore.ieee.org/document/8122258/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Vaswani:2017:Attention:NIPS,
address = {Long Beach, CA, USA},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
xdoi = {10.1109/2943.974352},
file = {::},
issn = {10772618},
number = {1},
pages = {8--15},
title = {{Attention Is All You Need}},
volume = {8},
year = {2017}
}
@article{Li:2020:DilatedCRNN:ARXIV,
abstract = {Convolutional recurrent neural networks (CRNNs) have achieved state-of-the-art performance for sound event detection (SED). In this paper, we propose to use a dilated CRNN, namely a CRNN with a dilated convolutional kernel, as the classifier for the task of SED. We investigate the effectiveness of dilation operations which provide a CRNN with expanded receptive fields to capture long temporal context without increasing the amount of CRNN's parameters. Compared to the classifier of the baseline CRNN, the classifier of the dilated CRNN obtains a maximum increase of 1.9{\%}, 6.3{\%} and 2.5{\%} at F1 score and a maximum decrease of 1.7{\%}, 4.1{\%} and 3.9{\%} at error rate (ER), on the publicly available audio corpora of the TUTSED Synthetic 2016, the TUT Sound Event 2016 and the TUT Sound Event 2017, respectively.},
author = {Li, Yanxiong and Liu, Mingle and Drossos, Konstantinos and Virtanen, Tuomas},
xdoi = {10.1109/icassp40776.2020.9054433},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks.pdf:pdf},
isbn = {6191101570},
pages = {286--290},
title = {{Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks}},
year = {2020}
}
@article{Grondin2019,
abstract = {This paper proposes sound event localization and detection methods from multichannel recording. The proposed system is based on two Convolutional Recurrent Neural Networks (CRNNs) to perform sound event detection (SED) and time difference of arrival (TDOA) estimation on each pair of microphones in a microphone array. In this paper, the system is evaluated with a four-microphone array, and thus combines the results from six pairs of microphones to provide a final classification and a 3-D direction of arrival (DOA) estimate. Results demonstrate that the proposed approach outperforms the DCASE 2019 baseline system.},
archivePrefix = {arXiv},
arxivId = {1910.10049},
author = {Grondin, Francois and Sobieraj, Iwona and Plumbley, Mark and Glass, James},
xdoi = {10.33682/4v2a-7q02},
eprint = {1910.10049},
file = {::},
number = {October},
pages = {84--88},
title = {{Sound Event Localization and Detection Using CRNN on Pairs of Microphones}},
year = {2019}
}
@article{Bilen:2020:AEDEval:ARXIV,
abstract = {This work defines a new framework for performance evaluation of polyphonic sound event detection (SED) systems, which overcomes the limitations of the conventional collar-based event decisions, event F-scores and event error rates. The proposed framework introduces a definition of event detection that is more robust against labelling subjectivity. It also resorts to polyphonic receiver operating characteristic (ROC) curves to deliver more global insight into system performance than F1-scores, and proposes a reduction of these curves into a single polyphonic sound detection score (PSDS), which allows system comparison independently from operating points (OPs). The presented method also delivers better insight into data biases and classification stability across sound classes. Furthermore, it can be tuned to varying applications in order to match a variety of user experience requirements. The benefits of the proposed approach are demonstrated by re-evaluating the baseline and two of the top-performing systems from DCASE 2019 Task 4.},
archivePrefix = {arXiv},
arxivId = {1910.08440},
author = {Bilen, Cagdas and Ferroni, Giacomo and Tuveri, Francesco and Azcarreta, Juan and Krstulovic, Sacha},
xdoi = {10.1109/icassp40776.2020.9052995},
eprint = {1910.08440},
file = {::},
pages = {61--65},
title = {{A Framework for the Robust Evaluation of Sound Event Detection}},
year = {2020}
}
@article{Imoto2020,
abstract = {Sound event detection (SED) and acoustic scene classification (ASC) are major tasks in environmental sound analysis. Considering that sound events and scenes are closely related to each other, some works have addressed joint analyses of sound events and acoustic scenes based on multitask learning (MTL), in which the knowledge of sound events and scenes can help in estimating them mutually. The conventional MTL-based methods utilize one-hot scene labels to train the relationship between sound events and scenes; thus, the conventional methods cannot model the extent to which sound events and scenes are related. However, in the real environment, common sound events may occur in some acoustic scenes; on the other hand, some sound events occur only in a limited acoustic scene. In this paper, we thus propose a new method for SED based on MTL of SED and ASC using the soft labels of acoustic scenes, which enable us to model the extent to which sound events and scenes are related. Experiments conducted using TUT Sound Events 2016/2017 and TUT Acoustic Scenes 2016 datasets show that the proposed method improves the SED performance by 3.80{\%} in F-score compared with conventional MTL-based SED.},
archivePrefix = {arXiv},
arxivId = {2002.05848},
author = {Imoto, Keisuke and Tonami, Noriyuki and Koizumi, Yuma and Yasuda, Masahiro and Yamanishi, Ryosuke and Yamashita, Yoichi},
xdoi = {10.1109/icassp40776.2020.9053912},
eprint = {2002.05848},
file = {::},
pages = {621--625},
title = {{Sound Event Detection by Multitask Learning of Sound Events and Scenes with Soft Scene Labels}},
year = {2020}
}
@article{Shimada2020,
abstract = {Few-shot learning systems for sound event recognition have gained interests since they require only a few examples to adapt to new target classes without fine-tuning. However, such systems have only been applied to chunks of sounds for classification or verification. In this paper, we aim to achieve few-shot detection of rare sound events, from query sequence that contain not only the target events but also the other events and background noise. Therefore, it is required to prevent false positive reactions to both the other events and background noise. We propose metric learning with background noise class for the few-shot detection. The contribution is to present the explicit inclusion of background noise as an independent class, a suitable loss function that emphasizes this additional class, and a corresponding sampling strategy that assists training. It provides a feature space where the event classes and the background noise class are sufficiently separated. Evaluations on few-shot detection tasks, using DCASE 2017 task2 and ESC-50, show that our proposed method outperforms metric learning without considering the background noise class. The few-shot detection performance is also comparable to that of the DCASE 2017 task2 baseline system, which requires huge amount of annotated audio data.},
archivePrefix = {arXiv},
arxivId = {1910.13724},
author = {Shimada, Kazuki and Koyama, Yuichiro and Inoue, Akira},
xdoi = {10.1109/icassp40776.2020.9054712},
eprint = {1910.13724},
file = {::},
pages = {616--620},
title = {{Metric Learning with Background Noise Class for Few-Shot Detection of Rare Sound Events}},
year = {2020}
}
@article{Zhao:2020:ActiveLearningSED:ARXIV,
abstract = {This paper proposes an active learning system for sound event detection (SED). It aims at maximizing the accuracy of a learned SED model with limited annotation effort. The proposed system analyzes an initially unlabeled audio dataset, from which it selects sound segments for manual annotation. The candidate segments are generated based on a proposed change point detection approach, and the selection is based on the principle of mismatch-first farthest-traversal. During the training of SED models, recordings are used as training inputs, preserving the long-term context for annotated segments. The proposed system clearly outperforms reference methods in the two datasets used for evaluation (TUT Rare Sound 2017 and TAU Spatial Sound 2019). Training with recordings as context outperforms training with only annotated segments. Mismatch-first farthest-traversal outperforms reference sample selection methods based on random sampling and uncertainty sampling. Remarkably, the required annotation effort can be greatly reduced on the dataset where target sound events are rare: by annotating only 2{\%} of the training data, the achieved SED performance is similar to annotating all the training data.},
archivePrefix = {arXiv},
arxivId = {2002.05033},
author = {Zhao, Shuyang and Heittola, Toni and Virtanen, Tuomas},
eprint = {2002.05033},
file = {::},
pages = {1--11},
title = {{Active Learning for Sound Event Detection}},
xurl = {http://arxiv.org/abs/2002.05033},
year = {2020}
}
@article{Spadini2019,
abstract = {Due to the growing demand for improving surveillance capabilities in smart cities, systems need to be developed to provide better monitoring capabilities to competent authorities, agencies responsible for strategic resource management, and emergency call centers. This work assumes that, as a complementary monitoring solution, the use of a system capable of detecting the occurrence of sound events, performing the Sound Events Recognition (SER) task, is highly convenient. In order to contribute to the classification of such events, this paper explored several classifiers over the SESA dataset, composed of audios of three hazard classes (gunshots, explosions, and sirens) and a class of casual sounds that could be misinterpreted as some of the other sounds. The best result was obtained by SGD, with an accuracy of 72.13{\%} with 6.81 ms classification time, reinforcing the viability of such an approach.},
archivePrefix = {arXiv},
arxivId = {1910.12369},
author = {Spadini, Tito and Silva, Dimitri Leandro de Oliveira and Suyama, Ricardo},
eprint = {1910.12369},
file = {:C$\backslash$:/Users/abr/Downloads/1910.12369.pdf:pdf},
pages = {2--5},
title = {{Sound Event Recognition in a Smart City Surveillance Context}},
xurl = {http://arxiv.org/abs/1910.12369},
year = {2019}
}
@article{Shi2020,
abstract = {In this paper, we propose a method called Hodge and Podge for sound event detection. We demonstrate Hodge and Podge on the dataset of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge Task 4. This task aims to predict the presence or absence and the onset and offset times of sound events in home environments. Sound event detection is challenging due to the lack of large scale real strongly labeled data. Recently deep semi-supervised learning (SSL) has proven to be effective in modeling with weakly labeled and unlabeled data. This work explores how to extend deep SSL to result in a new, state-of-the-art sound event detection method called Hodge and Podge. With convolutional recurrent neural networks (CRNN) as the backbone network, first, a multi-scale squeeze-excitation mechanism is introduced and added to generate a pyramid squeeze-excitation CRNN. The pyramid squeeze-excitation layer can pay attention to the issue that different sound events have different durations, and to adaptively recalibrate channel-wise spectrogram responses. Further, in order to remedy the lack of real strongly labeled data problem, we propose multi-hot MixMatch and composition consistency training with temporal-frequency augmentation. Our experiments with the public DCASE2019 challenge task 4 validation data resulted in an event-based F-score of 43.4$\backslash${\%}, and is about absolutely 1.6$\backslash${\%} better than state-of-the-art methods in the challenge. While the F-score of the official baseline is 25.8$\backslash${\%}.},
archivePrefix = {arXiv},
arxivId = {2002.06021},
author = {Shi, Ziqiang and Liu, Liu and Lin, Huibin and Liu, Rujie},
eprint = {2002.06021},
file = {::},
pages = {1--8},
title = {{Hodge and Podge: Hybrid Supervised Sound Event Detection with Multi-Hot MixMatch and Composition Consistence Training}},
xurl = {http://arxiv.org/abs/2002.06021},
year = {2020}
}
@inproceedings{Kao:2020:AEDPooling:ICASSP,
abstract = {Acoustic event classification (AEC) and acoustic event detection (AED) refer to the task of detecting whether specific target events occur in audios. As long short-term memory (LSTM) leads to state-of-the-art results in various speech related tasks, it is employed as a popular solution for AEC as well. This paper focuses on investigating the dynamics of LSTM model on AEC tasks. It includes a detailed analysis on LSTM memory retaining, and a benchmarking of nine different pooling methods on LSTM models using 1.7M generated mixture clips of multiple events with different signal-to-noise ratios. This paper focuses on understanding: 1) utterance-level classification accuracy; 2) sensitivity to event position within an utterance. The analysis is done on the dataset for the detection of rare sound events from DCASE 2017 Challenge. We find max pooling on the prediction level to perform the best among the nine pooling approaches in terms of classification accuracy and insensitivity to event position within an utterance. To authors' best knowledge, this is the first kind of such work focused on LSTM dynamics for AEC tasks.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {2002.06279},
author = {Kao, Chieh-Chi and Sun, Ming and Wang, Weiran and Wang, Chao},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/icassp40776.2020.9053150},
eprint = {2002.06279},
file = {::},
pages = {316--320},
title = {{A Comparison of Pooling Methods on LSTM Models for Rare Acoustic Event Classification}},
year = {2020}
}
@article{Naranjo-Alcazar2020,
abstract = {The problem of training a deep neural network with a small set of positive samples is known as few-shot learning (FSL). It is widely known that traditional deep learning (DL) algorithms usually show very good performance when trained with large datasets. However, in many applications, it is not possible to obtain such a high number of samples. In the image domain, typical FSL applications are those related to face recognition. In the audio domain, music fraud or speaker recognition can be clearly benefited from FSL methods. This paper deals with the application of FSL to the detection of specific and intentional acoustic events given by different types of sound alarms, such as door bells or fire alarms, using a limited number of samples. These sounds typically occur in domestic environments where many events corresponding to a wide variety of sound classes take place. Therefore, the detection of such alarms in a practical scenario can be considered an open-set recognition (OSR) problem. To address the lack of a dedicated public dataset for audio FSL, researchers usually make modifications on other available datasets. This paper is aimed at providing the audio recognition community with a carefully annotated dataset for FSL and OSR comprised of 1360 clips from 34 classes divided into pattern sounds and unwanted sounds. To facilitate and promote research in this area, results with two baseline systems (one trained from scratch and another based on transfer learning), are presented.},
archivePrefix = {arXiv},
arxivId = {2002.11561},
author = {Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Zuccarrello, Pedro and Cobos, Maximo},
eprint = {2002.11561},
file = {::},
title = {{An Open-set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments}},
xurl = {http://arxiv.org/abs/2002.11561},
year = {2020}
}
@article{Shi2020a,
abstract = {We study few-shot acoustic event detection (AED) in this paper. Few-shot learning enables detection of new events with very limited labeled data. Compared to other research areas like computer vision, few-shot learning for audio recognition has been under-studied. We formulate few-shot AED problem and explore different ways of utilizing traditional supervised methods for this setting as well as a variety of meta-learning approaches, which are conventionally used to solve few-shot classification problem. Compared to supervised baselines, meta-learning models achieve superior performance, thus showing its effectiveness on generalization to new audio events. Our analysis including impact of initialization and domain discrepancy further validate the advantage of meta-learning approaches in few-shot AED.},
archivePrefix = {arXiv},
arxivId = {arXiv:2002.09143v1},
author = {Shi, Bowen and Sun, Ming and Puvvada, Krishna C. and Kao, Chieh-Chi and Matsoukas, Spyros and Wang, Chao},
xdoi = {10.1109/icassp40776.2020.9053336},
eprint = {arXiv:2002.09143v1},
file = {::},
pages = {76--80},
title = {{Few-Shot Acoustic Event Detection Via Meta Learning}},
year = {2020}
}
@article{ThoNguyen2020,
abstract = {Polyphonic sound event detection and direction-of-arrival estimation require different input features from audio signals. While sound event detection mainly relies on time-frequency patterns, direction-of-arrival estimation relies on magnitude or phase differences between microphones. Previous approaches use the same input features for sound event detection and direction-of-arrival estimation, and train the two tasks jointly or in a two-stage transfer-learning manner. We propose a two-step approach that decouples the learning of the sound event detection and directional-of-arrival estimation systems. In the first step, we detect the sound events and estimate the directions-of-arrival separately to optimize the performance of each system. In the second step, we train a deep neural network to match the two output sequences of the event detector and the direction-of-arrival estimator. This modular and hierarchical approach allows the flexibility in the system design, and increase the performance of the whole sound event localization and detection system. The experimental results using the DCASE 2019 sound event localization and detection dataset show an improved performance compared to the previous state-of-the-art solutions.},
archivePrefix = {arXiv},
arxivId = {2002.05865},
author = {{Tho Nguyen}, Thi Ngoc and Jones, Douglas L. and Gan, Woon-Seng},
xdoi = {10.1109/icassp40776.2020.9053045},
eprint = {2002.05865},
file = {::},
pages = {71--75},
title = {{A Sequence Matching Network for Polyphonic Sound Event Localization and Detection}},
year = {2020}
}
@article{Koh2020,
abstract = {This paper presents a new learning strategy for the Sound Event Detection (SED) system to tackle the issues of i) knowledge migration from a pre-trained model to a new target model and ii) learning new sound events without forgetting the previously learned ones without re-training from scratch. In order to migrate the previously learned knowledge from the source model to the target one, a neural adapter is employed on the top of the source model. The source model and the target model are merged via this neural adapter layer. The neural adapter layer facilitates the target model to learn new sound events with minimal training data and maintaining the performance of the previously learned sound events similar to the source model. Our extensive analysis on the DCASE16 and US-SED dataset reveals the effectiveness of the proposed method in transferring knowledge between source and target models without introducing any performance degradation on the previously learned sound events while obtaining a competitive detection performance on the newly learned sound events.},
archivePrefix = {arXiv},
arxivId = {2003.12175},
author = {Koh, Eunjeong and Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu and Visser, Erik},
xdoi = {10.1109/icme46284.2020.9102859},
eprint = {2003.12175},
file = {::},
pages = {1--6},
title = {{Incremental Learning Algorithm For Sound Event Detection}},
year = {2020}
}
@article{Huang2020,
abstract = {There are two sub-tasks implied in the weakly-supervised SED: audio tagging and event boundary detection. Current methods which combine multi-task learning with SED requires annotations both for these two sub-tasks. Since there are only annotations for audio tagging available in weakly-supervised SED, we design multiple branches with different learning purposes instead of pursuing multiple tasks. Similar to multiple tasks, multiple different learning purposes can also prevent the common feature which the multiple branches share from overfitting to any one of the learning purposes. We design these multiple different learning purposes based on combinations of different MIL strategies and different pooling methods. Experiments on the DCASE 2018 Task 4 dataset and the URBAN-SED dataset both show that our method achieves competitive performance.},
archivePrefix = {arXiv},
arxivId = {2002.09661},
author = {Huang, Yuxin and Wang, Xiangdong and Lin, Liwei and Liu, Hong and Qian, Yueliang},
xdoi = {10.1109/icassp40776.2020.9053023},
eprint = {2002.09661},
file = {::},
pages = {641--645},
title = {{Multi-Branch Learning for Weakly-Labeled Sound Event Detection}},
year = {2020}
}
@article{Imoto2019,
abstract = {The types of sound events that occur in a situation are limited, and some sound events are likely to co-occur; for instance, »dishes» and »glass jingling.» In this paper, we propose a technique of sound event detection utilizing graph Laplacian regularization taking the sound event co-occurrence into account. In the proposed method, sound event occurrences are represented as a graph whose nodes indicate the frequency of event occurrence and whose edges indicate the co-occurrence of sound events. This graph representation is then utilized for sound event modeling, which is optimized under an objective function with a regularization term considering the graph structure. Experimental results obtained using TUT Sound Events 2016 development, 2017 development, and TUT Acoustic Scenes 2016 development indicate that the proposed method improves the detection performance of sound events by 7.9 percentage points compared to that of the conventional CNN-BiGRU-based method in terms of the segment-based F1-score. Moreover, the results show that the proposed method can detect co-occurring sound events more accurately than the conventional method.},
archivePrefix = {arXiv},
arxivId = {1902.00816},
author = {Imoto, Keisuke and Kyochi, Seisuke},
xdoi = {10.1109/ICASSP.2019.8683708},
eprint = {1902.00816},
file = {::},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Sound event detection,acoustic scene,convolutional recurrent neural network,graph Laplacian regularization,sound event co-occurrence},
pages = {1--5},
title = {{Sound Event Detection Using Graph Laplacian Regularization Based on Event Co-occurrence}},
volume = {2019-May},
year = {2019}
}
@article{Guirguis2020,
abstract = {The understanding of the surrounding environment plays a critical role in autonomous robotic systems, such as self-driving cars. Extensive research has been carried out concerning visual perception. Yet, to obtain a more complete perception of the environment, autonomous systems of the future should also take acoustic information into account. Recent sound event localization and detection (SELD) frameworks utilize convolutional recurrent neural networks (CRNNs). However, considering the recurrent nature of CRNNs, it becomes challenging to implement them efficiently on embedded hardware. Not only are their computations strenuous to parallelize, but they also require high memory bandwidth and large memory buffers. In this work, we develop a more robust and hardware-friendly novel architecture based on a temporal convolutional network(TCN). The proposed framework (SELD-TCN) outperforms the state-of-the-art SELDnet performance on four different datasets. Moreover, SELD-TCN achieves 4x faster training time per epoch and 40x faster inference time on an ordinary graphics processing unit (GPU).},
archivePrefix = {arXiv},
arxivId = {2003.01609},
author = {Guirguis, Karim and Schorn, Christoph and Guntoro, Andre and Abdulatif, Sherif and Yang, Bin},
eprint = {2003.01609},
file = {::},
title = {{SELD-TCN: Sound Event Localization {\&} Detection via Temporal Convolutional Networks}},
xurl = {http://arxiv.org/abs/2003.01609},
year = {2020}
}
@article{Koh2020a,
abstract = {This paper presents a new learning strategy for the Sound Event Detection (SED) system to tackle the issues of i) knowledge migration from a pre-trained model to a new target model and ii) learning new sound events without forgetting the previously learned ones without re-training from scratch. In order to migrate the previously learned knowledge from the source model to the target one, a neural adapter is employed on the top of the source model. The source model and the target model are merged via this neural adapter layer. The neural adapter layer facilitates the target model to learn new sound events with minimal training data and maintaining the performance of the previously learned sound events similar to the source model. Our extensive analysis on the DCASE16 and US-SED dataset reveals the effectiveness of the proposed method in transferring knowledge between source and target models without introducing any performance degradation on the previously learned sound events while obtaining a competitive detection performance on the newly learned sound events.},
archivePrefix = {arXiv},
arxivId = {2003.12175},
author = {Koh, Eunjeong and Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu and Visser, Erik},
xdoi = {10.1109/icme46284.2020.9102859},
eprint = {2003.12175},
file = {::},
pages = {1--6},
title = {{Incremental Learning Algorithm For Sound Event Detection}},
year = {2020}
}
@article{Liu2020,
abstract = {In recent years, the involvement of synthetic strongly labeled data,weakly labeled data and unlabeled data has drawn much research attentionin semi-supervised sound event detection (SSED). Self-training models carry out predictions without strong annotations and then take predictions with high probabilities as pseudo-labels for retraining. Such models have shown its effectiveness in SSED. However, probabilities are poorly calibrated confidence estimates, and samples with low probabilities are ignored. Hence, we introduce a method of learning confidence deliberately and retaining all data distinctly by applying confidence as weights. Additionally, linear pooling has been considered as a state-of-the-art aggregation function for SSED with weak labeling. In this paper, we propose a power pooling function whose coefficient can be trained automatically to achieve nonlinearity. A confidencebased semi-supervised sound event detection (C-SSED) framework is designed to combine confidence and power pooling. The experimental results demonstrate that confidence is proportional to the accuracy of the predictions. The power pooling function outperforms linear pooling at both error rate and F1 results. In addition, the C-SSED framework achieves a relative error rate reduction of 34{\%} in contrast to the baseline model.},
archivePrefix = {arXiv},
arxivId = {2005.11459},
author = {Liu, Yuzhuo and Chen, Hangting and Zhang, Pengyuan},
eprint = {2005.11459},
file = {::},
title = {{Power Pooling Operators and Confidence Learning for Semi-Supervised Sound Event Detection}},
xurl = {http://arxiv.org/abs/2005.11459},
year = {2020}
}
@article{Pankajakshan:2020:MSSSelfAttention:ARXIV,
abstract = {In this paper we investigate the importance of the extent of memory in sequential self attention for sound recognition. We propose to use a memory controlled sequential self attention mechanism on top of a convolutional recurrent neural network (CRNN) model for polyphonic sound event detection (SED). Experiments on the URBAN-SED dataset demonstrate the impact of the extent of memory on sound recognition performance with the self attention induced SED model. We extend the proposed idea with a multi-head self attention mechanism where each attention head processes the audio embedding with explicit attention width values. The proposed use of memory controlled sequential self attention offers a way to induce relations among frames of sound event tokens. We show that our memory controlled self attention model achieves an event based F -score of 33.92{\%} on the URBAN-SED dataset, outperforming the F -score of 20.10{\%} reported by the model without self attention.},
archivePrefix = {arXiv},
arxivId = {2005.06650},
author = {Pankajakshan, Arjun and Bear, Helen L. and Subramanian, Vinod and Benetos, Emmanouil},
eprint = {2005.06650},
file = {::},
title = {{Memory Controlled Sequential Self Attention for Sound Recognition}},
xurl = {http://arxiv.org/abs/2005.06650},
year = {2020}
}
@article{Shimada2020a,
abstract = {Our systems submitted to the DCASE2020 task{\~{}}3: Sound Event Localization and Detection (SELD) are described in this report. We consider two systems: a single-stage system that solve sound event localization{\~{}}(SEL) and sound event detection{\~{}}(SED) simultaneously, and a two-stage system that first handles the SED and SEL tasks individually and later combines those results. As the single-stage system, we propose a unified training framework that uses an activity-coupled Cartesian DOA vector{\~{}}(ACCDOA) representation as a single target for both the SED and SEL tasks. To efficiently estimate sound event locations and activities, we further propose RD3Net, which incorporates recurrent and convolution layers with dense skip connections and dilation. To generalize the models, we apply three data augmentation techniques: equalized mixture data augmentation{\~{}}(EMDA), rotation of first-order Ambisonic{\~{}}(FOA) singals, and multichannel extension of SpecAugment. Our systems demonstrate a significant improvement over the baseline system.},
archivePrefix = {arXiv},
arxivId = {2006.12014},
author = {Shimada, Kazuki and Takahashi, Naoya and Takahashi, Shusuke and Mitsufuji, Yuki},
eprint = {2006.12014},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shimada et al. - 2020 - Sound Event Localization and Detection Using Activity-Coupled Cartesian DOA Vector and RD3net.pdf:pdf},
title = {{Sound Event Localization and Detection Using Activity-Coupled Cartesian DOA Vector and RD3net}},
xurl = {http://arxiv.org/abs/2006.12014},
year = {2020}
}
@article{Geirhos:2019:TextureCNN:ICLR,
abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
archivePrefix = {arXiv},
arxivId = {1811.12231},
author = {Geirhos, Robert and Michaelis, Claudio and Wichmann, Felix A. and Rubisch, Patricia and Bethge, Matthias and Brendel, Wieland},
eprint = {1811.12231},
file = {::},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--22},
title = {{Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness}},
year = {2019}
}
@article{Favory:2020:COALA:ARXIV,
abstract = {Audio representation learning based on deep neural networks (DNNs) emerged as an alternative approach to hand-crafted features. For achieving high performance, DNNs often need a large amount of annotated data which can be difficult and costly to obtain. In this paper, we propose a method for learning audio representations, aligning the learned latent representations of audio and associated tags. Aligning is done by maximizing the agreement of the latent representations of audio and tags, using a contrastive loss. The result is an audio embedding model which reflects acoustic and semantic characteristics of sounds. We evaluate the quality of our embedding model, measuring its performance as a feature extractor on three different tasks (namely, sound event recognition, and music genre and musical instrument classification), and investigate what type of characteristics the model captures. Our results show that our method is in par with the state-of-the-art in the considered tasks and the embeddings produced with our method are well correlated with some acoustic descriptors.},
archivePrefix = {arXiv},
arxivId = {2006.08386},
author = {Favory, Xavier and Drossos, Konstantinos and Virtanen, Tuomas and Serra, Xavier},
eprint = {2006.08386},
file = {::},
title = {{COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio Representations}},
xurl = {http://arxiv.org/abs/2006.08386},
year = {2020}
}
@article{Kiran2018,
abstract = {Videos represent the primary source of information for surveillance applications. Video material is often available in large quantities but in most cases it contains little or no annotation for supervised learning. This article reviews the state-of-the-art deep learning based methods for video anomaly detection and categorizes them based on the type of model and criteria of detection. We also perform simple studies to understand the different approaches and provide the criteria of evaluation for spatio-temporal anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1801.03149},
author = {Kiran, B. Ravi and Thomas, Dilip Mathew and Parakkal, Ranjith},
xdoi = {10.3390/jimaging4020036},
eprint = {1801.03149},
file = {::},
issn = {2313433X},
journal = {Journal of Imaging},
keywords = {Anomaly detection,Autoencoders,Generative adversarial networks,LSTMs,Predictive models,Representation learning,Unsupervised methods,Variational Autoencoders},
number = {2},
title = {{An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos}},
volume = {4},
year = {2018}
}
@article{Meire2019,
abstract = {This paper compares several different Autoencoder architectures for unsupervised anomaly detection in acoustic signals. The goal of unsupervised anomaly detection in sound is to detect anomalies without having prior knowledge regarding potential anomalies. Use of autoencoders (AE) to learn a normal model is a state-of-the-art technique for unsupervised anomaly detection. However, the main focus is almost always to increase the difference between the reconstruction error of normal and anomalous data, without taking into account network architecture and speed. This is not a problem when enough computational power is available. However, if the aim is to bring this system to the edge, meaning implementing it on the sensor or close to the sensor, speed and amount of parameters of the network become more important. In this paper we will do a comparative study between different AE architectures. For this comparison both the detection accuracy and the computational complexity will be taken into account. Based on this information it can be decided which AE is most suited to be implemented on hardware for real-time applications.},
author = {Meire, Maarten and Karsmakers, Peter},
xdoi = {10.1109/IDAACS.2019.8924301},
file = {::},
isbn = {9781728140681},
journal = {Proceedings of the 2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2019},
keywords = {Anomaly detection in sound,autoencoder,deep learning,real-time},
month = {sep},
pages = {786--790},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Comparison of Deep Autoencoder Architectures for Real-time Acoustic Based Anomaly Detection in Assets}},
volume = {2},
year = {2019}
}
@article{Lin2020,
author = {Lin, Shuyu and Clark, Ronald and Birke, Robert and Sch, Sandro},
file = {::},
isbn = {9781509066315},
pages = {4322--4326},
title = {{ANOMALY DETECTION FOR TIME SERIES USING VAE-LSTM HYBRID MODEL}},
year = {2020}
}
@article{Chalapathy2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.06360v1},
author = {Chalapathy, Raghavendra and Menon, Aditya Krishna},
eprint = {arXiv:1802.06360v1},
file = {::},
number = {August},
pages = {19--23},
title = {{Anomaly Detection using One-Class Neural Networks}},
year = {2018}
}
@article{Ilmenau2020,
author = {Ilmenau, T U and Xplore, Ieee},
file = {::},
isbn = {9781509066315},
pages = {4297--4301},
title = {{TRAINING LSTM FOR UNSUPERVISED ANOMALY DETECTION WITHOUT A PRIORI KNOWLEDGE}},
year = {2020}
}
@article{Ahrens2019,
abstract = {In this paper we propose a novel machine-learning method for anomaly detection applicable to data with periodic characteristics where randomly varying period lengths are explicitly allowed. A multi-dimensional time series analysis is conducted by training a data-adapted classifier consisting of deep convolutional neural networks performing phase classification. The entire algorithm including data pre-processing, period detection, segmentation, and even dynamic adjustment of the neural networks is implemented for fully automatic execution. The proposed method is evaluated on three example datasets from the areas of cardiology, intrusion detection, and signal processing, presenting reasonable performance.},
author = {Ahrens, Lia and Ahrens, Julian and Schotten, Hans D.},
xdoi = {10.1186/s13634-019-0619-3},
file = {::},
issn = {16876180},
journal = {Eurasip Journal on Advances in Signal Processing},
keywords = {Anomaly detection,Convolutional neural networks,Machine learning,Phase classification,Time series analysis,anomaly,ima},
mendeley-tags = {anomaly,ima},
number = {1},
publisher = {EURASIP Journal on Advances in Signal Processing},
title = {{A machine-learning phase classification scheme for anomaly detection in signals with periodic characteristics}},
volume = {2019},
year = {2019}
}
@article{Zhou2017,
abstract = {Deep autoencoders, and other deep neural networks, have demon-strated their eeectiveness in discovering non-linear features across many problem domains. However, in many real-world problems, large outliers and pervasive noise are commonplace, and one may not have access to clean training data as required by standard deep denoising autoencoders. Herein, we demonstrate novel extensions to deep autoencoders which not only maintain a deep autoencoders' ability to discover high quality, non-linear features but can also eliminate outliers and noise without access to any clean training data. Our model is inspired by Robust Principal Component Anal-ysis, and we split the input data X into two parts, X = L D + S, where L D can be eeectively reconstructed by a deep autoencoder and S contains the outliers and noise in the original data X . Since such spliiing increases the robustness of standard deep autoen-coders, we name our model a " Robust Deep Autoencoder (RDA) " . Further, we present generalizations of our results to grouped spar-sity norms which allow one to distinguish random anomalies from other types of structured corruptions, such as a collection of fea-tures being corrupted across many instances or a collection of instances having more corruptions than their fellows. Such " Group Robust Deep Autoencoders (GRDA) " give rise to novel anomaly detection approaches whose superior performance we demonstrate on a selection of benchmark problems.},
author = {Zhou, Chong and Paffenroth, Randy C.},
xdoi = {10.1145/3097983.3098052},
file = {::},
isbn = {9781450348874},
issn = {1070-485X, 1938-3789},
journal = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '17},
keywords = {anomaly,anomaly detection,autoencoder,autoencoders,denoising,group robust deep,ima,robust deep autoencoders},
mendeley-tags = {anomaly,ima},
pages = {665--674},
title = {{Anomaly Detection with Robust Deep Autoencoders}},
xurl = {http://dl.acm.org/citation.cfm?xdoid=3097983.3098052},
year = {2017}
}
@article{Hershey:2017:CNN:ICASSP,
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.09430v2},
author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P.W. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
xdoi = {10.1109/ICASSP.2017.7952132},
eprint = {arXiv:1609.09430v2},
file = {::},
isbn = {9781509041176},
issn = {15206149},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification},
month = {mar},
pages = {131--135},
publisher = {IEEE},
title = {{CNN architectures for large-scale audio classification}},
xurl = {http://ieeexplore.ieee.org/document/7952132/},
year = {2017}
}
@article{Chalapathy2019,
abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
archivePrefix = {arXiv},
arxivId = {1901.03407},
author = {Chalapathy, Raghavendra and Chawla, Sanjay},
eprint = {1901.03407},
file = {::},
month = {jan},
title = {{Deep Learning for Anomaly Detection: A Survey}},
xurl = {http://arxiv.org/abs/1901.03407},
year = {2019}
}
@article{Principi2019,
abstract = {Fault diagnosis of electric motors is a fundamental task for production line testing, and it is usually performed by experienced human operators. In the recent years, several methods have been proposed in the literature for detecting faults automatically. Deep neural networks have been successfully employed for this task, but, up to the authors'knowledge, they have never been used in an unsupervised scenario. This paper proposes an unsupervised method for diagnosing faults of electric motors by using a novelty detection approach based on deep autoencoders. In the proposed method, vibration signals are acquired by using accelerometers and processed to extract Log-Mel coefficients as features. Autoencoders are trained by using normal data only, i.e., data that do not contain faults. Three different autoencoders architectures have been evaluated: the multi-layer perceptron (MLP) autoencoder, the convolutional neural network autoencoder, and the recurrent autoencoder composed of long short-term memory (LSTM) units. The experiments have been conducted by using a dataset created by the authors, and the proposed approaches have been compared to the one-class support vector machine (OC-SVM) algorithm. The performance has been evaluated in terms area under curve (AUC) of the receiver operating characteristic curve, and the results showed that all the autoencoder-based approaches outperform the OC-SVM algorithm. Moreover, the MLP autoencoder is the most performing architecture, achieving an AUC equal to 99.11{\%}.},
author = {Principi, Emanuele and Rossetti, Damiano and Squartini, Stefano and Piazza, Francesco},
xdoi = {10.1109/JAS.2019.1911393},
file = {::},
issn = {23299274},
journal = {IEEE/CAA Journal of Automatica Sinica},
keywords = {Autoencoder,convolutional neural networks,electric motor,fault detection,long short-term memory,neural networks,novelty detection},
number = {2},
pages = {441--451},
title = {{Unsupervised electric motor fault detection by using deep autoencoders}},
volume = {6},
year = {2019}
}
@article{Zhou2017,
abstract = {Deep autoencoders, and other deep neural networks, have demon-strated their eeectiveness in discovering non-linear features across many problem domains. However, in many real-world problems, large outliers and pervasive noise are commonplace, and one may not have access to clean training data as required by standard deep denoising autoencoders. Herein, we demonstrate novel extensions to deep autoencoders which not only maintain a deep autoencoders' ability to discover high quality, non-linear features but can also eliminate outliers and noise without access to any clean training data. Our model is inspired by Robust Principal Component Anal-ysis, and we split the input data X into two parts, X = L D + S, where L D can be eeectively reconstructed by a deep autoencoder and S contains the outliers and noise in the original data X . Since such spliiing increases the robustness of standard deep autoen-coders, we name our model a " Robust Deep Autoencoder (RDA) " . Further, we present generalizations of our results to grouped spar-sity norms which allow one to distinguish random anomalies from other types of structured corruptions, such as a collection of fea-tures being corrupted across many instances or a collection of instances having more corruptions than their fellows. Such " Group Robust Deep Autoencoders (GRDA) " give rise to novel anomaly detection approaches whose superior performance we demonstrate on a selection of benchmark problems.},
author = {Zhou, Chong and Paffenroth, Randy C.},
xdoi = {10.1145/3097983.3098052},
file = {::},
isbn = {9781450348874},
issn = {1070-485X, 1938-3789},
journal = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '17},
keywords = {anomaly,anomaly detection,autoencoder,autoencoders,denoising,group robust deep,ima,robust deep autoencoders},
mendeley-tags = {anomaly,ima},
pages = {665--674},
title = {{Anomaly Detection with Robust Deep Autoencoders}},
xurl = {http://dl.acm.org/citation.cfm?xdoid=3097983.3098052},
year = {2017}
}
@article{Hayashi2020,
author = {Hayashi, Tomoki and Komatsu, Tatsuya and Kondo, Reishi and Toda, Tomoki and Takeda, Kazuya},
xdoi = {10.23919/EUSIPCO.2018.8553423},
file = {::},
isbn = {9789082797015},
issn = {22195491},
journal = {ICASSP 2020},
keywords = {Anomalous sound event detection,Anomaly detection,Neural network,WaveNet},
pages = {2494--2498},
publisher = {IEEE},
title = {{ANOMALOUS SOUND DETECTION BASED ON INTERPOLATION DEEP NEURAL NETWORK Kaori}},
volume = {2018-Septe},
year = {2020}
}
@article{Kawaguchi2017,
author = {Kawaguchi, Yohei and Endo, Takashi},
xdoi = {10.1109/MLSP.2017.8168164},
file = {::},
isbn = {9781509063413},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Autoencoder,End-to-end,Long short-term memory (LSTM),Non-uniform sampling,Sub-Nyquist sampling,anomaly,ima},
mendeley-tags = {anomaly,ima},
pages = {1--6},
title = {{How can we detect anomalies from subsampled audio signals?}},
volume = {2017-Septe},
year = {2017}
}
@article{Purohit2019a,
abstract = {Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388},
archivePrefix = {arXiv},
arxivId = {1909.09347},
author = {Purohit, Harsh and Tanabe, Ryo and Ichige, Kenji and Endo, Takashi and Nikaido, Yuki and Suefusa, Kaori and Kawaguchi, Yohei},
eprint = {1909.09347},
file = {::},
month = {sep},
title = {{MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection}},
xurl = {http://arxiv.org/abs/1909.09347},
year = {2019}
}
@article{Tran2017,
abstract = {We propose a method for video anomaly detection using a winner-take-all convolu-tional autoencoder that has recently been shown to give competitive results in learning for classification task. The method builds on state of the art approaches to anomaly detection using a convolutional autoencoder and a one-class SVM to build a model of normality. The key novelties are (1) using the motion-feature encoding extracted from a convolutional autoencoder as input to a one-class SVM rather than exploiting reconstruc-tion error of the convolutional autoencoder, and (2) introducing a spatial winner-take-all step after the final encoding layer during training to introduce a high degree of sparsity. We demonstrate an improvement in performance over the state of the art on UCSD and Avenue (CUHK) datasets.},
author = {Tran, Hanh T M and Hogg, D C},
file = {::},
journal = {Proceedings of the British Machine Vision Conference 2017},
keywords = {anomaly,ima},
mendeley-tags = {anomaly,ima},
title = {{Anomaly Detection using a Convolutional Winner-Take-All Autoencoder}},
year = {2017}
}
@article{Meire2019,
abstract = {This paper compares several different Autoencoder architectures for unsupervised anomaly detection in acoustic signals. The goal of unsupervised anomaly detection in sound is to detect anomalies without having prior knowledge regarding potential anomalies. Use of autoencoders (AE) to learn a normal model is a state-of-the-art technique for unsupervised anomaly detection. However, the main focus is almost always to increase the difference between the reconstruction error of normal and anomalous data, without taking into account network architecture and speed. This is not a problem when enough computational power is available. However, if the aim is to bring this system to the edge, meaning implementing it on the sensor or close to the sensor, speed and amount of parameters of the network become more important. In this paper we will do a comparative study between different AE architectures. For this comparison both the detection accuracy and the computational complexity will be taken into account. Based on this information it can be decided which AE is most suited to be implemented on hardware for real-time applications.},
author = {Meire, Maarten and Karsmakers, Peter},
xdoi = {10.1109/IDAACS.2019.8924301},
file = {::},
isbn = {9781728140681},
journal = {Proceedings of the 2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2019},
keywords = {Anomaly detection in sound,autoencoder,deep learning,real-time},
month = {sep},
pages = {786--790},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Comparison of Deep Autoencoder Architectures for Real-time Acoustic Based Anomaly Detection in Assets}},
volume = {2},
year = {2019}
}
@article{Chalapathy:2019:AD:ARXIV,
abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
archivePrefix = {arXiv},
arxivId = {1901.03407},
author = {Chalapathy, Raghavendra and Chawla, Sanjay},
eprint = {1901.03407},
file = {::},
month = {jan},
title = {{Deep Learning for Anomaly Detection: A Survey}},
xurl = {http://arxiv.org/abs/1901.03407},
year = {2019}
}
@inproceedings{Hayashi:2020:IDNN:ICASSP,
address = {Barcelona, Spain},
author = {Hayashi, Tomoki and Komatsu, Tatsuya and Kondo, Reishi and Toda, Tomoki and Takeda, Kazuya and Suefusa, Kaori and Nishida, Tomoya and Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Kawaguchi, Yohei},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.23919/EUSIPCO.2018.8553423},
file = {::},
isbn = {9789082797015},
issn = {22195491},
keywords = {Anomalous sound event detection,Anomaly detection,Neural network,WaveNet},
pages = {271--275},
publisher = {IEEE},
title = {{Anomalous Sound Detection based on Interpolation Deep Neural Network}},
year = {2020}
}
@misc{Ilmenau2020,
address = {Barcelona, Spain},
author = {Ilmenau, T U and Xplore, Ieee and Cherdo, Yann and de Kerret, Paul and Pawlak, Renaud},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781509066315},
pages = {4297--4301},
title = {{No Title}},
year = {2020}
}
@article{Chalapathy2018,
abstract = {We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract a progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is sub-optimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN performs on par with state-of-the-art methods and outperformed conventional shallow methods in some scenarios.},
archivePrefix = {arXiv},
arxivId = {1802.06360},
author = {Chalapathy, Raghavendra and Menon, Aditya Krishna and Chawla, Sanjay},
eprint = {1802.06360},
file = {::;::},
number = {August},
pages = {19--23},
title = {{Anomaly Detection using One-Class Neural Networks}},
xurl = {http://arxiv.org/abs/1802.06360},
year = {2018}
}
@book{Lostanlen:2019:EventDetection:PLOS,
abstract = {Bioacoustic sensors, sometimes known as autonomous recording units (ARUs), can record sounds of wildlife over long periods of time in scalable and minimally invasive ways. Deriving per-species abundance estimates from these sensors requires detection, classification, and quantification of animal vocalizations as individual acoustic events. Yet, variability in ambient noise, both over time and across sensors, hinders the reliability of current automated systems for sound event detection (SED), such as convolutional neural networks (CNN) in the time-frequency domain. In this article, we develop, benchmark, and combine several machine listening techniques to improve the generalizability of SED models across heterogeneous acoustic environments. As a case study, we consider the problem of detecting avian flight calls from a ten-hour recording of nocturnal bird migration, recorded by a network of six ARUs in the presence of heterogeneous background noise. Starting from a CNN yielding state-of-the-art accuracy on this task, we introduce two noise adaptation techniques, respectively integrating short-term (60-millisecond) and long-term (30-minute) context. First, we apply per-channel energy normalization (PCEN) in the time-frequency domain, which applies short-term automatic gain control to every subband in the mel-frequency spectrogram. Secondly, we replace the last dense layer in the network by a context-adaptive neural network (CA-NN) layer, i.e. an affine layer whose weights are dynamically adapted at prediction time by an auxiliary network taking long-term summary statistics of spectrotemporal features as input. We show that both techniques are helpful and complementary. [...] We release a pre-trained version of our best performing system under the name of BirdVoxDetect, a ready-to-use detector of avian flight calls in field recordings.},
archivePrefix = {arXiv},
arxivId = {1905.08352},
author = {Lostanlen, Vincent and Salamon, Justin and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
booktitle = {PLoS ONE},
xdoi = {10.1371/journal.pone.0214168},
eprint = {1905.08352},
file = {::},
isbn = {1111111111},
issn = {19326203},
number = {10},
pages = {1--31},
publisher = {Public Library of Science},
title = {{Robust sound event detection in bioacoustic sensor networks}},
xurl = {http://arxiv.org/abs/1905.08352},
volume = {14},
year = {2019}
}
@inproceedings{Kapka:2019:CRNNEnsemble:DCASE,
abstract = {In this technical report, we describe our method for DCASE2019 task 3: Sound Event Localization and Detection. We use four CRNN SELDnet-like single output models which run in a consecutive manner to recover all possible information of occurring events. We decompose the SELD task into estimating number of active sources, estimating direction of arrival of a single source, estimating direction of arrival of the second source where the direction of the first one is known and a multi-label classification task. We use custom consecutive ensemble to predict events' onset, offset, direction of arrival and class. The proposed approach is evaluated on the development set of TAU Spatial Sound Events 2019-Ambisonic.},
author = {Kapka, S{\l}awomir Slawomir and Lewandowski, Mateusz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {CRNN,Index Terms-DCASE 2019,Sound Event Localization and Detection,ambisonics},
language = {en},
pages = {0--3},
title = {{Sound Source Detection, Localization and Classification using Consecutive Ensemble of CRNN Models}},
xurl = {http://dcase.community/documents/challenge2019/technical{\_}reports/DCASE2019{\_}Kapka{\_}26.pdf},
year = {2019}
}
@article{Lin2020,
address = {Barcelona, Spain},
author = {Lin, Shuyu and Clark, Ronald and Birke, Robert and Sch{\"{o}}nborn, Sandro and Trigoni, Niki and Roberts, Stephen and Sch, Sandro},
file = {::},
isbn = {9781509066315},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {4322--4326},
title = {{Anomaly Detection for Time Series using VAE-LSTM Hybrid Model}},
year = {2020}
}
@article{Koizumi2018,
abstract = {This paper proposes a novel optimization principle and its implementation for unsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The goal of unsupervised-ADS is to detect unknown anomalous sound without training data of anomalous sound. Use of an AE as a normal model is a state-of-the-art technique for unsupervised-ADS. To decrease the false positive rate (FPR), the AE is trained to minimize the reconstruction error of normal sounds and the anomaly score is calculated as the reconstruction error of the observed sound. Unfortunately, since this training procedure does not take into account the anomaly score for anomalous sounds, the true positive rate (TPR) does not necessarily increase. In this study, we define an objective function based on the Neyman-Pearson lemma by considering ADS as a statistical hypothesis test. The proposed objective function trains the AE to maximize the TPR under an arbitrary low FPR condition. To calculate the TPR in the objective function, we consider that the set of anomalous sounds is the complementary set of normal sounds and simulate anomalous sounds by using a rejection sampling algorithm. Through experiments using synthetic data, we found that the proposed method improved the performance measures of ADS under low FPR conditions. In addition, we confirmed that the proposed method could detect anomalous sounds in real environments.},
archivePrefix = {arXiv},
arxivId = {1810.09133},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Kawachi, Yuta and Harada, Noboru},
xdoi = {10.1109/TASLP.2018.2877258},
eprint = {1810.09133},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Anomaly detection in sound,Feature extraction,Linear programming,Neyman-Pearson lemma,Probability density function,Speech processing,Task analysis,Training data,anomaly,autoencoder,deep learning,eusipco2019,ima,microphone},
mendeley-tags = {anomaly,eusipco2019,ima,microphone},
pages = {1--13},
title = {{Unsupervised Detection of Anomalous Sound based on Deep Learning and the Neyman-Pearson Lemma}},
year = {2018}
}
@article{Koizumi2019d,
abstract = {This paper introduces a new dataset called "ToyADMOS" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at https://github.com/YumaKoizumi/ToyADMOS-dataset},
archivePrefix = {arXiv},
arxivId = {1908.03299},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Harada, Noboru and Imoto, Keisuke},
eprint = {1908.03299},
file = {::},
number = {Waspaa},
pages = {2--7},
title = {{ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection}},
xurl = {http://arxiv.org/abs/1908.03299},
year = {2019}
}
@article{Chalapathy2018,
abstract = {We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract a progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is sub-optimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN performs on par with state-of-the-art methods and outperformed conventional shallow methods in some scenarios.},
archivePrefix = {arXiv},
arxivId = {1802.06360},
author = {Chalapathy, Raghavendra and Menon, Aditya Krishna and Chawla, Sanjay},
eprint = {1802.06360},
file = {::},
title = {{Anomaly Detection using One-Class Neural Networks}},
xurl = {http://arxiv.org/abs/1802.06360},
year = {2018}
}
@article{Schlachter2019,
abstract = {This paper introduces a generic method which enables to use conventional deep neural networks as end-to-end one-class classifiers. The method is based on splitting given data from one class into two subsets. In one-class classification, only samples of one normal class are available for training. During inference, a closed and tight decision boundary around the training samples is sought which conventional binary or multi-class neural networks are not able to provide. By splitting data into typical and atypical normal subsets, the proposed method can use a binary loss and defines an auxiliary subnetwork for distance constraints in the latent space. Various experiments on three well-known image datasets showed the effectiveness of the proposed method which outperformed seven baselines and had a better or comparable performance to the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1902.01194},
author = {Schlachter, Patrick and Liao, Yiwen and Yang, Bin},
xdoi = {10.1109/DSW.2019.8755576},
eprint = {1902.01194},
file = {::},
isbn = {9781728107080},
journal = {2019 IEEE Data Science Workshop, DSW 2019 - Proceedings},
keywords = {anomaly detection,deep learning,end-to-end model,intra-class splitting,one-class classification},
pages = {100--104},
title = {{Deep One-Class Classification Using Intra-Class Splitting}},
year = {2019}
}
@article{Chalapathy2019,
abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
archivePrefix = {arXiv},
arxivId = {1901.03407},
author = {Chalapathy, Raghavendra and Chawla, Sanjay},
eprint = {1901.03407},
file = {::},
month = {jan},
title = {{Deep Learning for Anomaly Detection: A Survey}},
xurl = {http://arxiv.org/abs/1901.03407},
year = {2019}
}
@article{Koizumi2018,
abstract = {This paper proposes a novel optimization principle and its implementation for unsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The goal of unsupervised-ADS is to detect unknown anomalous sound without training data of anomalous sound. Use of an AE as a normal model is a state-of-the-art technique for unsupervised-ADS. To decrease the false positive rate (FPR), the AE is trained to minimize the reconstruction error of normal sounds and the anomaly score is calculated as the reconstruction error of the observed sound. Unfortunately, since this training procedure does not take into account the anomaly score for anomalous sounds, the true positive rate (TPR) does not necessarily increase. In this study, we define an objective function based on the Neyman-Pearson lemma by considering ADS as a statistical hypothesis test. The proposed objective function trains the AE to maximize the TPR under an arbitrary low FPR condition. To calculate the TPR in the objective function, we consider that the set of anomalous sounds is the complementary set of normal sounds and simulate anomalous sounds by using a rejection sampling algorithm. Through experiments using synthetic data, we found that the proposed method improved the performance measures of ADS under low FPR conditions. In addition, we confirmed that the proposed method could detect anomalous sounds in real environments.},
archivePrefix = {arXiv},
arxivId = {1810.09133},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Kawachi, Yuta and Harada, Noboru},
xdoi = {10.1109/TASLP.2018.2877258},
eprint = {1810.09133},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Anomaly detection in sound,Feature extraction,Linear programming,Neyman-Pearson lemma,Probability density function,Speech processing,Task analysis,Training data,anomaly,autoencoder,deep learning,eusipco2019,ima,microphone},
mendeley-tags = {anomaly,eusipco2019,ima,microphone},
pages = {1--13},
title = {{Unsupervised Detection of Anomalous Sound based on Deep Learning and the Neyman-Pearson Lemma}},
year = {2018}
}
@article{Purohit2019a,
abstract = {Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388},
archivePrefix = {arXiv},
arxivId = {1909.09347},
author = {Purohit, Harsh and Tanabe, Ryo and Ichige, Kenji and Endo, Takashi and Nikaido, Yuki and Suefusa, Kaori and Kawaguchi, Yohei},
eprint = {1909.09347},
file = {::},
month = {sep},
title = {{MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection}},
xurl = {http://arxiv.org/abs/1909.09347},
year = {2019}
}
@article{Chalapathy2017,
abstract = {PCA is a classical statistical technique whose simplicity and maturity has seen it find widespread use for anomaly detection. However, it is limited in this regard by being sensitive to gross perturbations of the input, and by seeking a linear subspace that captures normal behaviour. The first issue has been dealt with by robust PCA, a variant of PCA that explicitly allows for some data points to be arbitrarily corrupted; however, this does not resolve the second issue, and indeed introduces the new issue that one can no longer inductively find anomalies on a test set. This paper addresses both issues in a single model, the robust autoencoder. This method learns a nonlinear subspace that captures the majority of data points, while allowing for some data to have arbitrary corruption. The model is simple to train and leverages recent advances in the optimisation of deep neural networks. Experiments on a range of real-world datasets highlight the model's effectiveness.},
archivePrefix = {arXiv},
arxivId = {1704.06743},
author = {Chalapathy, Raghavendra and Menon, Aditya Krishna and Chawla, Sanjay},
xdoi = {10.1007/978-3-319-71249-9_3},
eprint = {1704.06743},
file = {::},
isbn = {9783319712482},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Anomaly detection,Autoencoders,Deep learning,Outlier detection,Robust PCA},
pages = {36--51},
title = {{Robust, Deep and Inductive Anomaly Detection}},
volume = {10534 LNAI},
year = {2017}
}
@inproceedings{Meire2019a,
abstract = {This paper compares several different Autoencoder architectures for unsupervised anomaly detection in acoustic signals. The goal of unsupervised anomaly detection in sound is to detect anomalies without having prior knowledge regarding potential anomalies. Use of autoencoders (AE) to learn a normal model is a state-of-the-art technique for unsupervised anomaly detection. However, the main focus is almost always to increase the difference between the reconstruction error of normal and anomalous data, without taking into account network architecture and speed. This is not a problem when enough computational power is available. However, if the aim is to bring this system to the edge, meaning implementing it on the sensor or close to the sensor, speed and amount of parameters of the network become more important. In this paper we will do a comparative study between different AE architectures. For this comparison both the detection accuracy and the computational complexity will be taken into account. Based on this information it can be decided which AE is most suited to be implemented on hardware for real-time applications.},
author = {Meire, Maarten and Karsmakers, Peter},
booktitle = {Proceedings of the 2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2019},
xdoi = {10.1109/IDAACS.2019.8924301},
file = {::},
isbn = {9781728140681},
keywords = {Anomaly detection in sound,autoencoder,deep learning,real-time},
month = {sep},
pages = {786--790},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Comparison of Deep Autoencoder Architectures for Real-time Acoustic Based Anomaly Detection in Assets}},
volume = {2},
year = {2019}
}
@article{Marchi2017,
abstract = {In the emerging field of acoustic novelty detection, most research efforts are devoted to probabilistic approaches such as mixture models or state-space models. Only recent studies introduced (pseudo-)generative models for acoustic novelty detection with recurrent neural networks in the form of an autoencoder. In these approaches, auditory spectral features of the next short term frame are predicted from the previous frames by means of Long-Short Term Memory recurrent denoising autoencoders. The reconstruction error between the input and the output of the autoencoder is used as activation signal to detect novel events. There is no evidence of studies focused on comparing previous efforts to automatically recognize novel events from audio signals and giving a broad and in depth evaluation of recurrent neural network-based autoencoders. The present contribution aims to consistently evaluate our recent novel approaches to fill this white spot in the literature and provide insight by extensive evaluations carried out on three databases: A3Novelty, PASCAL CHiME, and PROMETHEUS. Besides providing an extensive analysis of novel and state-of-the-art methods, the article shows how RNN-based autoencoders outperform statistical approaches up to an absolute improvement of 16.4{\%} average F-measure over the three databases.},
author = {Marchi, Erik and Vesperini, Fabio and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
xdoi = {10.1155/2017/4694860},
file = {::},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
title = {{Research article deep recurrent neural network-based autoencoders for acoustic novelty detection}},
volume = {2017},
year = {2017}
}
@inproceedings{Kumar2017,
abstract = {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
annote = {- for weakly labeled data
- CNN
- audio of variable length
- usable for transfer learning for domain and task adaptation
- problem on scene classification:
-- few datasets
-- labeling expensive/difficult
-- begin and end of event subjective
- audioset with weak labels (DCASE2017)
- "Soundnet" transfer visual model to audio data
- train on "Audioset"(weakly labeled example from Youtube with 527 classes)
- tested on ESC-50 (same task but different domain) and DCASE2016 (task adaption)
- SLAT -{\textgreater} strong label assumption training (class correct for all patches)
- logmel spec as input, 44.1kHz, 128 mel bands, winsize 23ms, overlap 11.5
- global pooling at the end -{\textgreater} fully conv for variable length of input
- ESC-50: outperform sota by 9.3{\%}
- DCASE2016: absolute improvement of 4.1{\%}
- Audioset: sota results},
author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2018.8462200},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Khadkevich, Fugen - 2018 - Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events an(3).pdf:pdf},
isbn = {978-1-5386-4658-8},
issn = {15206149},
keywords = {Audio Event Classification,Learning Representations,Transfer Learning,Weak Label Learning,acmus,dnn better},
mendeley-tags = {acmus,dnn better},
month = {apr},
pages = {326--330},
publisher = {IEEE},
title = {{Knowledge Transfer from Weakly Labeled Audio Using Convolutional Neural Network for Sound Events and Scenes}},
xurl = {https://ieeexplore.ieee.org/document/8462200/},
year = {2018}
}
@article{Koizumi2019d,
abstract = {This paper introduces a new dataset called "ToyADMOS" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at https://github.com/YumaKoizumi/ToyADMOS-dataset},
archivePrefix = {arXiv},
arxivId = {1908.03299},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Harada, Noboru and Imoto, Keisuke},
eprint = {1908.03299},
file = {::},
number = {Waspaa},
pages = {2--7},
title = {{ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection}},
xurl = {http://arxiv.org/abs/1908.03299},
year = {2019}
}
@inproceedings{Alaoui-Belghiti2020,
author = {Alaoui-Belghiti, Amina and Chevallier, Sylvain and Monacelli, Eric and Bao, Guillaume and Azabou, Eric},
xdoi = {10.1109/icassp40776.2020.9053464},
file = {::},
month = {apr},
pages = {2997--3001},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Semi-Supervised Optimal Transport Methods for Detecting Anomalies}},
year = {2020}
}
@article{Koizumi2019c,
abstract = {Use of an autoencoder (AE) as a normal model is a state-of-the-art technique for unsupervised-anomaly detection in sounds (ADS). The AE is trained to minimize the sample mean of the anomaly score of normal sounds in a mini-batch. One problem with this approach is that the anomaly score of rare-normal sounds becomes higher than that of frequent-normal sounds, because the sample mean is strongly affected by frequent-normal samples, resulting in preferentially decreasing the anomaly score of frequent-normal samples. To decrease anomaly scores for both frequent- and rare-normal sounds, we propose batch uniformization, a training method for unsupervised-ADS for minimizing a weighted average of the anomaly score on each sample in a mini-batch. We used the reciprocal of the probabilistic density of each sample as the weight, more intuitively, a large weight is given for rare-normal sounds. Such a weight works to give a constant anomaly score for both frequent- and rare-normal sounds. Since the probabilistic density is unknown, we estimate it by using the kernel density estimation on each training mini-batch. Verification- and objective-experiments show that the proposed batch uniformization improves the performance of unsupervised-ADS.},
archivePrefix = {arXiv},
arxivId = {1907.08338},
author = {Koizumi, Yuma and Saito, Shoichiro and Yamaguchi, Masataka and Murata, Shin and Harada, Noboru},
eprint = {1907.08338},
file = {::},
month = {jul},
title = {{Batch Uniformization for Minimizing Maximum Anomaly Score of DNN-based Anomaly Detection in Sounds}},
xurl = {http://arxiv.org/abs/1907.08338},
year = {2019}
}
@inproceedings{Gararsky:2018:Xception:SMC,
author = {Gajarsky, Tomas and Purwins, Hendrik},
booktitle = {Proceedings of the Sound and Music Computing Conference (SMC)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gajarsky, Purwins - 2018 - An Xception Residual Recurrent Neural Network for Audio Event Detection and Tagging(3).pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {210--216},
title = {{An Xception Residual Recurrent Neural Network for Audio Event Detection and Tagging}},
year = {2018}
}
@article{Marchi2017,
abstract = {In the emerging field of acoustic novelty detection, most research efforts are devoted to probabilistic approaches such as mixture models or state-space models. Only recent studies introduced (pseudo-)generative models for acoustic novelty detection with recurrent neural networks in the form of an autoencoder. In these approaches, auditory spectral features of the next short term frame are predicted from the previous frames by means of Long-Short Term Memory recurrent denoising autoencoders. The reconstruction error between the input and the output of the autoencoder is used as activation signal to detect novel events. There is no evidence of studies focused on comparing previous efforts to automatically recognize novel events from audio signals and giving a broad and in depth evaluation of recurrent neural network-based autoencoders. The present contribution aims to consistently evaluate our recent novel approaches to fill this white spot in the literature and provide insight by extensive evaluations carried out on three databases: A3Novelty, PASCAL CHiME, and PROMETHEUS. Besides providing an extensive analysis of novel and state-of-the-art methods, the article shows how RNN-based autoencoders outperform statistical approaches up to an absolute improvement of 16.4{\%} average F-measure over the three databases.},
author = {Marchi, Erik and Vesperini, Fabio and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
xdoi = {10.1155/2017/4694860},
file = {::},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
title = {{Research article deep recurrent neural network-based autoencoders for acoustic novelty detection}},
volume = {2017},
year = {2017}
}
@article{Ahrens2019,
abstract = {In this paper we propose a novel machine-learning method for anomaly detection applicable to data with periodic characteristics where randomly varying period lengths are explicitly allowed. A multi-dimensional time series analysis is conducted by training a data-adapted classifier consisting of deep convolutional neural networks performing phase classification. The entire algorithm including data pre-processing, period detection, segmentation, and even dynamic adjustment of the neural networks is implemented for fully automatic execution. The proposed method is evaluated on three example datasets from the areas of cardiology, intrusion detection, and signal processing, presenting reasonable performance.},
author = {Ahrens, Lia and Ahrens, Julian and Schotten, Hans D.},
xdoi = {10.1186/s13634-019-0619-3},
file = {::},
issn = {16876180},
journal = {Eurasip Journal on Advances in Signal Processing},
keywords = {Anomaly detection,Convolutional neural networks,Machine learning,Phase classification,Time series analysis,anomaly,ima},
mendeley-tags = {anomaly,ima},
number = {1},
publisher = {EURASIP Journal on Advances in Signal Processing},
title = {{A machine-learning phase classification scheme for anomaly detection in signals with periodic characteristics}},
volume = {2019},
year = {2019}
}
@article{Purohit2019a,
abstract = {Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388},
archivePrefix = {arXiv},
arxivId = {1909.09347},
author = {Purohit, Harsh and Tanabe, Ryo and Ichige, Kenji and Endo, Takashi and Nikaido, Yuki and Suefusa, Kaori and Kawaguchi, Yohei},
eprint = {1909.09347},
file = {::},
month = {sep},
title = {{MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection}},
xurl = {http://arxiv.org/abs/1909.09347},
year = {2019}
}
@article{Principi2019,
abstract = {Fault diagnosis of electric motors is a fundamental task for production line testing, and it is usually performed by experienced human operators. In the recent years, several methods have been proposed in the literature for detecting faults automatically. Deep neural networks have been successfully employed for this task, but, up to the authors'knowledge, they have never been used in an unsupervised scenario. This paper proposes an unsupervised method for diagnosing faults of electric motors by using a novelty detection approach based on deep autoencoders. In the proposed method, vibration signals are acquired by using accelerometers and processed to extract Log-Mel coefficients as features. Autoencoders are trained by using normal data only, i.e., data that do not contain faults. Three different autoencoders architectures have been evaluated: the multi-layer perceptron (MLP) autoencoder, the convolutional neural network autoencoder, and the recurrent autoencoder composed of long short-term memory (LSTM) units. The experiments have been conducted by using a dataset created by the authors, and the proposed approaches have been compared to the one-class support vector machine (OC-SVM) algorithm. The performance has been evaluated in terms area under curve (AUC) of the receiver operating characteristic curve, and the results showed that all the autoencoder-based approaches outperform the OC-SVM algorithm. Moreover, the MLP autoencoder is the most performing architecture, achieving an AUC equal to 99.11{\%}.},
author = {Principi, Emanuele and Rossetti, Damiano and Squartini, Stefano and Piazza, Francesco},
xdoi = {10.1109/JAS.2019.1911393},
file = {::},
issn = {23299274},
journal = {IEEE/CAA Journal of Automatica Sinica},
keywords = {Autoencoder,convolutional neural networks,electric motor,fault detection,long short-term memory,neural networks,novelty detection},
number = {2},
pages = {441--451},
title = {{Unsupervised electric motor fault detection by using deep autoencoders}},
volume = {6},
year = {2019}
}
@article{Koizumi2019d,
abstract = {This paper introduces a new dataset called "ToyADMOS" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at https://github.com/YumaKoizumi/ToyADMOS-dataset},
archivePrefix = {arXiv},
arxivId = {1908.03299},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Harada, Noboru and Imoto, Keisuke},
eprint = {1908.03299},
file = {::},
number = {Waspaa},
pages = {2--7},
title = {{ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection}},
xurl = {http://arxiv.org/abs/1908.03299},
year = {2019}
}
@article{Koizumi2018,
abstract = {This paper proposes a novel optimization principle and its implementation for unsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The goal of unsupervised-ADS is to detect unknown anomalous sound without training data of anomalous sound. Use of an AE as a normal model is a state-of-the-art technique for unsupervised-ADS. To decrease the false positive rate (FPR), the AE is trained to minimize the reconstruction error of normal sounds and the anomaly score is calculated as the reconstruction error of the observed sound. Unfortunately, since this training procedure does not take into account the anomaly score for anomalous sounds, the true positive rate (TPR) does not necessarily increase. In this study, we define an objective function based on the Neyman-Pearson lemma by considering ADS as a statistical hypothesis test. The proposed objective function trains the AE to maximize the TPR under an arbitrary low FPR condition. To calculate the TPR in the objective function, we consider that the set of anomalous sounds is the complementary set of normal sounds and simulate anomalous sounds by using a rejection sampling algorithm. Through experiments using synthetic data, we found that the proposed method improved the performance measures of ADS under low FPR conditions. In addition, we confirmed that the proposed method could detect anomalous sounds in real environments.},
archivePrefix = {arXiv},
arxivId = {1810.09133},
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Kawachi, Yuta and Harada, Noboru},
xdoi = {10.1109/TASLP.2018.2877258},
eprint = {1810.09133},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Anomaly detection in sound,Feature extraction,Linear programming,Neyman-Pearson lemma,Probability density function,Speech processing,Task analysis,Training data,anomaly,autoencoder,deep learning,eusipco2019,ima,microphone},
mendeley-tags = {anomaly,eusipco2019,ima,microphone},
pages = {1--13},
title = {{Unsupervised Detection of Anomalous Sound based on Deep Learning and the Neyman-Pearson Lemma}},
year = {2018}
}
@article{Tran2017,
abstract = {We propose a method for video anomaly detection using a winner-take-all convolu-tional autoencoder that has recently been shown to give competitive results in learning for classification task. The method builds on state of the art approaches to anomaly detection using a convolutional autoencoder and a one-class SVM to build a model of normality. The key novelties are (1) using the motion-feature encoding extracted from a convolutional autoencoder as input to a one-class SVM rather than exploiting reconstruc-tion error of the convolutional autoencoder, and (2) introducing a spatial winner-take-all step after the final encoding layer during training to introduce a high degree of sparsity. We demonstrate an improvement in performance over the state of the art on UCSD and Avenue (CUHK) datasets.},
author = {Tran, Hanh T M and Hogg, D C},
file = {::},
journal = {Proceedings of the British Machine Vision Conference 2017},
keywords = {anomaly,ima},
mendeley-tags = {anomaly,ima},
title = {{Anomaly Detection using a Convolutional Winner-Take-All Autoencoder}},
year = {2017}
}
@article{Kawaguchi2017,
author = {Kawaguchi, Yohei and Endo, Takashi},
xdoi = {10.1109/MLSP.2017.8168164},
file = {::},
isbn = {9781509063413},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Autoencoder,End-to-end,Long short-term memory (LSTM),Non-uniform sampling,Sub-Nyquist sampling,anomaly,ima},
mendeley-tags = {anomaly,ima},
pages = {1--6},
title = {{How can we detect anomalies from subsampled audio signals?}},
volume = {2017-Septe},
year = {2017}
}
@article{Principi2019,
abstract = {Fault diagnosis of electric motors is a fundamental task for production line testing, and it is usually performed by experienced human operators. In the recent years, several methods have been proposed in the literature for detecting faults automatically. Deep neural networks have been successfully employed for this task, but, up to the authors'knowledge, they have never been used in an unsupervised scenario. This paper proposes an unsupervised method for diagnosing faults of electric motors by using a novelty detection approach based on deep autoencoders. In the proposed method, vibration signals are acquired by using accelerometers and processed to extract Log-Mel coefficients as features. Autoencoders are trained by using normal data only, i.e., data that do not contain faults. Three different autoencoders architectures have been evaluated: the multi-layer perceptron (MLP) autoencoder, the convolutional neural network autoencoder, and the recurrent autoencoder composed of long short-term memory (LSTM) units. The experiments have been conducted by using a dataset created by the authors, and the proposed approaches have been compared to the one-class support vector machine (OC-SVM) algorithm. The performance has been evaluated in terms area under curve (AUC) of the receiver operating characteristic curve, and the results showed that all the autoencoder-based approaches outperform the OC-SVM algorithm. Moreover, the MLP autoencoder is the most performing architecture, achieving an AUC equal to 99.11{\%}.},
author = {Principi, Emanuele and Rossetti, Damiano and Squartini, Stefano and Piazza, Francesco},
xdoi = {10.1109/JAS.2019.1911393},
file = {::},
issn = {23299274},
journal = {IEEE/CAA Journal of Automatica Sinica},
keywords = {Autoencoder,convolutional neural networks,electric motor,fault detection,long short-term memory,neural networks,novelty detection},
number = {2},
pages = {441--451},
title = {{Unsupervised electric motor fault detection by using deep autoencoders}},
volume = {6},
year = {2019}
}
@article{Rushe2019,
abstract = {Anomaly detection involves the recognition of patterns outside of what is considered normal, given a certain set of input data. This presents a unique set of challenges for machine learning, particularly if we assume a semi-supervised scenario in which anomalous patterns are unavailable at training time meaning algorithms must rely on non-anomalous data alone. Anomaly detection in time series adds an additional level of complexity given the contextual nature of anomalies. For time series modelling, autoregressive deep learning architectures such as WaveNet have proven to be powerful generative models, specifically in the field of speech synthesis. In this paper, we propose to extend the use of this type of architecture to anomaly detection in raw audio. In experiments using multiple audio datasets we compare the performance of this approach to a baseline autoencoder model and show superior performance in almost all cases.},
author = {Rushe, Ellen and Namee, Brian Mac},
xdoi = {10.1109/ICASSP.2019.8683414},
file = {::},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {WaveNet,anomaly detection,deep learning,raw audio},
pages = {3597--3601},
title = {{Anomaly Detection in Raw Audio Using Deep Autoregressive Networks}},
volume = {2019-May},
year = {2019}
}
@article{Akcay2019,
abstract = {Despite inherent ill-definition, anomaly detection is a research endeavour of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most significant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains at most a sub-set of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within image space and a lower-dimensional embedding vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.},
archivePrefix = {arXiv},
arxivId = {1901.08954},
author = {Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P.},
xdoi = {10.1109/IJCNN.2019.8851808},
eprint = {1901.08954},
file = {::},
isbn = {9781728119854},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Anomaly Detection,GANomaly,Generative Adversarial Networks,Skip Connections,X-ray Security Screening},
title = {{Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection}},
volume = {2019-July},
year = {2019}
}
@inproceedings{Pauwels2013,
abstract = {In this paper, we perform an in-depth evaluation of a large number of algorithms for chord estimation that have been submitted to the MIREX competitions in 2010, 2011 and 2012. Therefore we first present a rigorous scheme to describe evaluation methods in a sound, unambiguous way that extends previous work specifically to take into account the large variance in chord estimation vocabularies and to perform evaluations on select sets of chords. Then we take a look at the evaluation metrics used so far and propose some alternative ones. Finally, we use these different methods to get a deeper insight into the strengths of each of the competing algorithms and show that the choice of evaluation measure greatly influences the ranking. {\textcopyright} 2013 IEEE.},
author = {Pauwels, Johan and Peeters, Geoffroy},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
xdoi = {10.1109/ICASSP.2013.6637748},
file = {::},
isbn = {9781479903566},
issn = {15206149},
keywords = {chord estimation,evaluation procedure,large scale evaluation,music information retrieval},
month = {oct},
pages = {749--753},
title = {{Evaluating automatically estimated chord sequences}},
year = {2013}
}
@article{Turchet:2020:InternetOfAudioThings:IEEE_ITJ,
author = {Turchet, Luca and Fazekas, Gy{\"{o}}rgy and Lagrange, Mathieu and Ghadikolaei, Hossein S. and Fischione, Carlo},
xdoi = {10.1109/JIOT.2020.2997047},
file = {::},
journal = {IEEE Internet of Things Journal},
number = {May},
title = {{The Internet of Audio Things: state-of-the-art, vision, and challenges}},
year = {2020}
}
@inproceedings{Pedersoli:2020:UNetMPE:ICASSP,
address = {Barcelona, Spain},
author = {Pedersoli, Fabrizio and Tzanetakis, George and Yi, Kwang Moo},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/icassp40776.2020.9052987},
file = {:C$\backslash$:/Users/abr/Downloads/09052987.pdf:pdf},
isbn = {9781509066315},
pages = {506--510},
publisher = {IEEE},
title = {{Improving Music Transcription by Pre-Stacking A U-Net}},
year = {2020}
}
@inproceedings{Nguyen:2020:MismatchedDevices:ICASSP,
address = {Barcelona, Spain},
author = {Nguyen, Truc and Pernkopf, Franz and Kosmider, Michal},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/Downloads/icassp2020{\_}paper{\_}TrucNguyen{\_}ieee{\_}version.pdf:pdf},
pages = {126--130},
title = {{Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction}},
year = {2020}
}
@article{Won:2020:Tagging:ARXIV,
abstract = {Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.},
archivePrefix = {arXiv},
arxivId = {2006.00751},
author = {Won, Minz and Ferraro, Andres and Bogdanov, Dmitry and Serra, Xavier},
eprint = {2006.00751},
file = {:C$\backslash$:/Users/abr/Downloads/2006.00751.pdf:pdf},
journal = {Arxiv},
title = {{Evaluation of CNN-based Automatic Music Tagging Models}},
xurl = {http://arxiv.org/abs/2006.00751},
year = {2020}
}
@article{Pankajakshan:2020:SeqSelfAtt:ARXIV,
abstract = {In this paper we investigate the importance of the extent of memory in sequential self attention for sound recognition. We propose to use a memory controlled sequential self attention mechanism on top of a convolutional recurrent neural network (CRNN) model for polyphonic sound event detection (SED). Experiments on the URBAN-SED dataset demonstrate the impact of the extent of memory on sound recognition performance with the self attention induced SED model. We extend the proposed idea with a multi-head self attention mechanism where each attention head processes the audio embedding with explicit attention width values. The proposed use of memory controlled sequential self attention offers a way to induce relations among frames of sound event tokens. We show that our memory controlled self attention model achieves an event based F -score of 33.92{\%} on the URBAN-SED dataset, outperforming the F -score of 20.10{\%} reported by the model without self attention.},
archivePrefix = {arXiv},
arxivId = {2005.06650},
author = {Pankajakshan, Arjun and Bear, Helen L. and Subramanian, Vinod and Benetos, Emmanouil},
eprint = {2005.06650},
file = {:C$\backslash$:/Users/abr/Downloads/2005.06650.pdf:pdf},
title = {{Memory Controlled Sequential Self Attention for Sound Recognition}},
xurl = {http://arxiv.org/abs/2005.06650},
year = {2020}
}
@article{Tian:2020:FewShot:ARXIV,
abstract = {The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.},
archivePrefix = {arXiv},
arxivId = {2003.11539},
author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B. and Isola, Phillip},
eprint = {2003.11539},
file = {::},
title = {{Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?}},
xurl = {http://arxiv.org/abs/2003.11539},
year = {2020}
}
@article{McCallum:2019:Segmentation:ICASSP,
abstract = {Music segmentation refers to the dual problem of identifying boundaries between, and labeling, distinct music segments, e.g., the chorus, verse, bridge etc. in popular music. The performance of a range of music segmentation algorithms has been shown to be dependent on the audio features chosen to represent the audio. Some approaches have proposed learning feature transformations from music segment annotation data, although, such data is time consuming or expensive to create and as such these approaches are likely limited by the size of their datasets. While annotated music segmentation data is a scarce resource, the amount of available music audio is much greater. In the neighboring field of semantic audio unsupervised deep learning has shown promise in improving the performance of solutions to the query-by-example and sound classification tasks. In this work, unsupervised training of deep feature embeddings using convolutional neural networks (CNNs) is explored for music segmentation. The proposed techniques exploit only the time proximity of audio features that is implicit in any audio timeline. Employing these embeddings in a classic music segmentation algorithm is shown not only to significantly improve the performance of this algorithm, but obtain state of the art performance in unsupervised music segmentation.},
author = {Mccallum, Matthew C.},
xdoi = {10.1109/ICASSP.2019.8683407},
file = {:C$\backslash$:/Users/abr/Downloads/08683407.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Acoustic signal processing,Convolutional neural network,Deep learning,Music information retrieval},
pages = {346--350},
title = {{Unsupervised Learning of Deep Features for Music Segmentation}},
year = {2019}
}
@article{Wang:2019:RepresentationLearning:ARXIV,
abstract = {Convolutional neural networks (CNN) are one of the best-performing neural network architectures for environmental sound classification (ESC). Recently, attention mechanisms have been used in CNN to capture the useful information from the audio signal for sound classification, especially for weakly labelled data where the timing information about the acoustic events is not available in the training data, apart from the availability of sound class labels. In these methods, however, the inherent time-frequency characteristics and variations are not explicitly exploited when obtaining the deep features. In this paper, we propose a new method, called time-frequency enhancement block (TFBlock), which temporal attention and frequency attention are employed to enhance the features from relevant frames and frequency bands. Compared with other attention mechanisms, in our method, parallel branches are constructed which allow the temporal and frequency features to be attended respectively in order to mitigate interference from the sections where no sound events happened in the acoustic environments. The experiments on three benchmark ESC datasets show that our method improves the classification performance and also exhibits robustness to noise.},
archivePrefix = {arXiv},
arxivId = {1912.06808},
author = {Wang, Helin and Zou, Yuexian and Chong, Dading and Wang, Wenwu},
eprint = {1912.06808},
file = {:C$\backslash$:/Users/abr/Downloads/1912.06808v1 (1).pdf:pdf},
journal = {arXiv},
title = {{Learning discriminative and robust time-frequency representations for environmental sound classification}},
xurl = {http://arxiv.org/abs/1912.06808},
year = {2019}
}
@article{Salamon:2014:Melody:SPM,
author = {Salamon, Justin and G{\'{o}}mez, Emilia and Ellis, Daniel P.W. and Richard, Gael},
file = {::},
journal = {IEEE Signal Processing Magazine},
number = {2},
title = {{Melody Extraction from Polyphonic Music Signals : Approaches, Applications and Challenges}},
volume = {31},
year = {2014}
}
@inproceedings{Raffel:2014:mireval:ISMIR,
abstract = {Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir{\_}eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir{\_}eval and quantitatively compare each to existing implementations. When the scores reported by mir{\_}eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir{\_}eval's architecture, design, and intended use.},
address = {Taipei, Taiwan},
author = {Raffel, Colin and McFee, Brian and Humphrey, Eric J. and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel P.W.},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
pages = {367--372},
title = {{mir{\_}eval: A Transparent Implementation of Common MIR Metrics}},
year = {2014}
}
@article{Breimann:2001:RandomForest:ML,
author = {Breimann, Leo},
journal = {Machine Learning},
pages = {5--32},
title = {{Random Forests}},
volume = {45},
year = {2001}
}
@article{Salamon:2013:TonalRepresentations:JMIR,
abstract = {In this study we compare the use of different music representations for retrieving alternative performances of the same musical piece, a task commonly referred to as version identification. Given the audio signal of a song, we compute descriptors representing its melody, bass line and harmonic progression using state-of-the-art algorithms. These descriptors are then employed to retrieve different versions of the same musical piece using a dynamic programming algorithm based on nonlinear time series analysis. First, we evaluate the accuracy obtained using individual descriptors, and then we examine whether performance can be improved by combining these music representations (i.e. descriptor fusion). Our results show that whilst harmony is the most reliable music representation for version identification, the melody and bass line representations also carry useful information for this task. Furthermore, we show that by combining these tonal representations we can increase version detection accuracy. Finally, we demonstrate how the proposed version identification method can be adapted for the task of query-by-humming. We propose a melody-based retrieval approach, and demonstrate how melody representations extracted from recordings of a cappella singing can be successfully used to retrieve the original song from a collection of polyphonic audio. The current limitations of the proposed approach are discussed in the context of version identification and query-by-humming, and possible solutions and future research directions are proposed.},
author = {Salamon, Justin and Serr{\`{a}}, Joan and G{\'{o}}mez, Emilia},
xdoi = {10.1007/s13735-012-0026-0},
file = {::},
isbn = {1373501200},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Bass line,Cover song detection,Harmony,Melody extraction,Music retrieval,Music similarity,Query by humming,Version identification},
number = {1},
pages = {45--58},
title = {{Tonal representations for music retrieval: from version identification to query-by-humming}},
volume = {2},
year = {2013}
}
@misc{Peruskovic,
author = {Peruskovic, Barbara},
title = {{The hitchhiker´s guide to privacy by design}}
}
@article{Otto2018,
abstract = {Der Industrial Data Space bietet eine informationstechnische Architektur zur Wahrung der Datensouver{\"{a}}nit{\"{a}}t in Gesch{\"{a}}fts{\"{o}}kosystemen. Er stellt einen virtuellen Datenraum dar, bei denen die Daten beim Daten-Owner verbleiben, bis sie von einem vertrauensw{\"{u}}rdigen Gesch{\"{a}}ftspartner ben{\"{o}}tigt werden. Beim Datenaustausch k{\"{o}}nnen Nutzungsbedingungen mit den Daten selbst verkn{\"{u}}pft werden.},
author = {Otto, Boris and ten Hompel, Michael and Wrobel, Stefan},
xdoi = {10.1007/978-3-662-55890-4_8},
journal = {Digitalisierung},
pages = {113--133},
title = {{Industrial Data Space}},
year = {2018}
}
@inproceedings{Saki2019,
author = {Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu and Kim, Lae-Hoon and Deshpande, Manyu and Moon, Sunkuk and Koh, Eunjeong and Visser, Erik},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
number = {November},
pages = {219--223},
title = {{OPEN-SET EVOLVING ACOUSTIC SCENE CLASSIFICATION SYSTEM}},
year = {2019}
}
@article{Wu2019,
abstract = {At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized datacenter infrastructure, Facebook is working to bring machine learning inference to the edge. By xdoing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms.},
author = {Wu, Carole Jean and Brooks, David and Chen, Kevin and Chen, Douglas and Choudhury, Sy and Dukhan, Marat and Hazelwood, Kim and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leyvand, Tommer and Lu, Hao and Lu, Yang and Qiao, Lin and Reagen, Brandon and Spisak, Joe and Sun, Fei and Tulloch, Andrew and Vajda, Peter and Wang, Xiaodong and Wang, Yanghan and Wasti, Bram and Wu, Yiming and Xian, Ran and Yoo, Sungjoo and Zhang, Peizhao},
xdoi = {10.1109/HPCA.2019.00048},
isbn = {9781728114446},
journal = {Proceedings - 25th IEEE International Symposium on High Performance Computer Architecture, HPCA 2019},
keywords = {Edge Inference,Machine learning},
pages = {331--344},
publisher = {IEEE},
title = {{Machine learning at facebook: Understanding inference at the edge}},
year = {2019}
}
@article{Oramas2017,
abstract = {An increasing amount of digital music is being published daily. Music streaming services often ingest all available music, but this poses a challenge: how to recommend new artists for which prior knowledge is scarce? In this work we aim to address this so-called cold-start problem by combining text and audio information with user feedback data using deep network architectures. Our method is divided into three steps. First, artist embeddings are learned from biographies by combining semantics, text features, and aggregated usage data. Second, track embeddings are learned from the audio signal and available feedback data. Finally, artist and track embeddings are combined in a multimodal network. Results suggest that both splitting the recommendation problem between feature levels (i.e., artist metadata and audio track), and merging feature embeddings in a multimodal approach improve the accuracy of the recommendations.},
archivePrefix = {arXiv},
arxivId = {1706.09739},
author = {Oramas, Sergio and Nieto, Oriol and Sordo, Mohamed and Serra, Xavier},
xdoi = {10.1145/3125486.3125492},
eprint = {1706.09739},
keywords = {deep learning,multimodal,music,recommender systems,semantics},
title = {{A Deep Multimodal Approach for Cold-start Music Recommendation}},
xurl = {http://arxiv.org/abs/1706.09739{\%}0Ahttp://dx.xdoi.org/10.1145/3125486.3125492},
year = {2017}
}
@article{Lopez-Ballester:2020:Annoyance:AS,
abstract = {Psycho-acoustic parameters have been extensively used to evaluate the discomfort or pleasure produced by the sounds in our environment. In this context, wireless acoustic sensor networks (WASNs) can be an interesting solution for monitoring subjective annoyance in certain soundscapes, since they can be used to register the evolution of such parameters in time and space. Unfortunately, the calculation of the psycho-acoustic parameters involved in common annoyance models implies a significant computational cost, and makes difficult the acquisition and transmission of these parameters at the nodes. As a result, monitoring psycho-acoustic annoyance becomes an expensive and inefficient task. This paper proposes the use of a deep convolutional neural network (CNN) trained on a large urban sound dataset capable of efficiently predicting psycho-acoustic annoyance from raw audio signals continuously. We evaluate the proposed regression model and compare the resulting computation times with the ones obtained by the conventional direct calculation approach. The results confirm that the proposed model based on CNN achieves high precision in predicting psycho-acoustic annoyance, predicting annoyance values with an average quadratic error of around 3{\%}. It also achieves a very significant reduction in processing time, which is up to 300 times faster than direct calculation, making CNN designed a clear exponent to work in IoT devices.},
author = {Lopez-Ballester, Jesus and Pastor-Aparicio, Adolfo and Segura-Garcia, Jaume and Felici-Castell, Santiago and Cobos, Maximo},
xdoi = {10.3390/app9153136},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez-Ballester et al. - 2019 - Computation of psycho-acoustic annoyance using deep neural networks.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences},
keywords = {Convolutional neural networks,Psycho-acoustic parameters,Subjective annoyance,Wireless acoustic sensor networks,Zwicker model},
number = {15},
title = {{Computation of psycho-acoustic annoyance using deep neural networks}},
volume = {9},
year = {2019}
}
@article{Morales:2011:RL:BOOK,
abstract = {This chapter provides a concise introduction to Reinforcement Learning (RL) from a machine learning perspective. It provides the required background to understand the chapters related to RL in this book. It makes no assumption on previous knowledge in this research area and includes short descriptions of some of the latest trends, which are normally excluded from other introductions or overviews on RL. The chapter provides more emphasis on the general conceptual framework and ideas of RL rather than on presenting a rigorous mathematical discussion that may require a great deal of effort by the reader. The first section provides a general introduction to the area. The following section describes the most common solution techniques. In the third section, some of the most recent techniques proposed to deal with large search spaces are described. Finally, the last section provides some final remarks and current research challenges in RL. {\textcopyright} 2012, IGI Global.},
author = {Morales, Eduardo F. and Zaragoza, Julio H.},
xdoi = {10.4018/978-1-60960-165-2.ch004},
file = {::},
isbn = {9781609601652},
journal = {Decision Theory Models for Applications in Artificial Intelligence: Concepts and Solutions},
pages = {63--80},
title = {{An introduction to reinforcement learning}},
year = {2011}
}
@inproceedings{Goto:2002:RWC:ISMIR,
address = {Paris, France},
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goto, Nishimura - 2002 - {\{}RWC{\}} music database Popular, classical, and jazz music databases.pdf:pdf},
pages = {287--288},
title = {{RWC Music Database: Popular, Classical, and Jazz Music Databases}},
year = {2002}
}
@inproceedings{Jansson:2017:DeepUNet:ISMIR,
address = {Suzhou, China},
author = {Jansson, Andreas and Humphrey, Eric and Montecchio, Nicola and Bittner, Rachel and Kumar, Aparna and Weyde, Tillman},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
title = {{Singing Voice Separation with Deep U-Net CNN}},
year = {2017}
}
@inproceedings{Wu:2019:Transcription:ICASSP,
abstract = {The multi-instrument transcription task refers to joint recognition of instrument and pitch of every event in polyphonic music signals generated by one or more classes of music instruments. In this paper, we leverage multi-object semantic segmentation techniques to solve this problem. We design a time-frequency representation, which has multiple channels to jointly represent the harmonic structure and pitch saliency of a pitch activation. The transcription task therefore becomes a pixel-wise multi-task classification problem including pitch activity detection and instrument recognition. Experiments on both single- and multi-instrument data verify the competitiveness of the proposed method.},
address = {Brighton, UK},
author = {Wu, Yu Te and Chen, Berlin and Su, Li},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/Downloads/08682605.pdf:pdf},
keywords = {Automatic music transcription,multipitch estimation,semantic segmentation.},
pages = {166--170},
title = {{Polyphonic Music Transcription with Semantic Segmentation}},
year = {2019}
}
@inproceedings{Stoller:2018:WaveUNet:ISMIR,
abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {1806.03185},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {1806.03185},
file = {::},
isbn = {9782954035123},
pages = {334--340},
title = {{Wave-U-Net: A multi-scale neural network for end-to-end audio source separation}},
year = {2018}
}
@article{Ravanelli:2019:MutualInformation:INTERSPEECH,
abstract = {Learning good representations is of crucial importance in deep learning. Mutual Information (MI) or similar measures of statistical dependence are promising tools for learning these representations in an unsupervised way. Even though the mutual information between two random variables is hard to measure directly in high dimensional spaces, some recent studies have shown that an implicit optimization of MI can be achieved with an encoder-discriminator architecture similar to that of Generative Adversarial Networks (GANs). In this work, we learn representations that capture speaker identities by maximizing the mutual information between the encoded representations of chunks of speech randomly sampled from the same sentence. The proposed encoder relies on the SincNet architecture and transforms raw speech waveform into a compact feature vector. The discriminator is fed by either positive samples (of the joint distribution of encoded chunks) or negative samples (from the product of the marginals) and is trained to separate them. We report experiments showing that this approach effectively learns useful speaker representations, leading to promising results on speaker identification and verification tasks. Our experiments consider both unsupervised and semi-supervised settings and compare the performance achieved with different objective functions.},
archivePrefix = {arXiv},
arxivId = {1812.00271},
author = {Ravanelli, Mirco and Bengio, Yoshua},
xdoi = {10.21437/Interspeech.2019-2380},
eprint = {1812.00271},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravanelli, Bengio - 2019 - Learning speaker representations with mutual information(2).pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Deep Learning,Mutual Information,SincNet,Speaker Recognition,Unsupervised Learning},
pages = {1153--1157},
title = {{Learning speaker representations with mutual information}},
volume = {2019-Septe},
year = {2019}
}
@article{Chollet:2019:Intelligence:ARXIV,
abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
archivePrefix = {arXiv},
arxivId = {1911.01547},
author = {Chollet, Fran{\c{c}}ois},
eprint = {1911.01547},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chollet - 2019 - On the Measure of Intelligence(2).pdf:pdf},
pages = {1--64},
title = {{On the Measure of Intelligence}},
xurl = {http://arxiv.org/abs/1911.01547},
year = {2019}
}
@inproceedings{Inoue:2019:SufflingMixing:DCASE,
address = {New York, NY, USA},
author = {Inoue, Tadanobu and Vinayavekhin, Phongtharin and Wang, Shiqiang and Wood, David and Munawar, Asim and Ko, Bong Jun and Greco, Nancy and Tachibana, Ryuki},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.33682/wgyb-bt40},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue et al. - 2019 - Shuffling and Mixing Data Augmentation for Environmental Sound Classification(2).pdf:pdf},
pages = {109--113},
title = {{Shuffling and Mixing Data Augmentation for Environmental Sound Classification}},
year = {2019}
}
@techreport{Addepalli:2019:SelfSupervisedML:REPORT,
author = {Addepalli, B V Nithish and Khilnani, Jatin and Choudhary, Ravi and Chandrakaladharan, Shreyas and Lostanlen, Vincent and Mcfee, Brian},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Addepalli et al. - 2019 - Self-Supervised Machine Listening Fall 2019 Capstone Report(2).pdf:pdf},
title = {{Self-Supervised Machine Listening Fall 2019 Capstone Report}},
year = {2019}
}
@article{Phan:2019:ASC:INTERSPEECH,
abstract = {Acoustic scenes are rich and redundant in their content. In this work, we present a spatio-temporal attention pooling layer coupled with a convolutional recurrent neural network to learn from patterns that are discriminative while suppressing those that are irrelevant for acoustic scene classification. The convolutional layers in this network learn invariant features from time-frequency input. The bidirectional recurrent layers are then able to encode the temporal dynamics of the resulting convolutional features. Afterwards, a two-dimensional attention mask is formed via the outer product of the spatial and temporal attention vectors learned from two designated attention layers to weigh and pool the recurrent output into a final feature vector for classification. The network is trained with between-class examples generated from between-class data augmentation. Experiments demonstrate that the proposed method not only outperforms a strong convolutional neural network baseline but also sets new state-of-the-art performance on the LITIS Rouen dataset.},
archivePrefix = {arXiv},
arxivId = {1904.03543},
author = {Phan, Huy and Ch{\'{e}}n, Oliver Y. and Pham, Lam and Koch, Philipp and {De Vos}, Maarten and McLoughlin, Ian and Mertins, Alfred},
xdoi = {10.21437/Interspeech.2019-3040},
eprint = {1904.03543},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Phan et al. - 2019 - Spatio-temporal attention pooling for audio scene classification(2).pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Attention pooling,Audio scene classification,Convolutional neural network,Recurrent neural network},
number = {April},
pages = {3845--3849},
title = {{Spatio-temporal attention pooling for audio scene classification}},
volume = {2019-Septe},
year = {2019}
}
@article{Thakur:2019:DynamicTripletLoss:JASA,
abstract = {This paper proposes multiscale convolutional neural network (CNN)-based deep metric learning for bioacoustic classification, under low training data conditions. The proposed CNN is characterized by the utilization of four different filter sizes at each level to analyze input feature maps. This multiscale nature helps in describing different bioacoustic events effectively: smaller filters help in learning the finer details of bioacoustic events, whereas, larger filters help in analyzing a larger context leading to global details. A dynamic triplet loss is employed in the proposed CNN architecture to learn a transformation from the input space to the embedding space, where classification is performed. The triplet loss helps in learning this transformation by analyzing three examples, referred to as triplets, at a time where intra-class distance is minimized while maximizing the inter-class separation by a dynamically increasing margin. The number of possible triplets increases cubically with the dataset size, making triplet loss more suitable than the softmax cross-entropy loss in low training data conditions. Experiments on three different publicly available datasets show that the proposed framework performs better than existing bioacoustic classification frameworks. Experimental results also confirm the superiority of the triplet loss over the cross-entropy loss in low training data conditions},
archivePrefix = {arXiv},
arxivId = {1903.10713},
author = {Thakur, Anshul and Thapar, Daksh and Rajan, Padmanabhan and Nigam, Aditya},
xdoi = {10.1121/1.5118245},
eprint = {1903.10713},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thakur et al. - 2019 - Deep metric learning for bioacoustic classification Overcoming training data scarcity using dynamic triplet lo(2).pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {534--547},
publisher = {Acoustical Society of America},
title = {{Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss}},
volume = {146},
year = {2019}
}
@inproceedings{Oswald:2020:ContinualLearning:ICLR,
abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.},
archivePrefix = {arXiv},
arxivId = {1906.00695},
author = {von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~{a}}o and Grewe, Benjamin F.},
booktitle = {Proceedings of the ICLR},
eprint = {1906.00695},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/von Oswald et al. - 2020 - Continual learning with hypernetworks(2).pdf:pdf},
pages = {1--28},
title = {{Continual learning with hypernetworks}},
xurl = {http://arxiv.org/abs/1906.00695},
year = {2020}
}
@article{Xu:2019:AudioTagging:JASA,
abstract = {Audio tagging aims to infer descriptive labels from audio clips. Audio tagging is challenging due to the limited size of data and noisy labels. In this paper, we describe our solution for the DCASE 2018 Task 2 general audio tagging challenge. The contributions of our solution include: We investigated a variety of convolutional neural network architectures to solve the audio tagging task. Statistical features are applied to capture statistical patterns of audio features to improve the classification performance. Ensemble learning is applied to ensemble the outputs from the deep classifiers to utilize complementary information. a sample re-weight strategy is employed for ensemble training to address the noisy label problem. Our system achieves a mean average precision (mAP@3) of 0.958, outperforming the baseline system of 0.704. Our system ranked the 1st and 4th out of 558 submissions in the public and private leaderboard of DCASE 2018 Task 2 challenge. Our codes are available at https://github.com/Cocoxili/DCASE2018Task2/.},
archivePrefix = {arXiv},
arxivId = {1810.12832},
author = {Xu, Kele and Zhu, Boqing and Kong, Qiuqiang and Mi, Haibo and Ding, Bo and Wang, Dezhi and Wang, Huaimin},
xdoi = {10.1121/1.5111059},
eprint = {1810.12832},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {EL521--EL527},
title = {{General audio tagging with ensembling convolutional neural networks and statistical features}},
volume = {145},
year = {2019}
}
@article{Abesser:2020:ASC:AS,
author = {Abe{\ss}er, Jakob},
xdoi = {10.3390/app10062020},
file = {:C$\backslash$:/Users/abr/Downloads/applsci-10-02020.pdf:pdf},
journal = {applied sciences},
keywords = {abt-md,acoustic scene classification,deep neural networks,idmt,machine listening},
mendeley-tags = {abt-md,acoustic scene classification,idmt,machine listening},
number = {6},A Survey: Neural Network-Based Deep
title = {{A Review of Deep Learning Based Methods for Acoustic Scene Classification}},
volume = {10},
year = {2020}
}
@article{Pham2020,
abstract = {This article proposes an encoder-decoder network model for Acoustic Scene Classification (ASC), the task of identifying the scene of an audio recording from its acoustic signature. We make use of multiple low-level spectrogram features at the front-end, transformed into higher level features through a well-trained CNN-DNN front-end encoder. The high level features and their combination (via a trained feature combiner) are then fed into different decoder models comprising random forest regression, DNNs and a mixture of experts, for back-end classification. We report extensive experiments to evaluate the accuracy of this framework for various ASC datasets, including LITIS Rouen and IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 Task 1, 2017 Task 1, 2018 Tasks 1A {\&} 1B and 2019 Tasks 1A {\&} 1B. The experimental results highlight two main contributions; the first is an effective method for high-level feature extraction from multi-spectrogram input via the novel C-DNN architecture encoder network, and the second is the proposed decoder which enables the framework to achieve competitive results on various datasets. The fact that a single framework is highly competitive for several different challenges is an indicator of its robustness for performing general ASC tasks.},
archivePrefix = {arXiv},
arxivId = {2002.04502},
author = {Pham, Lam and Phan, Huy and Nguyen, Truc and Palaniappan, Ramaswamy and Mertins, Alfred and McLoughlin, Ian},
eprint = {2002.04502},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham et al. - 2020 - Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework.pdf:pdf},
month = {feb},
title = {{Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework}},
xurl = {http://arxiv.org/abs/2002.04502},
year = {2020}
}
@article{Zeghidour2020d,
abstract = {We introduce Wavesplit, an end-to-end speech separation system. From a single recording of mixed speech, the model infers and clusters representations of each speaker and then estimates each source signal conditioned on the inferred representations. The model is trained on the raw waveform to jointly perform the two tasks. Our model infers a set of speaker representations through clustering, which addresses the fundamental permutation problem of speech separation. Moreover, the sequence-wide speaker representations provide a more robust separation of long, challenging sequences, compared to previous approaches. We show that Wavesplit outperforms the previous state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2mix, WSJ0-3mix), as well as in noisy (WHAM!) and reverberated (WHAMR!) conditions. As an additional contribution, we further improve our model by introducing online data augmentation for separation.},
archivePrefix = {arXiv},
arxivId = {2002.08933},
author = {Zeghidour, Neil and Grangier, David},
eprint = {2002.08933},
month = {feb},
title = {{Wavesplit: End-to-End Speech Separation by Speaker Clustering}},
xurl = {http://arxiv.org/abs/2002.08933},
year = {2020}
}
@article{Papadopoulos2012,
abstract = {In this paper, we present a method for estimating the progression of musical key from an audio signal. We address the problem of local key finding by investigating the possible combination and extension of different previously proposed approaches for global key estimation. In this work, key progression is estimated from the chord progression. Specifically, we introduce key dependency on the harmonic and the metrical structures. A contribution of our work is that we address the problem of finding an analysis window length for local key estimation that is adapted to the intrinsic music content of the analyzed piece by introducing information related to the metrical structure in our model. Key estimation is not performed on empirically chosen segments but on segments that are expressed in relationship with the tempo period. We evaluate and analyze our results on two databases of different styles. We systematically analyze the influence of various parameters to determine factors important to our model, we study the relationships between the various musical attributes that are taken into account in our work, and we provide case study examples. {\textcopyright} 2011 IEEE.},
author = {Papadopoulos, H{\'{e}}l{\`{e}}ne and Peeters, Geoffroy},
xdoi = {10.1109/TASL.2011.2175385},
file = {::},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Acoustic signal analysis,chord,chroma,downbeat,hidden Markov model (HMM),information retrieval,key,music analysis},
number = {4},
pages = {1297--1312},
title = {{Local key estimation from an audio signal relying on harmonic and metrical structures}},
volume = {20},
year = {2012}
}
@techreport{Papadopoulos,
abstract = {In this paper, we present a method for estimating the local keys of an audio signal. We propose to address the problem of local key finding by investigating the possible combination and extension of different previous proposed global key estimation approaches. The specificity of our approach is that we introduce key dependency on the harmonic and the metric structures. In this work, we focus on the relationship between the chord progression and the local key progression in a piece of music. A contribution of our work is that we address the problem of finding a good analysis window length for local key estimation by introducing information related to the metric structure in our model. Key estimation is not performed on empirical-chosen segment length but on segments that are adapted to the analyzed piece and independent from the tempo. We evaluate and analyze our results on a new database composed of classical music pieces.},
author = {Papadopoulos, H{\'{e}}l{\`{e}}ne and Peeters, Geoffroy},
file = {::},
title = {{LOCAL KEY ESTIMATION BASED ON HARMONIC AND METRIC STRUCTURES}}
}
@techreport{Bello,
abstract = {When considering the problem of audio-to-audio matching , determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and poly-phonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research.},
author = {Bello, Juan P and Pickens, Jeremy},
file = {::},
keywords = {Harmonic description,music similarity,segmentation},
title = {{A Robust Mid-level Representation for Harmonic Content in Music Signals}}
}
@techreport{Harte,
abstract = {In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, context-independent syntax for representing chord symbols in text, supported with a new database of annotations using this system.},
author = {Harte, Christopher and Sandler, Mark and Abdallah, Samer and G{\'{o}}mez, Emilia},
file = {::},
keywords = {Anno-tation,Chords,Harmony,Music,Notation},
title = {{SYMBOLIC REPRESENTATION OF MUSICAL CHORDS: A PROPOSED SYNTAX FOR TEXT ANNOTATIONS}}
}
@inproceedings{Dang:2017:SurveyAED:ICOT,
abstract = {Deep learning has achieved state of the art in various machine learning problems, such as computer vision, speech recognition, and natural language processing. Sound event detection (SED), which is about recognizing audio events in real-life environments, has attracted a lot of attention recently. Many works have been successful when applying deep learning techniques for the SED problem as can be seen in Detection and Classification of Acoustic Scenes and Events (DCASE) challenge 2016-2017. In this paper, we present a review of the SED problem and discuss different deep learning approaches for the problem.},
address = {Singapore, Singapore},
author = {Dang, An and Vu, Toan H. and Wang, Jia Ching},
booktitle = {Proceedings of the International Conference on Orange Technologies (ICOT)},
xdoi = {10.1109/ICOT.2017.8336092},
file = {:C$\backslash$:/Users/abr/Downloads/08336092.pdf:pdf},
keywords = {Convolutional neural networks,Deep learning,Neural networks,Recurrent neural networks,Sound event detection},
pages = {75--78},
title = {{A survey of Deep Learning for Polyphonic Sound Event Detection}},
year = {2017}
}
@article{Barchiesi:2015:ASC:SPM,
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {Barchiesi, Daniele and Giannoulis, D. Dimitrios and Stowell, Dan and Plumbley, Mark D.},
xdoi = {10.1109/MSP.2014.2326181},
file = {:C$\backslash$:/Users/abr/Downloads/BarchiesiGiannoulisStowellP15-asc{\_}accepted.pdf:pdf},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
number = {3},
pages = {16--34},
title = {{Acoustic Scene Classification: Classifying environments from the sounds they produce}},
volume = {32},
year = {2015}
}
@inproceedings{Ducher:2019:FoldedCQT:ISMIR,
address = {Delft, The Netherlands},
author = {Ducher, Jean-Fran{\c{c}}ois and Esling, Philippe},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ducher, Esling - 2019 - Folded CQT RCNN for Real-Time Recognition of.pdf:pdf},
pages = {708--714},
title = {{Folded CQT RCNN for Real-Time Recognition of}},
xurl = {http://archives.ismir.net/ismir2019/paper/000086.pdf},
year = {2019}
}
@article{Ronneberger:2015:UNET:LNCS,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
xdoi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-net Convolutional networks for biomedical image segmentation.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
journal = {Lecture Notes in Computer Science},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@article{Lu:2018:UNET:ISMIR,
abstract = {The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets.},
author = {Lu, Wei Tsung and Su, Li},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Su - 2018 - Vocal melody extraction with semantic segmentation and audio-symbolic domain transfer learning.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
pages = {521--528},
title = {{Vocal melody extraction with semantic segmentation and audio-symbolic domain transfer learning}},
year = {2018}
}
@inproceedings{Hsieh:2019:Melody:ICASSP,
abstract = {Melody extraction in polyphonic musical audio is important for music signal processing. In this paper, we propose a novel streamlined encoder/decoder network that is designed for the task. We make two technical contributions. First, drawing inspiration from a state-of-the-art model for semantic pixel-wise segmentation, we pass through the pooling indices between pooling and un-pooling layers to localize the melody in frequency. We can achieve result close to the state-of-the-art with much fewer convolutional layers and simpler convolution modules. Second, we propose a way to use the bottleneck layer of the network to estimate the existence of a melody line for each time frame, and make it possible to use a simple argmax function instead of ad-hoc thresholding to get the final estimation of the melody line. Our experiments on both vocal melody extraction and general melody extraction validate the effectiveness of the proposed model.},
address = {Brighton, UK},
archivePrefix = {arXiv},
arxivId = {1810.12947},
author = {Hsieh, Tsung Han and Su, Li and Yang, Yi Hsuan},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
xdoi = {10.1109/ICASSP.2019.8682389},
eprint = {1810.12947},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh, Su, Yang - 2019 - A Streamlined Encoderdecoder Architecture for Melody Extraction(2).pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {Melody extraction,encoder/decoder},
pages = {156--160},
title = {{A Streamlined Encoder/Decoder Architecture for Melody Extraction}},
volume = {2019-May},
year = {2019}
}
@article{Salamon:2017:ASC:SPL,
abstract = {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a 'shallow' dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
archivePrefix = {arXiv},
arxivId = {1608.04363},
author = {Salamon, Justin and Bello, Juan Pablo},
xdoi = {10.1109/LSP.2017.2657381},
eprint = {1608.04363},
file = {::},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset},
number = {3},
pages = {279--283},
title = {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
volume = {24},
year = {2017}
}
@techreport{Jiang,
abstract = {While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., ma-jor/minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training. In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary. Large-vocabulary chord transcription is a difficult task, as the number of chord qualities is large, and the distribution of training chord classes is extremely biased. For example, the Billboard dataset [2], a human-annotated dataset, contains 230 different chord qualities, or equivalently, 2,749 distinct chord classes 1. While the first 10{\%} chord qualities cover 93.86{\%} of the data, the last 50{\%} chord qualities only cover 0.35{\%} of the data altogether 2. Such a long-tailed chord distribution makes it extremely hard to model rare chord qualities. To bypass the problem, former systems typically adopt two kinds of strategies: chord quality simplification and 1 We here assume that each chord quality can be combined with all possible 12 roots except for the N chord. 2 In calculation, the chord quality counts are weighted by their durations .},
author = {Jiang, Junyan and Chen, Ke and Li, Wei and Xia, Gus},
file = {::},
title = {{LARGE-VOCABULARY CHORD TRANSCRIPTION VIA CHORD STRUCTURE DECOMPOSITION}}
}
@techreport{Lee,
abstract = {Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and query-by-singer in both the in-domain and cross-domain tasks.},
author = {Lee, Kyungyun and Nam, Juhan},
file = {::},
title = {{LEARNING A JOINT EMBEDDING SPACE OF MONOPHONIC AND MIXED MUSIC SIGNALS FOR SINGING VOICE}},
xurl = {http://github.com/kyungyunlee/mono2mixed-singer}
}
@techreport{Zahray,
abstract = {This paper presents a novel approach to recurrent neural network (RNN)-based beat and downbeat tracking that simultaneously performs chord recognition as an assisting task to improve performance. RNNs have been used successfully, but independently, for these two tasks. Since downbeat detection is still a challenging problem and chords are likely to change on downbeats, we propose two architectures of multi-task learning sharing the same latent representations of beats, downbeats, and chords. One is a Y-shaped architecture that decodes in parallel from the shared representations, and the other is a cascade architecture that decodes in a sequentially-dependent manner. Our comparative experiments using 85 popular songs with the time signature 4/4 from the RWC Music database showed that the F-measures of beat and down-beat detection are significantly improved on the same feature set due to the addition of simultaneous chord recognition. The Y-shaped architecture achieved the best results with a beat F-measure of 0.901 and a downbeat F-measure of 0.866, which are comparable with the current state-of-the-art.},
author = {Zahray, Lisa and Nakamura, Eita and Yoshii, Kazuyoshi},
file = {::},
keywords = {Index Terms-Beat and downbeat detection,chord recognition,multi-task learning,recurrent neural network},
title = {{BEAT AND DOWNBEAT DETECTION WITH CHORD RECOGNITION BASED ON MULTI-TASK LEARNING OF RECURRENT NEURAL NETWORKS}}
}
@inproceedings{Lassseck:2018:Bird:DCASE,
abstract = {This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 $\backslash$$\backslash${\%} on the public challenge leaderboard.},
address = {Surrey, UK},
author = {Lasseck, Mario},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE)},
file = {:C$\backslash$:/Users/abr/Downloads/DCASE2018{\_}Lasseck{\_}76 (1).pdf:pdf},
keywords = {Bird Detection,Data Augmentation,Deep Convolutional Neural Networks,Deep Learning,bird{\_}recognition,machine{\_}listening},
mendeley-tags = {bird{\_}recognition,machine{\_}listening},
pages = {143--147},
title = {{Acoustic bird detection with deep convolutional neural networks}},
year = {2018}
}
@article{Han2017,
abstract = {Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In xdoing so, we investigated two different aggregation methods: one takes the class-wise average followed by normalization, and the other perform temporally local class-wise max-pooling on the output probability prior to averaging and normalization steps to minimize the effect of averaging process suppresses the activation of sporadically appearing instruments. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Our analysis on the instrument-wise performance found that the onset type is a critical factor for recall and precision of each instrument. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.619 for micro and 0.513 for macro, respectively, achieving 23.1{\%} and 18.8{\%} in performance improvement compared with the state-of-the-art algorithm.},
archivePrefix = {arXiv},
arxivId = {1605.09507},
author = {Han, Yoonchang and Kim, Jaehun and Lee, Kyogu},
xdoi = {10.1109/TASLP.2016.2632307},
eprint = {1605.09507},
file = {:C$\backslash$:/Users/abr/Downloads/1605.09507.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Convolutional neural networks,deep learning,instrument recognition,multi-layer neural network,music information retrieval},
number = {1},
pages = {208--221},
title = {{Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music}},
volume = {25},
year = {2017}
}
@article{Ruder:2017:MultitaskLearning:ARXIV,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {::},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
xurl = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@phdthesis{Caruana:1997:MultitaskLearning:THESIS,
author = {Caruana, Rich},
xdoi = {10.1007/978-1-4899-7687-1_100322},
file = {::},
school = {Carnegie Mellon University},
title = {{Multitask Learning}},
type = {PhD thesis},
year = {1997}
}
@article{Bach:2015:LRP:PLOS,
abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest.We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
xdoi = {10.1371/journal.pone.0130140},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--46},
pmid = {26161953},
title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
volume = {10},
year = {2015}
}
@article{Russakovsky:2015:ImageNet:IJCV,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
xdoi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {::},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@inproceedings{Gemmeke:2017:Audioset:ICASSP,
address = {New Orleans, LA, USA},
author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
pages = {776--780},
title = {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
year = {2017}
}
@inproceedings{Abesser:2019:ICASSP,
author = {Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {::},
title = {{Fundamental Frequency Contour Classification: A Comparison between Hand-Crafted and CNN-Based Features}},
year = {2019}
}
@inproceedings{Abesser:2019:Stadtlaerm:DCASE,
address = {New York, NY, USA},
author = {Abe{\ss}er, Jakob and G{\"{o}}tze, Marco and Clau{\ss}, Tobias and Zapf, Dominik and K{\"{u}}hn, Christian and Lukashevich, Hanna and K{\"{u}}hnlenz, Stephanie and Mimilakis, Stylianos},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - 2019 - URBAN NOISE MONITORING IN THE STADTL ARM PROJECT - A FIELD REPORT Fraunhofer Institute for Digital Media Technolo.pdf:pdf},
keywords = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening,smart{\_}city},
title = {{Urban Noise Monitoring in the Stadtl{\"{a}}rm Project - A Field Report}},
year = {2019}
}
@article{Sigtia:2016:PerformanceCost:IEEE_TASLP,
abstract = {In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.},
author = {Sigtia, Siddharth and Stark, Adam M. and Krstulovi{\'{c}}, Sacha and Plumbley, Mark D.},
xdoi = {10.1109/TASLP.2016.2592698},
file = {:C$\backslash$:/Users/abr/Downloads/07515194.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic environmental sound recognition,computational auditory scene analysis,deep learning,machine learning,machine{\_}listening},
mendeley-tags = {machine{\_}listening},
number = {11},
pages = {2096--2107},
publisher = {IEEE},
title = {{Automatic Environmental Sound Recognition: Performance Versus Computational Cost}},
volume = {24},
year = {2016}
}
@article{Bello:2018:SONYC:CACM,
abstract = {We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs. SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment. SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.},
archivePrefix = {arXiv},
arxivId = {1805.00889},
author = {Bello, Juan Pablo and Silva, Claudio and Nov, Oded and DuBois, R. Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish},
eprint = {1805.00889},
file = {::},
isbn = {1234567245},
journal = {Communications of the ACM (CACM)},
keywords = {acm reference format,citizen science,cyber-physical systems,machine listening,noise pollution,sensor networks,smart cities,visualization},
number = {2},
title = {{SONYC: A System for the Monitoring, Analysis and Mitigation of Urban Noise Pollution}},
xurl = {http://arxiv.org/abs/1805.00889},
volume = {62},
year = {2018}
}
@inproceedings{Abesser:2018:Stadtlaerm:FICLOUD,
abstract = {Smart city applications for acoustic monitoring become essential to cope with the overall increasing noise pollution in urban environments. This paper gives an overview over a distributed sensor network for noise monitoring in the German city of Jena. Several acoustic sensor units allow for classifying among various acoustic scenes and events using Convolutional Neural Networks (CNN) and measuring different noise level parameters. Connected by a communication system based on MQTT (Message Queue Telemetry Transport), these sensors communicate measurement data to a central server for data postprocessing and storage. Finally, a web-based application allows for various real-time visualizations of noise exposure distributed over the city.},
address = {Barcelona, Spain},
author = {Abe{\ss}er, Jakob and Gr{\"{a}}fe, Robert and K{\"{u}}hn, Christian and Clau{\ss}, Tobias and Lukashevich, Hanna and G{\"{o}}tze, Marco and K{\"{u}}hnlenz, Stephanie},
booktitle = {Proceedings of the 6th IEEE International Conference on Future Internet of Things and Cloud (FiCLOUD)},
xdoi = {10.1109/FiCloud.2018.00053},
file = {::},
isbn = {9781538675038},
keywords = {TA L{\"{a}}rm,acoustic scene classification,event detection,machine{\_}learning,noise level measurement,sensor network,smart city},
mendeley-tags = {machine{\_}learning},
pages = {318--324},
title = {{A Distributed Sensor Network for Monitoring Noise Level and Noise Sources in Urban Environments}},
year = {2018}
}
@article{Grollmisch2019,
abstract = {The ongoing process of automation in production lines increases the requirements for robust and reliable quality control. Acoustic quality control can play a major part in advanced quality control systems since several types of faults such as changes in machine conditions can be heard by experienced machine operators but can hardly be detected otherwise. To this day, acoustic detection systems using airborne sounds struggle due to the highly complex noise scenarios inside factories. Machine learning systems are theoretically able to cope with these conditions. However, recent advancements in the field of Industrial Sound Analysis (ISA) are sparse compared to related research fields like Music Information Retrieval (MIR) or Acoustic Event Detection (AED). One main reason is the lack of freely available datasets since most of the data is very sensitive for companies. Therefore, three novel datasets for ISA with different application fields were recorded and published along with this paper: detection of the operational state of an electric engine, detection of the surface of rolling metal balls, and detection of different bulk materials. For each dataset, neural network based baseline systems were evaluated. The results show that such systems obtain high classification accuracies over all datasets in many of the subtasks which demonstrates the feasibility of audio-based analysis of industrial analysis scenarios. However, the baseline systems remain highly sensitive to changes in the recording setup, which leaves a lot of room for improvement. The main goal of this paper is to stimulate further research in the field of ISA.},
author = {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
file = {::},
journal = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
keywords = {ima,own},
mendeley-tags = {ima,own},
title = {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis}},
year = {2019}
}
@inproceedings{Sandler:2018:MobileNet:CVPR,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
xdoi = {10.1109/CVPR.2018.00474},
eprint = {1801.04381},
file = {:C$\backslash$:/Users/abr/Downloads/1801.04381.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {4510--4520},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
year = {2018}
}
@inproceedings{Stoller:2019:LyricsAudioAlignment:ICASSP,
abstract = {Time-aligned lyrics can enrich the music listening experience by enabling karaoke, text-based song retrieval and intra-song navigation, and other applications. Compared to text-to-speech alignment, lyrics alignment remains highly challenging, despite many attempts to combine numerous sub-modules including vocal separation and detection in an effort to break down the problem. Furthermore, training required fine-grained annotations to be available in some form. Here, we present a novel system based on a modified Wave-U-Net architecture, which predicts character probabilities directly from raw audio using learnt multi-scale representations of the various signal components. There are no sub-modules whose interdependencies need to be optimized. Our training procedure is designed to work with weak, line-level annotations available in the real world. With a mean alignment error of 0.35s on a standard dataset our system outperforms the state-of-the-art by an order of magnitude.},
address = {Brighton, UK},
archivePrefix = {arXiv},
arxivId = {1902.06797},
author = {Stoller, Daniel and Durand, Simon and Ewert, Sebastian},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683470},
eprint = {1902.06797},
file = {:C$\backslash$:/Users/abr/Downloads/08683470.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {CTC training,Lyrics alignment,lyrics,lyrics transcription,multi-scale representation,neural networks},
mendeley-tags = {lyrics},
pages = {181--185},
title = {{End-to-end Lyrics Alignment for Polyphonic Music Using an Audio-to-character Recognition Model}},
year = {2019}
}
@article{Gordon:2018:MorphNet:CVPR,
abstract = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
archivePrefix = {arXiv},
arxivId = {1711.06798},
author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien Ju and Choi, Edward},
xdoi = {10.1109/CVPR.2018.00171},
eprint = {1711.06798},
file = {:C$\backslash$:/Users/abr/Downloads/1711.06798.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {1586--1595},
title = {{MorphNet: Fast {\&} Simple Resource-Constrained Structure Learning of Deep Networks}},
year = {2018}
}
@inproceedings{Tan:2019:EfficientNet:ICML,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
address = {Long Beach, CA, USA},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
eprint = {1905.11946},
file = {:C$\backslash$:/Users/abr/Downloads/1905.11946.pdf:pdf},
isbn = {9781510886988},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
xurl = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@inproceedings{Kong:2019:SceneGeneration:ICASSP,
address = {Brighton, UK},
author = {Kong, Qiuqiang and Xu, Yong and Iqbal, Turab and Cao, Yin and Wang, Wenwu and Plumbley, Mark D},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/Downloads/08683727.pdf:pdf},
isbn = {9781538646588},
pages = {925--929},
title = {{Acoustic Scene Generation with Conditional SampleRNN}},
year = {2019}
}
@inproceedings{Drossos:2019:DomainAdaptation:WASPAA,
abstract = {A challenging problem in deep learning-based machine listening field is the degradation of the performance when using data from unseen conditions. In this paper we focus on the acoustic scene classification (ASC) task and propose an adversarial deep learning method to allow adapting an acoustic scene classification system to deal with a new acoustic channel resulting from data captured with a different recording device. We build upon the theoretical model of $\Delta$-distance and previous adversarial discriminative deep learning method for ASC unsupervised domain adaptation, and we present an adversarial training based method using the Wasserstein distance. We improve the state-of-the-art mean accuracy on the data from the unseen conditions from 32{\%} to 45{\%}, using the TUT Acoustic Scenes dataset.},
address = {New Paltz, NY, USA},
author = {Drossos, Konstantinos and Magron, Paul and Virtanen, Tuomas},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {:C$\backslash$:/Users/abr/Downloads/08937231.pdf:pdf},
keywords = {Acoustic scene classification,Wasserstein distance,acoustic{\_}scene{\_}classificaiton,adversarial training,domain{\_}adaptation,machine{\_}listening,unsupervised domain adaptation},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,domain{\_}adaptation,machine{\_}listening},
pages = {259--263},
publisher = {IEEE},
title = {{Unsupervised Adversarial Domain Adaptation based on the Wasserstein Distance for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Jati:2020:ASC:ICASSP,
abstract = {Devices capable of detecting and categorizing acoustic scenes have numerous applications such as providing context-aware user experiences. In this paper, we address the task of characterizing acoustic scenes in a workplace setting from audio recordings collected with wearable microphones. The acoustic scenes, tracked with Bluetooth transceivers, vary dynamically with time from the egocentric perspective of a mobile user. Our dataset contains experience sampled long audio recordings collected from clinical providers in a hospital, who wore the audio badges during multiple work shifts. To handle the long egocentric recordings, we propose a Time Delay Neural Network{\~{}}(TDNN)-based segment-level modeling. The experiments show that TDNN outperforms other models in the acoustic scene classification task. We investigate the effect of primary speaker's speech in determining acoustic scenes from audio badges, and provide a comparison between performance of different models. Moreover, we explore the relationship between the sequence of acoustic scenes experienced by the users and the nature of their jobs, and find that the scene sequence predicted by our model tend to possess similar relationship. The initial promising results reveal numerous research directions for acoustic scene classification via wearable devices as well as egocentric analysis of dynamic acoustic scenes encountered by the users.},
address = {Barcelona, Spain},
archivePrefix = {arXiv},
arxivId = {1911.03843},
author = {Jati, Arindam and Nadarajan, Amrutha and Mundnich, Karel and Narayanan, Shrikanth},
booktitle = {submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {1911.03843},
file = {::},
keywords = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
title = {{Characterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting}},
xurl = {http://arxiv.org/abs/1911.03843},
year = {2020}
}
@inproceedings{Goodfellow:2014:GAN:NIPS,
abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players - a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and {Yoshua Bengio}},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {::},
issn = {10495258},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
year = {2014}
}
@inproceedings{Wang:2017:PCEN:ICASSP,
abstract = {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
archivePrefix = {arXiv},
arxivId = {1607.05666},
author = {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2017.7953242},
eprint = {1607.05666},
file = {::},
isbn = {9781509041176},
issn = {15206149},
keywords = {automatic gain control,deep neural networks,robust and far-field speech recognition},
number = {1},
pages = {5670--5674},
title = {{Trainable Frontend for Robust and Far-Field Keyword Spotting}},
year = {2017}
}
@article{Jankowski2020,
abstract = {Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available.},
archivePrefix = {arXiv},
arxivId = {2001.04619},
author = {Jankowski, Charles and Mruthyunjaya, Vishwas and Lin, Ruixi},
eprint = {2001.04619},
file = {::},
title = {{Improved Robust ASR for Social Robots in Public Spaces}},
xurl = {http://arxiv.org/abs/2001.04619},
year = {2020}
}
@inproceedings{Rafii:2012:REPET:ISMIR,
abstract = {Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. {\textcopyright} 2012 International Society for Music Information Retrieval.},
address = {Porto, Portugal},
author = {Rafii, Zafar and Pardo, Bryan},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
isbn = {9789727521449},
pages = {583--588},
title = {{Music/Voice Separation using the Similarity Matrix}},
year = {2012}
}
@inproceedings{Gururani:2019:IREC:ISMIR,
abstract = {While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or `attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results.},
address = {Delft, The Netherlands},
archivePrefix = {arXiv},
arxivId = {1907.04294},
author = {Gururani, Siddharth and Sharma, Mohit and Lerch, Alexander},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference},
eprint = {1907.04294},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gururani, Sharma, Lerch - 2019 - An Attention Mechanism for Musical Instrument Recognition(2).pdf:pdf},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
title = {{An Attention Mechanism for Musical Instrument Recognition}},
xurl = {http://arxiv.org/abs/1907.04294},
year = {2019}
}
@article{Xia:2019:EventDetection:CSSR,
abstract = {Recently, neural network-based deep learning methods have been popularly applied to computer vision, speech signal processing and other pattern recognition areas. Remarkable success has been demonstrated by using the deep learning approaches. The purpose of this article is to provide a comprehensive survey for the neural network-based deep learning approaches on acoustic event detection. Different deep learning-based acoustic event detection approaches are investigated with an emphasis on both strongly labeled and weakly labeled acoustic event detection systems. This paper also discusses how deep learning methods benefit the acoustic event detection task and the potential issues that need to be addressed for prospective real-world scenarios.},
author = {Xia, Xianjun and Togneri, Roberto and Sohel, Ferdous and Zhao, Yuanjun and Huang, Defeng},
xdoi = {10.1007/s00034-019-01094-1},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia et al. - 2019 - A Survey Neural Network-Based Deep Learning for Acoustic Event Detection.pdf:pdf},
issn = {15315878},
journal = {Circuits, Systems, and Signal Processing},
keywords = {Acoustic event detection,Deep learning,Strongly labeled,Weakly labeled},
title = {{A Survey: Neural Network-Based Deep Learning for Acoustic Event Detection}},
year = {2019}
}
@inproceedings{Wiggins:2019:GuitarCNN:ISMIR,
abstract = {Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation , followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset [24], and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation.},
address = {Delft, The Netherlands},
author = {Wiggins, Andrew and Kim, Youngmoo},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/Downloads/000033 (3).pdf:pdf},
keywords = {music transcription},
mendeley-tags = {music transcription},
title = {{Guitar Tablature Estimation With a Convolutional Neural Network}},
xurl = {https://github.com/andywiggins/tab-cnn},
year = {2019}
}
@techreport{Davies2009a,
abstract = {A fundamental research topic in music information retrieval is the automatic extraction of beat locations from music signals. In this paper we address the under-explored topic of beat tracking evaluation. We present a review of existing evaluation models and, given their strengths and weaknesses, we propose a new method based on a novel visualisation for beat tracking performance, the beat error histogram. To investigate the properties of evaluation methods we undertake a large scale beat tracking experiment. We conduct experiments using a new annotated test database which we make available to the research community. We demonstrate that the choice of evaluation method can have a significant impact on the relative performance of different beat tracking algorithms. On this basis we make a set of recommendations for comparative beat tracking experiments.},
author = {Davies, Matthew E P and Degara, Norberto and Plumbley, Mark D},
file = {::},
title = {{Evaluation Methods for Musical Audio Beat Tracking Algorithms}},
year = {2009}
}
@article{Zhang2020,
abstract = {Learning concepts from the limited number of datapoints is a challenging task usually addressed by the so-called one- or few-shot learning. Recently, an application of second-order pooling in few-shot learning demonstrated its superior performance due to the aggregation step handling varying image resolutions without the need of modifying CNNs to fit to specific image sizes, yet capturing highly descriptive co-occurrences. However, using a single resolution per image (even if the resolution varies across a dataset) is suboptimal as the importance of image contents varies across the coarse-to-fine levels depending on the object and its class label e. g., generic objects and scenes rely on their global appearance while fine-grained objects rely more on their localized texture patterns. Multi-scale representations are popular in image deblurring, super-resolution and image recognition but they have not been investigated in few-shot learning due to its relational nature complicating the use of standard techniques. In this paper, we propose a novel multi-scale relation network based on the properties of second-order pooling to estimate image relations in few-shot setting. To optimize the model, we leverage a scale selector to re-weight scale-wise representations based on their second-order features. Furthermore, we propose to a apply self-supervised scale prediction. Specifically, we leverage an extra discriminator to predict the scale labels and the scale discrepancy between pairs of images. Our model achieves state-of-the-art results on standard few-shot learning datasets.},
archivePrefix = {arXiv},
arxivId = {2001.01600},
author = {Zhang, Hongguang and Torr, Philip H. S. and Koniusz, Piotr},
eprint = {2001.01600},
file = {:C$\backslash$:/Users/abr/Downloads/2001.01600.pdf:pdf},
title = {{Few-shot Learning with Multi-scale Self-supervision}},
xurl = {http://arxiv.org/abs/2001.01600},
year = {2020}
}
@article{Rouditchenko2019,
abstract = {Segmenting objects in images and separating sound sources in audio are challenging tasks, in part because traditional approaches require large amounts of labeled data. In this paper we develop a neural network model for visual object segmentation and sound source separation that learns from natural videos through self-supervision. The model is an extension of recently proposed work that maps image pixels to sounds [1]. Here, we introduce a learning approach to disentangle concepts in the neural networks, and assign semantic categories to network feature channels to enable independent image segmentation and sound source separation after audio-visual training on videos. Our evaluations show that the disentangled model outperforms several baselines in semantic segmentation and sound source separation.},
archivePrefix = {arXiv},
arxivId = {1904.09013},
author = {Rouditchenko, Andrew and Zhao, Hang and Gan, Chuang and McDermott, Josh and Torralba, Antonio},
xdoi = {10.1109/ICASSP.2019.8682467},
eprint = {1904.09013},
file = {:C$\backslash$:/Users/abr/Downloads/1904.09013.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {audio-visual,co-segmentation,disentangled,self-supervised,source separation},
pages = {2357--2361},
title = {{Self-supervised Audio-visual Co-segmentation}},
volume = {2019-May},
year = {2019}
}
@article{Korbar2018,
abstract = {There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9{\%} in action recognition accuracy on UCF101 and a boost of +17.7{\%} on HMDB51.},
archivePrefix = {arXiv},
arxivId = {1807.00230},
author = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
eprint = {1807.00230},
file = {:C$\backslash$:/Users/abr/Downloads/1807.00230.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {7763--7774},
title = {{Cooperative learning of audio and video models from self-supervised synchronization}},
volume = {2018-Decem},
year = {2018}
}
@article{Tagliasacchi2019,
abstract = {We explore self-supervised models that can be potentially deployed on mobile devices to learn general purpose audio representations. Specifically, we propose methods that exploit the temporal context in the spectrogram domain. One method estimates the temporal gap between two short audio segments extracted at random from the same audio clip. The other methods are inspired by Word2Vec, a popular technique used to learn word embeddings, and aim at reconstructing a temporal spectrogram slice from past and future slices or, alternatively, at reconstructing the context of surrounding slices from the current slice. We focus our evaluation on small encoder architectures, which can be potentially run on mobile devices during both inference (re-using a common learned representation across multiple downstream tasks) and training (capturing the true data distribution without compromising users' privacy when combined with federated learning). We evaluate the quality of the embeddings produced by the self-supervised learning models, and show that they can be re-used for a variety of downstream tasks, and for some tasks even approach the performance of fully supervised models of similar size.},
archivePrefix = {arXiv},
arxivId = {1905.11796},
author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, F{\'{e}}lix de Chaumont and Roblek, Dominik},
eprint = {1905.11796},
file = {::},
title = {{Self-supervised audio representation learning for mobile devices}},
xurl = {http://arxiv.org/abs/1905.11796},
year = {2019}
}
@article{Lee2019,
abstract = {While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6{\%} through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.},
archivePrefix = {arXiv},
arxivId = {1910.12587},
author = {Lee, Tyler and Gong, Ting and Padhy, Suchismita and Rouditchenko, Andrew and Ndirango, Anthony},
eprint = {1910.12587},
file = {:C$\backslash$:/Users/abr/Downloads/1910.12587.pdf:pdf},
pages = {1--10},
title = {{Label-efficient audio classification through multitask learning and self-supervision}},
xurl = {http://arxiv.org/abs/1910.12587},
year = {2019}
}
@article{Dinkel2019,
abstract = {Depression detection research has increased over the last few decades as this disease is becoming a socially-centered problem. One major bottleneck for developing automatic depression detection methods lies on the limited data availability. Recently, pretrained text-embeddings have seen success in sparse data scenarios, while pretrained audio embeddings are rarely investigated. This paper proposes DEPA, a self-supervised, Word2Vec like pretrained depression audio embedding method for depression detection. An encoder-decoder network is used to extract DEPA on sparse-data in-domain (DAIC) and large-data out-domain (switchboard, Alzheimer's) datasets. With DEPA as the audio embedding, performance significantly outperforms traditional audio features regarding both classification and regression metrics. Moreover, we show that large-data out-domain pretraining is beneficial to depression detection performance.},
archivePrefix = {arXiv},
arxivId = {1910.13028},
author = {Dinkel, Heinrich and Zhang, Pingyue and Wu, Mengyue and Yu, Kai},
eprint = {1910.13028},
file = {:C$\backslash$:/Users/abr/Downloads/1910.13028.pdf:pdf},
title = {{Depa: Self-supervised audio embedding for depression detection}},
xurl = {http://arxiv.org/abs/1910.13028},
year = {2019}
}
@article{Kim2019,
abstract = {We describe a novel weakly labeled Audio Event Classification approach based on a self-supervised attention model. The weakly labeled framework is used to eliminate the need for expensive data labeling procedure and self-supervised attention is deployed to help a model distinguish between relevant and irrelevant parts of a weakly labeled audio clip in a more effective manner compared to prior attention models. We also propose a highly effective strongly supervised attention model when strong labels are available. This model also serves as an upper bound for the self-supervised model. The performances of the model with self-supervised attention training are comparable to the strongly supervised one which is trained using strong labels. We show that our self-supervised attention method is especially beneficial for short audio events. We achieve 8.8{\%} and 17.6{\%} relative mean average precision improvements over the current state-of-the-art systems for SL-DCASE-17 and balanced AudioSet.},
archivePrefix = {arXiv},
arxivId = {1908.02876},
author = {Kim, Bongjun and Ghaffarzadegan, Shabnam},
xdoi = {10.23919/EUSIPCO.2019.8902567},
eprint = {1908.02876},
file = {:C$\backslash$:/Users/abr/Downloads/1908.02876.pdf:pdf},
isbn = {9789082797039},
issn = {22195491},
title = {{Self-supervised Attention Model for Weakly Labeled Audio Event Classification}},
xurl = {http://arxiv.org/abs/1908.02876},
year = {2019}
}
@inproceedings{Cramer2019,
address = {Brighton, United Kingdom},
author = {Cramer, Jason and Wu, Ho-hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682475},
file = {::},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {3852--3856},
publisher = {IEEE},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}},
xurl = {https://ieeexplore.ieee.org/document/8682475/},
year = {2019}
}
@article{Aytar:2016:SoundNet:NIPS,
abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
archivePrefix = {arXiv},
arxivId = {1610.09001},
author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
eprint = {1610.09001},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {892--900},
title = {{SoundNet: Learning Sound Representations from Unlabeled Video}},
year = {2016}
}
@article{Boddapati:2017:ASC:PCS,
abstract = {Automatic classification of environmental sounds, such as dog barking and glass breaking, is becoming increasingly interesting, especially for mobile devices. Most mobile devices contain both cameras and microphones, and companies that develop mobile devices would like to provide functionality for classifying both videos/images and sounds. In order to reduce the development costs one would like to use the same technology for both of these classification tasks. One way of achieving this is to represent environmental sounds as images, and use an image classification neural network when classifying images as well as sounds. In this paper we consider the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds. We evaluate the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition (AlexNet and GoogLeNet). Our experiments show that we obtain good classification accuracy for the three datasets.},
author = {Boddapati, Venkatesh and Petef, Andrej and Rasmusson, Jim and Lundberg, Lars},
xdoi = {10.1016/j.procs.2017.08.250},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boddapati et al. - 2017 - Classifying environmental sounds using image recognition networks.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolutional Neural Networks,Deep Learning,Environmental Sound Classification,GPU Processing,Image Classification},
pages = {2048--2056},
publisher = {Elsevier B.V.},
title = {{Classifying environmental sounds using image recognition networks}},
xurl = {http://dx.xdoi.org/10.1016/j.procs.2017.08.250},
volume = {112},
year = {2017}
}
@article{Kim2018,
abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
archivePrefix = {arXiv},
arxivId = {1802.04051},
author = {Kim, Jaehun and Urbano, Juli{\'{a}}n and Liem, Cynthia C. S. and Hanjalic, Alan},
eprint = {1802.04051},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - One Deep Music Representation to Rule Them All A comparative analysis of different representation learning strate(2).pdf:pdf},
keywords = {accepted to,learning for music and,multi-task learning,music information retrieval,neural computing and applications,representation learning,special issue on deep,this work has been},
month = {feb},
title = {{One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies}},
xurl = {http://arxiv.org/abs/1802.04051},
year = {2018}
}
@inproceedings{Arandjelovic2017,
abstract = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
author = {Arandjelovic, Relja and Zisserman, Andrew},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
xdoi = {10.1109/ICCV.2017.73},
file = {::},
isbn = {978-1-5386-1032-9},
issn = {15505499},
month = {oct},
pages = {609--617},
publisher = {IEEE},
title = {{Look, Listen and Learn}},
xurl = {http://ieeexplore.ieee.org/document/8237335/},
volume = {2017-Octob},
year = {2017}
}
@article{Benetos:2012:ASC:DAFX,
abstract = {In this paper, we propose a method for modeling and classifying acoustic scenes using temporally-constrained shift-invariant probabilistic latent component analysis (SIPLCA). SIPLCA can be used for extracting time-frequency patches from spectrograms in an unsupervised manner. Component-wise hidden Markov models are incorporated to the SIPLCA formulation for enforcing temporal constraints on the activation of each acoustic component. The time-frequency patches are converted to cepstral coefficients in order to provide a compact representation of acoustic events within a scene. Experiments are made using a corpus of train station recordings, classified into 6 scene classes. Results show that the proposed model is able to model salient events within a scene and outperforms the non-negative matrix factorization algorithm for the same task. In addition, it is demonstrated that the use of temporal constraints can lead to improved performance.},
author = {Benetos, Emmanouil and Lagrange, Mathieu and Dixon, Simon},
file = {::},
journal = {Proceedings of the 15th International Conference on Digital Audio Effects (DAFx-12)},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {1--7},
title = {{Characterisation of Acoustic Scenes using a Temporally-Constrained Shift-Invariant Model}},
year = {2012}
}
@inproceedings{Chen:2018:Scalogram:INTERSPEECH,
abstract = {Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5{\%} in the evaluation step in DCASE2016.},
address = {Hyderabad, India},
author = {Chen, Hangting and Zhang, Pengyuan and Bai, Haichuan and Yuan, Qingsheng and Bao, Xiuguo and Yan, Yonghong},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
xdoi = {10.21437/Interspeech.2018-1524},
file = {::},
issn = {19909772},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {3304--3308},
title = {{Deep convolutional neural network with scalogram for audio scene modeling}},
year = {2018}
}
@inproceedings{Adavanne:2017:AEDWeaklyLabels:DCASE,
abstract = {This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log mel-band energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3{\%} for weak labels on the unseen test split.},
address = {Munich, Germany},
archivePrefix = {arXiv},
arxivId = {1710.02998},
author = {Adavanne, Sharath and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
eprint = {1710.02998},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adavanne, Virtanen - 2017 - Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network.pdf:pdf},
title = {{Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network}},
xurl = {http://arxiv.org/abs/1710.02998},
year = {2017}
}
@article{Gfeller:2019:SPLICE:ARXIV,
abstract = {We propose a model to estimate the fundamental frequency in monophonic audio, often referred to as pitch estimation. We acknowledge the fact that obtaining ground truth annotations at the required temporal and frequency resolution is a particularly daunting task. Therefore, we propose to adopt a self-supervised learning technique, which is able to estimate (relative) pitch without any form of supervision. The key observation is that pitch shift maps to a simple translation when the audio signal is analysed through the lens of the constant-Q transform (CQT). We design a self-supervised task by feeding two shifted slices of the CQT to the same convolutional encoder, and require that the difference in the outputs is proportional to the corresponding difference in pitch. In addition, we introduce a small model head on top of the encoder, which is able to determine the confidence of the pitch estimate, so as to distinguish between voiced and unvoiced audio. Our results show that the proposed method is able to estimate pitch at a level of accuracy comparable to fully supervised models, both on clean and noisy audio samples, yet it does not require access to large labeled datasets},
archivePrefix = {arXiv},
arxivId = {1910.11664},
author = {Gfeller, Beat and Frank, Christian and Roblek, Dominik and Sharifi, Matt and Tagliasacchi, Marco and Velimirovi{\'{c}}, Mihajlo},
eprint = {1910.11664},
file = {:C$\backslash$:/Users/abr/Downloads/1910.11664.pdf:pdf},
journal = {ArXiv e-prints},
pages = {1--9},
title = {{SPICE: Self-supervised Pitch Estimation}},
xurl = {http://arxiv.org/abs/1910.11664},
year = {2019}
}
@inproceedings{Cartwright:2019:Tricycle:WASPAA,
address = {New Paltz, NY, USA},
author = {Cartwright, Mark and Cramer, Jason and Salamon, Justin and Bello, Juan Pablo},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
file = {:C$\backslash$:/Users/abr/Downloads/cartwright{\_}tricycle{\_}waspaa2019.pdf:pdf},
keywords = {self{\_}supervised{\_}learning},
mendeley-tags = {self{\_}supervised{\_}learning},
title = {{Tricycle: Audio Representation Learning from Sensor Network Data using Self-Supervision}},
year = {2019}
}
@article{Abesser:2019:UrbanNoise:ERCIM,
author = {Abe{\ss}er, Jakob and Kepplinger, Sara},
file = {::},
journal = {ERCIM News 119},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Smart Solutions to Cope with Urban Noise Pollution}}
}
@inproceedings{Taenzer:2019:Instrument:ISMIR,
address = {Delft, The Netherlands},
author = {Taenzer, Michael and Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and M{\"{u}}ller, Meinard and Lukashevich, Hanna},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Investigating CNN-based Instrument Family Recognition for Western Classical Music Recordings}},
year = {2019}
}
@inproceedings{Cano:2019:LBD:ISMIR,
address = {Delft, The Netherlands},
author = {Cano, Estefania and Escamilla, Antonio and Grollmisch, Sascha and Kehling, Christian and Gil, Gustavo Adolfo L{\'{o}}pez and {\'{A}}ngel, Fernando Mora},
booktitle = {Late Breaking Demo at the International Society for Music Information Retrieval (ISMIR)},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{ACMus - Advancing Computational Musicology: Semi-Supervised and Unsupervised Segmentation and Annotation of Musical Collections}},
year = {2019}
}
@inproceedings{Grollmisch:2019:EnsembleSize:CMMR,
address = {Marseille, France},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a and Mora-{\'{A}}ngel, Fernando and Gil, Gustavo L{\'{o}}pez},
booktitle = {Proceedings of the 14th International Symposium of Computer Music Multidisciplinary Research (CMMR)},
file = {::},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Ensemble size classification in Colombian Andean string music recordings}},
year = {2019}
}
@article{Mimilakis:2020:Denoising:TASLP,
author = {Mimilakis, Stylianos Ioannis and Drosos, Konstantinos and Cano, Estefan{\'{i}}a and Schuller, Gerald},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Examining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation}},
volume = {28},
year = {2020}
}
@inproceedings{Mora-Angel:2019:ACMUS:DLFM,
address = {Delft, The Netherlands},
author = {Mora-{\'{A}}ngel, Fernando and Gil, Gustavo A L{\'{o}}pez and Cano, Estefan{\'{i}}a and Grollmisch, Sascha},
booktitle = {Proceedings of the Digital Libraries for Musicology (DLfM)},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{ACMUS-MIR: A new annotated data set of Andean Colombian music}},
year = {2019}
}
@inproceedings{Mimilakis:2019:CrossVersion:MML,
author = {Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and Arifi-M{\"{u}}ller, Vlora and Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 12th International Workshop on Machine Learning and Music (MML)},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Cross-Version Singing Voice Detection in Opera Recordings: Challenges for Supervised Learning}},
year = {2019}
}
@inproceedings{Hawley:2019:Compressor:AES,
author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis},
booktitle = {Proceedings of the 147th Audio Engineering Society Convention},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{SignalTrain: Profiling Audio Compressors with Deep Neural Networks}},
year = {2019}
}
@article{Thickstun2018,
abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
xdoi = {10.1109/ICASSP.2018.8461686},
file = {::},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural networks,Invariances,Learning,Music information retrieval},
pages = {2241--2245},
publisher = {IEEE},
title = {{Invariances and Data Augmentation for Supervised Music Transcription}},
volume = {2018-April},
year = {2018}
}
@article{Hawthorne2017,
abstract = {We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100{\%} relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.},
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
eprint = {1710.11153},
file = {::},
title = {{Onsets and Frames: Dual-Objective Piano Transcription}},
xurl = {http://arxiv.org/abs/1710.11153},
year = {2017}
}
@article{Cong2018,
abstract = {In this paper, a supervised approach based on Convolutional Neural Networks (CNN) for polyphonic piano transcription is presented. The system consists of pitch detection model, onset/offset detection model, and note search model. The pitch detection model is a single-channel CNN predicting the probabilities of pitches contained in one frame of the audio. The onset/offset model based on dual-channel CNN is used for estimating the probabilities of each pitch's onset or offset in a frame. The note search model is rule-based; it integrates the outputs of the pitch model and onset/offset model to determine the final onset, offset and pitch of notes in audio. Two experiments with different dataset conditions are accomplished to compare with state-of-the-art approaches on the same datasets. Experimental results reveal that the proposed approach preforms better in both frame-and note-based metrics.},
author = {Cong, Fu'Ze and Liu, Shuchang and Guo, Li and Wiggins, Geraint A.},
xdoi = {10.1109/ICASSP.2018.8461794},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
title = {{A Parallel Fusion Approach to Piano Music Transcription Based on Convolutional Neural Network}},
year = {2018}
}
@article{Hawthorne2018,
abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\~{}}0.1 ms to {\~{}}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\~{}}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
archivePrefix = {arXiv},
arxivId = {1810.12247},
author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
eprint = {1810.12247},
file = {::},
pages = {1--12},
title = {{Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset}},
xurl = {http://arxiv.org/abs/1810.12247},
year = {2018}
}
@article{Kelz2019,
abstract = {We investigate a late-fusion approach to piano transcription , combined with a strong temporal prior in the form of a handcrafted Hidden Markov Model (HMM). The network architecture under consideration is compact in terms of its number of parameters and easy to train with gradient descent and momentum. The network outputs are fused over time in the final stage to obtain note segmentations, with an HMM whose transition-and observation probabilities are chosen based on a model of attack decay sustain release (ADSR) envelope. The note segments are then subject to a final binary decision rule to reject too weak note segment hypotheses. 1. METHODS We would like to transcribe a polyphonic audio recording of a piano into a symbolic score. For each note sounding, we expect to obtain a tuple (s, e, n, v), denoting start, end, MIDI note number, and optionally, volume. 1.1 Deep convolutional neural network We employ a model with multiple outputs, predicting different note phases. A conceptual drawing is shown in Figure 1. The network input x t ∈ R c×b is a spectro-gram snippet, extending c context frames in time, and b bins in frequency. b is the number of bins resulting from passing a linear STFT through a filterbank with semi-logarithmically spaced, triangular filters, resulting in a resolution of approximately two bins per semitone. We choose c = 11, b = 144. The temporal resolution of the model is 50 [frames/s]. The target matrix y t ∈ {\{}0, 1{\}} 88×3 decomposes into vectors y on t , y f rm t , and y off t respectively, denoting the onset , the intermediate note phases, and the offset for each note for the center frame within the context window c.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.09165v1},
author = {Kelz, Rainer and Bock, Sebastian and Widmer, Gerhard},
xdoi = {10.1109/icassp.2019.8683582},
eprint = {arXiv:1906.09165v1},
file = {::},
pages = {246--250},
title = {{Deep Polyphonic ADSR Piano Note Transcription}},
year = {2019}
}
@article{Sigtia2016,
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network-based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yield the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
xdoi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic music transcription,Deep learning,Music language models,Recurrent neural networks},
number = {5},
pages = {927--939},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
volume = {24},
year = {2016}
}
@article{Kelz2016,
abstract = {In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset -- without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.},
archivePrefix = {arXiv},
arxivId = {1612.05153},
author = {Kelz, Rainer and Dorfer, Matthias and Korzeniowski, Filip and B{\"{o}}ck, Sebastian and Arzt, Andreas and Widmer, Gerhard},
eprint = {1612.05153},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelz et al. - 2016 - On the Potential of Simple Framewise Approaches to Piano Transcription.pdf:pdf},
month = {dec},
title = {{On the Potential of Simple Framewise Approaches to Piano Transcription}},
xurl = {http://arxiv.org/abs/1612.05153},
year = {2016}
}
@article{Li2019a,
abstract = {Convolutional neural networks are widely adopted in Acoustic Scene Classification (ASC) tasks, but they generally carry a heavy computational burden. In this work, we propose a lightweight yet high-performing baseline network inspired by MobileNetV2, which replaces square convolutional kernels with unidirectional ones to extract features alternately in temporal and frequency dimensions. Furthermore, we explore a dynamic architecture space built on the basis of the proposed baseline with the recent Neural Architecture Search (NAS) paradigm, which first trains a supernet that incorporates all candidate networks and then applies a well-known evolutionary algorithm NSGA-II to discover more efficient networks with higher accuracy and lower computational cost. Experimental results demonstrate that our searched network is competent in ASC tasks, which achieves 90.3{\%} F1-score on the DCASE2018 task 5 evaluation set, marking a new state-of-the-art performance while saving 25{\%} of FLOPs compared to our baseline network.},
archivePrefix = {arXiv},
arxivId = {1912.12825},
author = {Li, Jixiang and Liang, Chuming and Zhang, Bo and Wang, Zhao and Xiang, Fei and Chu, Xiangxiang},
eprint = {1912.12825},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Neural Architecture Search on Acoustic Scene Classification.pdf:pdf},
title = {{Neural Architecture Search on Acoustic Scene Classification}},
xurl = {http://arxiv.org/abs/1912.12825},
year = {2019}
}
@article{Bisot:2017:ASC:TASLP,
abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
author = {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Ga{\"{e}}l},
xdoi = {10.1109/TASLP.2017.2690570},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Acoustic scene classification,feature learning,matrix factorization},
number = {6},
pages = {1216--1229},
title = {{Feature Learning with Matrix Factorization Applied to Acoustic Scene Classification}},
volume = {25},
year = {2017}
}
@article{Ye:2018:ASC:AS,
abstract = {This paper presents a novel approach for acoustic scene classification based on efficient acoustic feature extraction using spectro-temporal descriptors fusion. Grounded on the finding in neuroscience-"auditory system summarizes the temporal details of sounds using time-averaged statistics to understand acoustic scenes", we devise an efficient computational framework for sound scene classification by using multipe time-frequency descriptors fusion with discriminant information enhancement. To characterize rich information of sound, i.e., local structures on the time-frequency plane, we adopt 2-dimensional local descriptors. A more critical issue raised in how to logically 'summarize' those local details into a compact feature vector for scene classification. Although 'time-averaged statistics' is suggested by the psychological investigation, directly computing time average of local acoustic features is not a logical way, since arithmetic mean is vulnerable to extreme values which are anticipated to be generated by interference sounds which are irrelevant to the scene category. To tackle this problem, we develop time-frame weighting approach to enhance sound textures as well as to suppress scene-irrelevant events. Subsequently, robust acoustic feature for scene classification can be efficiently characterized. The proposed method had been validated by using Rouen dataset which consists of 19 acoustic scene categories with 3029 real samples. Extensive results demonstrated the effectiveness of the proposed scheme.},
author = {Ye, Jiaxing and Kobayashi, Takumi and Toyama, Nobuyuki and Tsuda, Hiroshi and Murakawa, Masahiro},
xdoi = {10.3390/app8081363},
file = {::},
issn = {20763417},
journal = {Applied Sciences},
keywords = {Acoustic scene classification,Convex combination,Local descriptor,Summary statistics,Time-frequency analysis},
number = {8},
pages = {1--12},
title = {{Acoustic scene classification using efficient summary statistics and multiple spectro-temporal descriptor fusion}},
volume = {8},
year = {2018}
}
@article{Zhong:2017:RandomErasing:ARXIV,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
eprint = {1708.04896},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong et al. - 2017 - Random Erasing Data Augmentation.pdf:pdf},
journal = {Arxiv},
keywords = {data{\_}augmentation},
mendeley-tags = {data{\_}augmentation},
title = {{Random Erasing Data Augmentation}},
xurl = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@article{Lostanlen:2018:PCEN:SPL,
abstract = {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named per-channel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This letter investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Second, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Third, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise, PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
author = {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
xdoi = {10.1109/LSP.2018.2878620},
file = {::},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Acoustic noise,acoustic sensors,acoustic signal detection,machine{\_}listening,signal classification,spectrogram},
mendeley-tags = {machine{\_}listening},
number = {1},
pages = {39--43},
title = {{Per-channel energy normalization: Why and how}},
volume = {26},
year = {2019}
}
@inproceedings{Lehner:2019:ASCReject:DCASE,
address = {New York, NY, USA},
author = {Lehner, Bernhard and Koutini, Khaled and Schwarzlm{\"{u}}ller, Christopher and Gallien, Thomas and Widmer, Gerhard},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events},
file = {::},
keywords = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classificaiton,machine{\_}listening},
title = {{Acoustic Scene Classification with Reject Option based on Resnets}},
year = {2019}
}
@inproceedings{Chen:2019:ASCFilters:ICASSP,
address = {Brighton, UK},
author = {Chen, Hangting and Zhang, Pengyuan and Yan, Yonghong},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781538646588},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {835--839},
title = {{An Audio Scene Classification Framework with Embedded Filters and a DCT-Based Temporal Module}},
year = {2019}
}
@article{Hasani2019,
author = {Hasani, Moein},
file = {::},
keywords = {batch normalization,convolutional neural networks},
title = {{IEEE Copyright Notice {\textcopyright} 2019 IEEE . Personal use of this material is permitted . Permission from IEEE must be obtained for all other uses , in any current or future media , including reprinting / republishing this material for advertising or promotional p}},
year = {2019}
}
@inproceedings{Bisot:2017:NFASC:DCASE,
address = {Munich, Germany},
author = {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Nonnegative Feature Learning Methods for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Zoehrer:2016:GRN_ASC:DCASE,
address = {Budapest, Hungary},
author = {Z{\"{o}}hrer, Matthias and Pernkopf, Franz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Gated Recurrent Networks Applied to Acoustic Scene Classification and Acoustic Event Detection}},
year = {2016}
}
@inproceedings{Abesser:2017:ASC:DCASE,
abstract = {Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban en-vironments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-the-art CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configura-tion leads to a classification accuracy of 75{\%} for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.},
address = {Munich, Germany},
author = {Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Gr{\"{a}}fe, Robert and Lukashevich, Hanna},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - 2017 - Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Ne.pdf:pdf},
title = {{Acoustic Scene Classification By Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks}},
year = {2017}
}
@inproceedings{Moritz:2016:TDNN:DCASE,
abstract = {This paper presents a system for acoustic scene classification (SC) that is applied to data of the SC task of the DCASE'16 challenge (Task 1). The proposed method is based on extracting acoustic features that employ a relatively long temporal context, i.e., amplitude modulation filer bank (AMFB) features, prior to detection of acoustic scenes using a neural network (NN) based classification approach. Recurrent neural networks (RNN) are well suited to model long-term acoustic dependencies that are known to encode important information for SC tasks. However, RNNs require a relatively large amount of training data in com-parison to feed-forward deep neural networks (DNNs). Hence, the time-delay neural network (TDNN) approach is used in the present work that enables analysis of long contextual infor-mation similar to RNNs but with training efforts comparable to conventional DNNs. The proposed SC system attains a recogni-tion accuracy of 76.5 {\%}, which is 4.0 {\%} higher compared to the DCASE'16 baseline system.},
address = {Budapest, Hungary},
author = {Moritz, Niko and Schr{\"{o}}der, Jens and Goetze, Stefan and Anem{\"{u}}ller, J{\"{o}}rn and Kollmeier, Birger},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using Time-Delay Neural Networks and Amplitude Modulation Filter Bank Features}},
year = {2016}
}
@inproceedings{Mafra:2016:CompactASC:DCASE,
address = {Budapest, Hungary},
author = {Mafra, Gustavo Sena and Duong, Ngoc Q. K. and Ozerov, Alexey and P{\'{e}}rez, Patrick},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification: An Evaluation of an Extremely Compact Feature Representations}},
year = {2016}
}
@inproceedings{Valenti:2016:ASC:DCASE,
address = {Budapest, Hungary},
author = {Valenti, Michele and Diment, Aleksandr and Parascandolo, Giambattista and Squartini, Stefano and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.1111/j.1558-5646.2010.01180.x},
file = {::},
isbn = {1558-5646 (Electronic)$\backslash$r0014-3820 (Linking)},
keywords = {Acoustic scene classification,DCASE,computational audio processing,convolutional neural networks},
pmid = {21361918},
title = {{DCASE 2016 Acoustic Scene Classification Using Convolutional Neural Networks}},
year = {2016}
}
@inproceedings{Xu:2016:HierarchicalASC:DCASE,
address = {Budapest, Hungary},
author = {Xu, Yong and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Hierarchical Learning for DNN-Based Acoustic Scene Classification}},
year = {2016}
}
@inproceedings{Bae:2016:LSTMCNN:DCASE,
address = {Budapest, Hungary},
author = {Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
title = {{Acoustic Scene Classification using Parallel Combination of LSTM and CNN}},
year = {2016}
}
@inproceedings{Elizalde:2016:ASC:DCASE,
abstract = {In this paper we present our work on Task 1 Acoustic Scene Classi- fication and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9{\%} compared to the baseline of 72.6{\%} and for Task 3 we achieved a Segment-Based Error Rate of 0.76 compared to the baseline of 0.91.},
address = {Budapest, Hungary},
archivePrefix = {arXiv},
arxivId = {1607.06706},
author = {Elizalde, Benjamin and Kumar, Anurag and Shah, Ankit and Badlani, Rohan and Vincent, Emmanuel and Raj, Bhiksha and Lane, Ian},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
eprint = {1607.06706},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Experiments on the DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording}},
xurl = {http://arxiv.org/abs/1607.06706},
year = {2016}
}
@inproceedings{Weiping:2017:SpectrogramFusion:DCASE,
address = {Munich, Germany},
author = {Weiping, Zheng and Jiantao, Yi and Xiaotao, Xing and Xiangtao, Liu and Shaohu, Peng},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiping et al. - 2017 - ACOUSTIC SCENE CLASSIFICATION USING DEEP CONVOLUTIONAL NEURAL NETWORK AND MULTIPLE SPECTROGRAMS FUSION School(2).pdf:pdf},
isbn = {1299670261},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using Deep Convolutional Neural Networks and Multiple Spectrogram Fusions}},
year = {2017}
}
@inproceedings{Marchi:2016:MKSL:DCASE,
abstract = {We propose a system for acoustic scene classification using pair-wise decomposition with deep neural networks and dimensionality reduction by multiscale kernel subspace learning. It is our contri-bution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2016). The system classifies 15 different acoustic scenes. First, auditory spectral features are extracted and fed into 15 binary deep multilayer perceptron neural networks (MLP). MLP are trained with the 'one-against-all' paradigm to perform a pair-wise decomposition. In a second stage, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Multiscale Gaussian kernels are then used in constructing optimal linear combination of Gram matrices for multiple kernel subspace learning. The reduced feature set is fed into a nearest-neighbour classifier. Predictions from the two systems are then combined by a threshold-based decision function. On the official development set of the challenge, an accuracy of 81.4{\%} is achieved.},
address = {Budapest, Hungary},
author = {Marchi, Erik and Tonelli, Dario and Xu, Xinzhou and Ringeval, Fabien and Deng, Jun and Squartini, Stefano and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Pairwise Decomposition with Deep Neural Networks and Multiscale Kernel Subspace Learning for Acoustic Scene Classification}},
year = {2016}
}
@inproceedings{Park:2017:DoubleImageASC:DCASE,
address = {Munich, Germany},
author = {Park, Sangwook and Mun, Seonkyu and Lee, Younglo and Ko, Hanseok},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2017 - Acoustic Scene Classification Based on Convolutional Neural Network using Double Image Features.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification Based on Convolutional Neural Network using Double Image Features}},
year = {2017}
}
@inproceedings{Mun:2017:GANASC:DCASE,
address = {Munich, Germany},
author = {Mun, Seongkyu and Park, Sangwook and Han, David K. and Ko, Hanseok},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scenes - 2017 - GENERATIVE ADVERSARIAL NETWORK BASED ACOUSTIC SCENE TRAINING SET AUGMENTATION AND SELECTION USING SVM HYPER-PLANE Seongk.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Generative Adversarial Networks based Acoustic Scene Training Set Augmentation and Selection using SVM Hyperplane}},
year = {2017}
}
@inproceedings{Vafeiadis:2017:HybridASC:DCASE,
address = {Munich, Germany},
author = {Vafeiadis, Anastasios and Kalatzis, Dimitrios and Votis, Konstantinos and Giakoumis, Dimitrios and Tzovaras, Dimitrios and Chen, Liming and {Raouf Hamzaoui}},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.1109/TMM.2015.2428998},
file = {::},
issn = {15209210},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning}},
year = {2017}
}
@inproceedings{Ren:2017:DeepSequentialASC:DCASE,
address = {Munich, Germany},
author = {Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Deep Sequential Image Features for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Qian:2017:WaveletASC:DCASE,
address = {Munich, Germany},
author = {Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Wavelets Revisited for the Classification of Acoustic Scenes}},
year = {2017}
}
@inproceedings{Piczak:2017:ASC:DCASE,
abstract = {This study describes a convolutional neural network model submit-ted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with dif-ferent frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolu-tional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration.},
address = {Munich, Germany},
author = {Piczak, Karol J.},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification}},
xurl = {http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge{\_}technical{\_}reports/DCASE2017{\_}Piczak{\_}208.pdf},
year = {2017}
}
@inproceedings{Jimenez:2017:ShiftInvariantASC:DCASE,
address = {Munich, Germany},
author = {Jim{\'{e}}nez, Abelino and Elizalde, Benjam{\'{i}}n and Raj, Bhiksha},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{DCASE 2017 Task 1: Acoustic Scene Classification using Shift-Invariant Kernels and Random Features}},
year = {2017}
}
@inproceedings{Fonseca:2017:ASC:DACSE,
address = {Munich, Germany},
author = {Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks}},
year = {2017}
}
@inproceedings{Han:2017:BinauralASC:DCASE,
author = {Han, Yoonchang and Park, Jeongsoo and Lee, Kyogu},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Park, Lee - 2017 - CONVOLUTIONAL NEURAL NETWORKS WITH BINAURAL REPRESENTATIONS AND BACKGROUND SUBTRACTION FOR ACOUSTIC SCENE CLA(2).pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Green:2017:SpatialFeaturesASC:DCASE,
abstract = {Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or bin-aural datasets. This paper introduces EigenScape -a new dataset of 4th-order Ambisonic acoustic scene recordings -and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient -Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Direc-tional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The dif-ferences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.},
address = {Munich, Germany},
author = {Green, Marc C. and Murphy, Damian},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using Spatial Features}},
year = {2017}
}
@article{Rolnick2019,
abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
archivePrefix = {arXiv},
arxivId = {1906.05433},
author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
eprint = {1906.05433},
file = {:C$\backslash$:/Users/abr/Downloads/1906.05433.pdf:pdf},
title = {{Tackling Climate Change with Machine Learning}},
xurl = {http://arxiv.org/abs/1906.05433},
year = {2019}
}
@inproceedings{Wang:2018:SelfDeterminationASC:APSIPA,
address = {Malaysia},
author = {Wang, Chien-Yao and Santoso, Andri and Wang, Jia-Ching},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
xdoi = {10.1109/APSIPA.2017.8281995},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Santoso, Wang - 2018 - Acoustic Scene Classification using Self-Determination Convolutional Neural Network(2).pdf:pdf},
isbn = {9781538615423},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {19--22},
title = {{Acoustic Scene Classification using Self-Determination Convolutional Neural Network}},
year = {2018}
}
@article{Xu:2018:ASCMobileNet:ISM,
author = {Xu, Jun-Xiang and Lin, Tzu-Ching and Yu, Tsai-Ching and Tai, Tzu-Chiang and Chang, Pao-Chi},
xdoi = {10.1109/ISM.2018.00038},
file = {::},
isbn = {9781538668573},
journal = {Proceedings of the IEEE International Symposium on Multimedia (ISM)},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {267--270},
title = {{Acoustic Scene Classification Using Reduced MobileNet Architecture}},
year = {2018}
}
@inproceedings{Phaye:2019:Subspectralnet:ICASSP,
address = {Brighton, UK},
author = {Phaye, Sai Samarth R and Benetos, Emmanouil and Wang, Ye},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {::},
isbn = {9781538646588},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {825--829},
title = {{Subspectralnet - Using Sub-Spectrogram based Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Imoto:2017:ASC:EUSIPCO,
address = {Kos Island, Greece},
author = {Imoto, Keisuke and Ono, Nobutaka},
booktitle = {Proceedings of the 25th European Signal Processing Conference (EUSIPCO)},
xdoi = {10.23919/EUSIPCO.2017.8081616},
file = {::},
isbn = {9780992862671},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {2279--2283},
title = {{Acoustic Scene Classification Based on Generative Model of Acoustic Spatial Words for Distributed Microphone Array}},
year = {2017}
}
@inproceedings{Basbug:2019:SpatialPyramidPoolingASC:ICSC,
address = {Newport, CA, USA, 30 January - 1 February},
author = {Basbug, Ahmet Melih and Sert, Mustafa},
booktitle = {Proceedings of the 13th IEEE International Conference on Semantic Computing (ICSC),},
xdoi = {10.1109/ICOSC.2019.8665547},
file = {::},
isbn = {9781538667835},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {128--131},
title = {{Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks}},
year = {2019}
}
@inproceedings{Dang:2018:ASCMulti:ICCE,
address = {Hue City, Vietnam},
author = {Dang, An and Vu, Toan H. and Wang, Jia-Ching},
booktitle = {Proceedings of the IEEE International Conference on Consumer Electronics (ICCE)},
xdoi = {10.1109/ICCE.2018.8326315},
file = {::},
isbn = {9781538630259},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
publisher = {IEEE},
title = {{Acoustic Scene Classification using Convolutional Neural Networks and Multi-Scale Multi-Feature Extraction}},
year = {2018}
}
@inproceedings{Nwe:2017:MultiTaskASC:APSIPA,
abstract = {Deep Neural Network (DNN) with Multi-Task Learning (MTL) methods have recently demonstrated significant performance gains on a number of classification, detection, recognition tasks compared to conventional DNN. DNN with MTL framework involves cross-task and within-task knowledge sharing layers. MTL methods have benefit for regularization effect from the cross-task knowledge sharing layers. And, within- task knowledge sharing layers allow MTL based DNN to learn information to optimize the performance for individual task. We formulate our acoustic scene classification in MTL framework using Convolutional Neural Network to learn information specific to different types of environment. We conduct experiments using DCASE2016 dataset. Proposed approach achieves 83.8{\%} accuracy to classify 15 acoustic scene classes.},
address = {Malaysia},
author = {Nwe, Tin Lay and Dat, Tran Huy and Ma, Bin},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
xdoi = {10.1109/APSIPA.2017.8282241},
file = {::},
isbn = {9781538615423},
pages = {1347--1350},
title = {{Convolutional Neural Network with Multi-Task Learning Scheme for Acoustic Scene Classification}},
year = {2018}
}
@article{Mesaros:2016:ASC:IEEE_TASLP,
abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events DCASE 2016 has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: Acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
author = {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
xdoi = {10.1109/TASLP.2017.2778423},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesaros et al. - 2018 - Detection and Classification of Acoustic Scenes and Events Outcome of the DCASE 2016 Challenge.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Acoustic scene classification,audio datasets,pattern recognition,sound event detection},
number = {2},
pages = {379--393},
publisher = {IEEE},
title = {{Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge}},
volume = {26},
year = {2018}
}
@inproceedings{Wu:2019:SoundTexture:ICASSP,
address = {Brighton, UK},
archivePrefix = {arXiv},
arxivId = {1901.01502},
author = {Wu, Yuzhong and Lee, Tan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683490},
eprint = {1901.01502},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Lee - 2019 - Enhancing Sound Texture in CNN-based Acoustic Scene Classification.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {815--819},
title = {{Enhancing Sound Texture in CNN-based Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Amiriparian:2018:GANASC:EUSIPCO,
abstract = {Unsupervised representation learning shows high promise for generating robust features for acoustic scene analysis. In this regard, we propose and investigate a novel combination of features learnt using both a deep convolutional generative adversarial network (DCGAN) and a recurrent sequence to sequence autoencoder (S2SAE). Each of the representation learning algorithms is trained individually on spectral features extracted from audio instances. The learnt representations are: (i) the activations of the discriminator in case of the DCGAN, and (ii) the activations of a fully connected layer between the decoder and encoder units in case of the S2SAE. We then train two multilayer perceptron neural networks on the DCGAN and S2SAE feature vectors to predict the class labels. The individual predicted labels are combined in a weighted decision-level fusion to achieve the final prediction. The system is evaluated on the development partition of the acoustic scene classification data set of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). In comparison to the baseline, the accuracy is increased from 74.8 {\%} to 86.4 {\%} using only the DCGAN, to 88.5 {\%} on the development set using only the S2SAE, and to 91.1 {\%} after fusion of the individual predictions.},
address = {Rome, Italy},
author = {Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Gerczuk, Maurice and Pugachevskiy, Sergey and Schuller, Bj{\"{o}}rn},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
xdoi = {10.23919/EUSIPCO.2018.8553225},
file = {::},
isbn = {9789082797015},
issn = {22195491},
keywords = {Acoustic scene classification,Generative adversarial networks,Sequence to sequence autoencoders,Unsupervised feature learning},
pages = {977--981},
title = {{A Fusion of Deep Convolutional Generative Adversarial Networks and Sequence to Sequence Autoencoders for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Li:2019:MultilevelAttention:ICMEW,
abstract = {Acoustic scene classification (ASC) refers to the classification of audio into one of predefined classes that characterize the environment. People are used to combine log-mel filterbank features with convolutional neural network (CNN) to build ASC system. In this paper, we explore the use of deep scattering spectrum (DSS) features combined with a multi-level attention model based on CNN for ASC tasks. First, the time scatter and frequency scatter coefficients of DSS with different resolutions are explored as ASC features. Second, we incorporate a multi-level attention model into CNN to build the classification system. We then evaluate the proposed approach on the IEEE challenge of detection and classification of acoustic scenes and events 2018 (DCASE 2018) dataset. Results show that the DSS features provide between a 11{\%}-14{\%} relative improvement in accuracy over log-mel features, within a state-of-the-art framework. The application of multilevel attention model on CNN can improve the accuracy by nearly 5{\%}. The highest accuracy of our proposed system is 78.3{\%} on the development set.},
address = {Shanghai, China},
author = {Li, Zhitong and Hou, Yuanbo and Xie, Xiang and Li, Shengchen and Zhang, Liqiang and Du, Shixuan and Liu, Wei},
booktitle = {Proceedings of the IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
xdoi = {10.1109/ICMEW.2019.00074},
file = {::},
isbn = {9781538692141},
keywords = {Acoustic scene classification,DCASE 2018,Deep scattering spectrum,Multi-level attention mechanism,acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {396--401},
title = {{Multi-Level Attention Model with Deep Scattering Spectrum for Acoustic Scene Classification}},
year = {2019}
}
@article{Melih,
author = {Melih, Ahmet and Bu, B A Ş},
file = {::},
isbn = {9781509064946},
keywords = {00,17,2019 ieee,31,978-1-5090-6494-6,acoustic scene classification,convolutional neural,gated recurrent units,long short term memory,network},
pages = {2--5},
title = {{Akustik Sahne S ı n ı fland ı rma i{\c{c}}in Derin {\"{O}} ğ renme Modellerinin Analizi Analysis of Deep Neural Network Models for Acoustic Scene Classification}}
}
@inproceedings{Bisot:2015:ASC:EUSIPCO,
abstract = {Acoustic scene classification is a difficult problem mostly due to the high density of events concurrently occurring in audio scenes. In order to capture the occurrences of these events we propose to use the Subband Power Distribution (SPD) as a feature. We extract it by computing the histogram of amplitude values in each frequency band of a spectrogram image. The SPD allows us to model the density of events in each frequency band. Our method is evaluated on a large acoustic scene dataset using support vector machines. We outperform the previous methods when using the SPD in conjunction with the histogram of gradients. To reach further improvement, we also consider the use of an approximation of the earth mover's distance kernel to compare histograms in a more suitable way. Using the so-called Sinkhorn kernel improves the results on most of the feature configurations. Best performances reach a 92.8{\%} F1 score.},
address = {Nice, France},
author = {Bisot, Victor and Essid, Slim and Richard, Gael},
booktitle = {Proceedings of the 23rd European Signal Processing Conference (EUSIPCO)},
xdoi = {10.1109/EUSIPCO.2015.7362477},
file = {::},
isbn = {9780992862633},
keywords = {Acoustic scene classification,Sinkhorn distance,acoustic{\_}scene{\_}classification,machine{\_}listening,subband power distribution image,support vector machine},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {719--723},
title = {{HOG and Subband Power Distribution Image Features for Acoustic Scene Classification}},
year = {2015}
}
@inproceedings{Abidin:2017:LBP:ICASSP,
address = {New Orleans, LA, USA},
author = {Abidin, Shamsiah and Togneri, Roberto and Sohel, Ferdous},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781509041176},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {626--630},
title = {{Enhanced LBP Texture Features from Time Frequency Representations for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Zielinski:2018:BinauralASC:FEDCSIS,
abstract = {Binaural technology becomes increasingly popular in the multimedia systems. This paper identifies a set of features of binaural recordings suitable for the automatic classification of the four basic spatial audio scenes representing the most typical patterns of audio content distribution around a listener. Moreover, it compares the five artificial-intelligence-based methods applied to the classification of binaural recordings. The results show that both the spatial and the spectro-temporal features are essential to accurate classification of binaurally rendered acoustic scenes. The spectro-temporal features appear to have a stronger influence on the classification results than the spatial metrics. According to the obtained results, the method based on the support vector machine, exploiting the features identified in the study, yields the classification accuracy approaching 84{\%}.},
address = {Pozna{\'{n}}, Poland},
author = {Zieli{\'{n}}ski, S{\l}awomir K. and Lee, Hyunkook},
booktitle = {Proceedings of the Federated Conference on Computer Science and Information Systems (FedCSIS)},
xdoi = {10.15439/2018F182},
file = {::},
isbn = {9788394941970},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {585--588},
title = {{Feature Extraction of Binaural Recordings for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Paseddula:2018:ASC:ICIIS,
abstract = {Herein, we propose an Acoustic Scene Classification (ASC) based on Deep Neural Networks (DNN). The design of Mel-filer bank helps in capturing the acoustic scene characteristics in the low-frequency regions during MFCC extraction. In this paper, inverse MFCC are used as interdependent to structure of Mel filter bank. We can effectively capture the acoustic information in the total audio frequency range using MFCC and IMFCC features. An experiment is carried on Tampere University of Technology (TUT) Acoustic Scenes 2017 Dataset. DNN architecture at utterance level classification with supervised learning is adopted. Scores from the DNN models corresponding to MFCC and IMFCC features are combined for testing the model. The relative improvement of 5.22{\%} wih respect to baseline system is achieved by the proposed system on setup of 4-fold cross-validation. We participated in the DCASE 2017 challenge for ASC task, also got 45.9{\%} accuracy on given evaluation dataset. This approach got 76th rank out of 97 submissions.},
address = {Ropar, India},
author = {Paseddula, Chandrasekhar and Gangashetty, Suryakanth V.},
booktitle = {Proceedings of the 13th IEEE International Conference on Industrial and Information Systems (ICIIS)},
xdoi = {10.1109/ICIINFS.2018.8721379},
file = {::},
isbn = {9781538616765},
keywords = {Acoustic Scene Classification,Adaptive Moments,Deep Neural Network,Discrete Cosine Transform,Inverse MFCC,MFCC,Multilayer Perceptron Model,acoustic{\_}event{\_}detection,machine{\_}listening},
mendeley-tags = {acoustic{\_}event{\_}detection,machine{\_}listening},
pages = {18--21},
title = {{DNN based Acoustic Scene Classification using Score Fusion of MFCC and Inverse MFCC}},
year = {2018}
}
@inproceedings{Wang:2017:ASC:ISCE,
address = {Kuala Lumpur, Malaysia},
author = {Wang, Chien-Yao and Wang, Jia-Ching and Wu, Yu-Chi and Chang, Pao-Chi},
booktitle = {Proceedings of the IEEE International Symposium on Consumer Electronics (ISCE)},
file = {::},
isbn = {9781538621899},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {11--12},
title = {{Asymmetric Kernel Convolution Neural Networks for Acoustic Scenes Classification}},
year = {2017}
}
@inproceedings{Mun:2017:ASC:ICASSP,
abstract = {Deep Neural Network (DNN) based transfer learning has been shown to be effective in Visual Object Classification (VOC) for complementing the deficit of target domain training samples by adapting classifiers that have been pre- trained for other large-scaled DataBase (DB). Although there exists an abundance of acoustic data, it can also be said that datasets of specific acoustic scenes are sparse for training Acoustic Scene Classification (ASC) models. By exploiting VOC DNN‟s ability of learning beyond its pre- trained environments, this paper proposes DNN based transfer learning for ASC. Effectiveness of the proposed method is demonstrated on the database of IEEE DCASE Challenge 2016 Task 1 and home surveillance environment via representative experiments. Its improved performance is verified by comparing it to prominent conventional methods.},
address = {New Orleans, LA, USA},
author = {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Han, David K. and Ko, Hanseok},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1097/IOP.0000000000000348},
file = {::},
isbn = {9781509041176},
issn = {15372677},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {796--800},
title = {{Deep Neural Network Based Learning and Transferring Mid-Level Audio Features for Acoustic Scene Classification}},
year = {2017}
}
@inproceedings{Prakruthi:2018:ASC:ICISC,
abstract = {Acoustic Scene Classification (ASC) has become an integral component in applications such as smart hearing AIDS, user alert applications for physically challenged persons and robot based navigation. The Deep Learning (DL) techniques such as Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) etc. improve the accuracy and efficiency of ASC but increase computational complexity. This paper has explored the possibility of using neural network as classifiers for ASC. The observations show that an appropriately trained simple neural network can achieve similar performance as DL techniques. The developed model utilizes Mel Frequency Cepstral Coefficients (MFCC) for feature extraction and has been verified on TUT Acoustic Scenes 2016 dataset. The developed model has attained 14.3{\%} better accuracy than existing DL models for frame based analysis.},
author = {Prakruthi, U. S. and Kiran, Divya and Ramasangu, Hariharan},
booktitle = {Proceedings of the 2nd International Conference on Inventive Systems and Control (ICISC)},
xdoi = {10.1109/ICISC.2018.8398905},
file = {::},
isbn = {9781538608074},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {781--784},
publisher = {IEEE},
title = {{High Performance Neural Network based Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Mun:2019:DomainMismatch:ICASSP,
abstract = {In recent acoustic scene classification (ASC) research field, training and test device channel mismatch have become an issue for the real world implementation. To address the issue, this paper proposes a channel domain conversion using factor-ized hierarchical variational autoencoder. Proposed method adapts both the source and target domain to a pre-defined specific domain. Unlike the conventional approach, the relationship between the target and source domain and information of each domain are not required in the adaptation process. Based on the experimental results using the IEEE Detection and Classification of Acoustic Scenes and Event 2018 task 1-B dataset and the baseline system, it is shown that the proposed approach can mitigate the channel mismatching issue of different recording devices.},
address = {Brighton, UK},
author = {Mun, Seongkyu and Shon, Suwon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683514},
file = {::},
isbn = {9781479981311},
issn = {15206149},
keywords = {acoustic scene classification,acoustic{\_}scene{\_}classification,domain adaptation,factorized hierarchical variational autoencoder,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {845--849},
title = {{Domain Mismatch Robust Acoustic Scene Classification Using Channel Information Conversion}},
year = {2019}
}
@inproceedings{Takahashi:2017:ASC:APSIPA,
abstract = {We previously proposed a method of acoustic scene classification using a deep neural network-Gaussian mixture model (DNN-GMM) and frame-concatenated acoustic features. It was submitted to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 Challenge and was ranked eighth among 49 algorithms. In the proposed method, acoustic features in temporally distant frames were concatenated to capture their temporal relationship. The experimental results indicated that the classification accuracy is improved by increasing the number of concatenated frames. On the other hand, the frame concatenation interval, which is the interval with which the frames used for frame concatenation are selected, is another important parameter. In our previous method, the frame concatenation interval was fixed to 100 ms. In this paper, we optimize the number of concatenated frames and the frame concatenation interval for the previously proposed method. As a result, it was confirmed that the classification accuracy of the method was improved by 2.61{\%} in comparison with the result submitted to the DCASE 2016.},
author = {Takahashi, Gen and Yamada, Takeshi and Ono, Nobutaka and Makino, Shoji},
booktitle = {Proceedings of the 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
xdoi = {10.1109/APSIPA.2017.8282314},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takahashi et al. - 2018 - Performance Evaluation of Acoustic Scene Classification using DNN-GMM and Frame-Concatenated Acoustic Features.pdf:pdf},
isbn = {9781538615423},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {1739--1743},
title = {{Performance Evaluation of Acoustic Scene Classification using DNN-GMM and Frame-Concatenated Acoustic Features}},
year = {2018}
}
@inproceedings{Singh:2018:EnsembleASC:EUSIPCO,
abstract = {Scene classification based on acoustic information is a challenging task due to various factors such as the non-stationary nature of the environment and multiple overlapping acoustic events. In this paper, we address the acoustic scene classification problem using SoundNet, a deep convolution neural network, pre-trained on raw audio signals. We propose a classification strategy by combining scores from each layer. This is based on the hypothesis that layers of the deep convolutional network learn complementary information and combining this layer-wise information provides better classification than the features extracted from an individual layer. In addition, we also propose a pooling strategy to reduce the dimensionality of features extracted from different layers of SoundNet. Our experiments on DCASE 2016 acoustic scene classification dataset reveals the effectiveness of this layer-wise ensemble approach. The proposed approach provides a relative improvement of approx. 30.85{\%} over the classification accuracy provided by the best individual layer of SoundNet.},
address = {Rome, Italy},
author = {Singh, Arshdeep and Thakur, Anshul and Rajan, Padmanabhan and Bhavsar, Arnav},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
xdoi = {10.23919/EUSIPCO.2018.8553052},
file = {::},
isbn = {9789082797015},
issn = {22195491},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {837--841},
title = {{A Layer-Wise Score Level Ensemble Framework for Acoustic Scene Detection}},
year = {2018}
}
@inproceedings{Li:2018:ASC:ICALIP,
abstract = {Although acoustic scene classification has been received great attention from researchers in the field of audio signal processing, it is still a challenging and unsolved task to date. In this paper, we present our work of acoustic scene classification for the challenge of the Detection and Classification of Acoustic Scenes and Events 2017, i.e., DCASE2017 challenge, using a feature of Deep Audio Feature (DAF) for acoustic scene representation and a classifier of Bidirectional Long Short Term Memory (BLSTM) network for acoustic scene classification. We first use a deep neural network to generate the DAF from Mel frequency cepstral coefficients, and then adopt a network of BLSTM fed by the DAF for acoustic scene classification. When evaluated on the official datasets of the DCASE2017 challenge, the proposed system outperforms the baseline system in terms of classification accuracy.},
address = {Copenhagen, Denmark},
author = {Li, Yanxiong and Li, Xianku and Zhang, Yuhan and Wang, Wucheng and Liu, Mingle and Feng, Xiaohui},
booktitle = {Proceedings of the 6th International Conference on Audio, Language and Image Processing (ICALIP)},
xdoi = {10.1109/ICALIP.2018.8455765},
file = {::},
isbn = {9781538651957},
keywords = {acoustic scene classification,acoustic{\_}scene{\_}classification,bidirectional long short term memory network,deep audio feature,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {371--374},
title = {{Acoustic Scene Classification Using Deep Audio Feature and BLSTM Network}},
year = {2018}
}
@inproceedings{Paseddula:2018:ASC:ICIIS,
abstract = {In this paper, we propose a feature set by concatenating Mel-Frequency Cepstral Coefficients (MFCC) and Spectral Centroid Magnitude Coefficients (SCMC) features for Acoustic Scene Classification (ASC) using Deep Neural Networks (DNN). MFCC features are used to hold the acoustic characteristics such as spectral envelope of an acoustic scene in each frame. It also carries the sub-band average energy as a single dimension. SCMC features are used to hold the distribution of energy in a sub-band effectively. A test is carried out on Tampere University of Technology (TUT) Acoustic Scenes 2017 Dataset. The DNN architecture for utterance level classification has been used. The proposed system's performance on a 4-fold cross-validation setup is 80.2{\%} and it gives 5.4{\%} relative improvement in performance when compared to the baseline system that uses log-Mel band energies with Multi-Layer Perceptron model.},
author = {Paseddula, Chandrasekhar and Gangashetty, Suryakanth V.},
booktitle = {Proceedings of the 13th International Conference on Industrial and Information Systems (ICIIS)},
xdoi = {10.1109/ICIINFS.2018.8721416},
file = {::},
isbn = {9781538616765},
keywords = {Acoustic Scene Classification,Adaptive Moments,Deep Neural Network,Discrete Cosine Transform,Mel-Frequency Cepstral Coefficients,Multilayer Perceptron Model,Spectral Centroid Magnitude Coefficients,acoust,machine{\_}listening},
mendeley-tags = {acoust,machine{\_}listening},
pages = {13--17},
title = {{Input Fusion of MFCC and SCMC Features for Acoustic Scene Classification using DNN}},
year = {2018}
}
@inproceedings{Mesaros:2017:HumanASC:WASPAA,
abstract = {Human and machine performance in acoustic scene classification is examined through a parallel experiment using TUT Acoustic Scenes 2016 dataset. The machine learning perspective is presented based on the systems submitted for the 2016 challenge on Detection and Classification of Acoustic Scenes and Events. The human performance, assessed through a listening experiment, was found to be significantly lower than machine performance. Test subjects exhibited different behavior throughout the experiment, leading to significant differences in performance between groups of subjects. An expert listener trained for the task obtained similar accuracy to the average of submitted systems, comparable also to previous studies of human abilities in recognizing everyday acoustic scenes.},
address = {New Paltz, NY, USA},
author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
xdoi = {10.1109/WASPAA.2017.8170047},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesaros, Heittola, Virtanen - 2017 - Assessment of Human and Machine Performance in Acoustic Scene Classification DCASE 2016 Case Study.pdf:pdf},
isbn = {9781538616321},
keywords = {acoustic scene classification,acoustic{\_}scene{\_}classification,human performance,listening experiment,machine learning,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {319--323},
title = {{Assessment of Human and Machine Performance in Acoustic Scene Classification: DCASE 2016 Case Study}},
year = {2017}
}
@article{Gharib:2019:ASCCompetition:MLSP,
archivePrefix = {arXiv},
arxivId = {1808.02357},
author = {Gharib, Shayan and Derrar, Honain and Niizumi, Daisuke and Senttula, Tuukka and Tommola, Janne and Heittola, Toni and Virtanen, Tuomas and Huttunen, Heikki},
xdoi = {10.1109/MLSP.2018.8517000},
eprint = {1808.02357},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/maybe/1808.02357.pdf:pdf},
isbn = {9781538654774},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Acoustic Scene Classification,DCASE,Data Augmentation,Kaggle,acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification: A Competition Review}},
year = {2018}
}
@inproceedings{Roletscheck:2019:EvolutionaryASC:DCASE,
address = {New York, NY, USA},
author = {Roletscheck, Christian and Watzka, Tobias and Seiderer, Andreas and Schiller, Dominik and Andr{\'{e}}, Elisabeth},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Roletscheck{\_}137.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Using an Evolutionary Approach To Explore Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Mesaros:2018:MultiDeviceDataset:DCASE,
address = {Surrey, UK},
author = {Mesaros, Annemaria and Heittola, Toni and {Tuomas Virtanen}},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Mesaros{\_}8.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{A Multi-Device Dataset for Urban Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Cho:DCASE:LargeMarginCNN:DCASE,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1910.06784},
author = {Cho, Janghoon and Yun, Sungrack and Park, Hyoungwoo and Eum, Jungyun and Hwang, Kyuwoong},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.33682/8xh4-jm46},
eprint = {1910.06784},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2019Workshop{\_}Cho{\_}69.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {45--49},
title = {{Acoustic Scene Classification Based on a Large-Margin Factorized CNN}},
year = {2019}
}
@inproceedings{Nguyen:2018:ASCEnsemble:DCASE,
address = {Surrey, UK},
author = {Nguyen, Truc and Pernkopf, Franz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Nguyen{\_}60.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using a Convolutional Neural Network Ensemble and Nearest Neighbor Filters}},
year = {2018}
}
@inproceedings{Huang:2019:ASCEnsemble:DCASE,
address = {New York, NY, USA},
author = {Huang, Jonathan and Lu, Hong and Lopez-Meyer, Paulo and Maruri, Hector A. Cordourier and Ontiveros, Juan A. del Hoyo},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2019 - Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {94--98},
title = {{Acoustic Scene Classification using Deep Learning-Based Ensemble Averaging}},
year = {2019}
}
@inproceedings{Zeinali:2018:XVektorEmbeddings:DCASE,
address = {Surrey, UK},
author = {Zeinali, Hossein and Burget, Luk{\'{a}}s and Cernocky, Jan},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Zeinali{\_}149.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Convolutional Neural Networks and X-Vector Embeddings for DCASE2018 Acoustic Scene Classification Challenge}},
year = {2018}
}
@inproceedings{Mesaros:2019:ClosedOpenSet:DCASE,
address = {New York, NY, USA},
author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.33682/m5kp-fa97},
file = {::},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {164--168},
title = {{Acoustic Scene Classification in DCASE 2019 Challenge:Closed and Open Set Classification and Data Mismatch Setups}},
year = {2019}
}
@inproceedings{Singh:2019:MultiViewFeatures:DCASE,
address = {New Paltz, NY, USA},
author = {Singh, Arshdeep and Rajan, Padmanabhan and Bhavsar, Arnav},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Rajan, Bhavsar - 2019 - Deep Multi-View Features from Raw Audio for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {229--233},
title = {{Deep Multi-View Features from Raw Audio for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Yang:2018:MultiScaleFeatures:DCASE,
address = {Surrey, UK},
author = {Yang, Liping and Chen, Xinxing and Tao, Lianjie},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Liping{\_}37.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Acoustic Scene Classification using Multi-Scale Features}},
year = {2018}
}
@inproceedings{Koutini:2019:ReceptiveField:DCASE,
address = {New York, NY, USA},
author = {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koutini, Eghbal-zadeh, Widmer - 2019 - Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {124--128},
title = {{Receptive-Field-Regularized CNN Variants for Acoustic Scene Classification}},
year = {2019}
}
@inproceedings{Saki:2019:OpenSetASC:DCASE,
address = {New York, NY, USA},
author = {Saki, Fatemeh and Guo, Yinyi and Hung, Cheng-Yu},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2019Workshop{\_}Saki{\_}77.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {219--223},
title = {{Open-Set Evolving Acoustic Scene Classification System}},
year = {2019}
}
@inproceedings{Ren:2018:AttentionASC:DCASE,
address = {Surrey, UK},
author = {Ren, Zhao and Kong, Qiuqiang and Qian, Kun and Plumbley, Mark D. and Schuller, Bj{\"{o}}rn W.},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Ren{\_}67.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Attention-based Convolutional Neural Networks for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Mariotti:2018:DeepVisionASC:DCASE,
address = {Surrey, UK},
author = {Mariotti, Octave and Cord, Matthieu and Schwander, Olivier},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Mariotti{\_}123.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Exploring Deep Vision Models for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Maka:2018:FeatureSpaceASC:DCASE,
address = {Surrey, UK},
author = {Maka, Tomasz},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Maka{\_}127.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{Audio Feature Space Analysis for Acoustic Scene Classification}},
year = {2018}
}
@inproceedings{Wilkinghoff:2019:OpenSetASC:DCASE,
address = {New York, NY, USA},
author = {Wilkinghoff, Kevin and {Frank Kurth}},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2019Workshop{\_}Wilkinghoff{\_}12.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {258--262},
title = {{Open-Set Acoustic Scene Classification with Deep Convolutional Autoencoders}},
year = {2019}
}
@inproceedings{Jung:2018:FeatureEnsembleASC:DCASE,
address = {Surrey, UK},
author = {Jung, Jee-weon and Heo, Hee-soo and Shim, Hye-jin and Yu, Ha-jin},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2018Workshop{\_}Jung{\_}128.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
title = {{DNN based Multi-Level Feature Ensemble for Acoustic Scene Detection}},
year = {2018}
}
@inproceedings{Gharib:2018:DomainAdaptationASC:DCASE,
address = {Surrey, UK},
author = {Gharib, Shayan and Drossos, Konstantinos and Emre, Cakir and Serdyuk, Dmitriy and Virtanen, Tuomas},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gharib, Drossos, Emre - 2018 - UNSUPERVISED ADVERSARIAL DOMAIN ADAPTATION FOR ACOUSTIC SCENE CLASSIFICATION Audio Research Group , Lab ..pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
mendeley-tags = {acoustic{\_}scene{\_}classification,domain{\_}adaptation},
title = {{Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification}},
year = {2018}
}
@article{Park:2019:SpecAugment:INTERSPEECH,
abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8{\%} WER on test-other without the use of a language model, and 5.8{\%} WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5{\%} WER. For Switchboard, we achieve 7.2{\%}/14.6{\%} on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8{\%}/14.1{\%} with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3{\%}/17.3{\%} WER.},
archivePrefix = {arXiv},
arxivId = {1904.08779},
author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
xdoi = {10.21437/Interspeech.2019-2680},
eprint = {1904.08779},
file = {:C$\backslash$:/Users/abr/Downloads/1904.08779.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Data augmentation,End-to-end speech recognition},
pages = {2613--2617},
title = {{Specaugment: A simple data augmentation method for automatic speech recognition}},
volume = {2019-Septe},
year = {2019}
}
@article{Bear:2019:JointASCAED:INTERSPEECH,
abstract = {Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two separate tasks in the field of computational sound scene analysis. In this work, we present a new dataset with both sound scene and sound event labels and use this to demonstrate a novel method for jointly classifying sound scenes and recognizing sound events. We show that by taking a joint approach, learning is more efficient and whilst improvements are still needed for sound event detection, SED results are robust in a dataset where the sample distribution is skewed towards sound scenes.},
archivePrefix = {arXiv},
arxivId = {1904.10408},
author = {Bear, Helen L. and Nolasco, In{\^{e}}s and Benetos, Emmanouil},
xdoi = {10.21437/Interspeech.2019-2169},
eprint = {1904.10408},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bear, Nolasco, Benetos - 2019 - Towards joint sound scene and polyphonic sound event recognition.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Acoustic scene classification,CRNN,Computational sound scene analysis,Sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification},
pages = {4594--4598},
title = {{Towards joint sound scene and polyphonic sound event recognition}},
volume = {2019-Septe},
year = {2019}
}
@techreport{Whiteley,
abstract = {This paper presents a probabilistic model of temporal structure in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model naturally quantifies these three musical concepts in terms of hidden state-variables, allowing resolution of otherwise apparent ambiguities in musical structure. At the heart of the system is a probabilistic model of a hypothetical 'bar-pointer' which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates different input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior probability distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo.},
author = {Whiteley, Nick and Cemgil, A Taylan and Godsill, Simon},
file = {::},
keywords = {Bayesian inference,meter recog-nition,rhythm recognition,tempo tracking},
title = {{Bayesian Modelling of Temporal Structure in Musical Audio}}
}
@techreport{Srinivasamurthy,
abstract = {Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music.},
author = {Srinivasamurthy, Ajay and Holzapfel, Andre and {Taylan Cemgil}, Ali and Serra, Xavier},
file = {::},
title = {{PARTICLE FILTERS FOR EFFICIENT METER TRACKING WITH DYNAMIC BAYESIAN NETWORKS}}
}
@article{Pham:2019:AcousticScene:INTERSPEECH,
abstract = {Acoustic scene classification (ASC) using front-end time-frequency features and back-end neural network classifiers has demonstrated good performance in recent years. However a profusion of systems has arisen to suit different tasks and datasets, utilising different feature and classifier types. This paper aims at a robust framework that can explore and utilise a range of different time-frequency features and neural networks, either singly or merged, to achieve good classification performance. In particular, we exploit three different types of front-end time-frequency feature; log energy Mel filter, Gammatone filter and constant Q transform. At the back-end we evaluate effective a two-stage model that exploits a Convolutional Neural Network for pre-trained feature extraction, followed by Deep Neural Network classifiers as a post-trained feature adaptation model and classifier. We also explore the use of a data augmentation technique for these features that effectively generates a variety of intermediate data, reinforcing model learning abilities, particularly for marginal cases. We assess performance on the DCASE2016 dataset, demonstrating good classification accuracies exceeding 90{\%}, significantly outperforming the DCASE2016 baseline and highly competitive compared to state-of-the-art systems.},
author = {Pham, Lam and McLoughlin, Ian and Phan, Huy and Palaniappan, Ramaswamy},
xdoi = {10.21437/Interspeech.2019-1841},
file = {:C$\backslash$:/Users/abr/Desktop/NEW{\_}PAPERS/5dc03975a6fdcc2128011ee7.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Acoustic scene classification,Constant Q transform,Convolutional neural network,Deep neural network,Gammatone filter,Log-Mel,Machine hearing,Spectrogram,acoustic{\_}scene{\_}classification},
mendeley-tags = {acoustic{\_}scene{\_}classification},
number = {September},
pages = {3634--3638},
title = {{A robust framework for acoustic scene classification}},
volume = {2019-Septe},
year = {2019}
}
@techreport{Bock,
abstract = {We propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolu-tional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy.},
author = {B{\"{o}}ck, Sebastian and Davies, Matthew E P and Knees, Peter},
file = {::},
title = {{MULTI-TASK LEARNING OF TEMPO AND BEAT: LEARNING ONE TO IMPROVE THE OTHER}}
}
@article{Rasmus:2015:LadderNetworks,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02672v2},
author = {Rasmus, Antti and Valpola, Harri and Berglund, Mathias},
eprint = {arXiv:1507.02672v2},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmus, Valpola, Berglund - 2015 - Semi-Supervised Learning with Ladder Networks.pdf:pdf},
title = {{Semi-Supervised Learning with Ladder Networks}},
year = {2015}
}
@article{Manilow2019,
abstract = {We present a single deep learning architecture that can both separate an audio recording of a musical mixture into constituent single-instrument recordings and transcribe these instruments into a human-readable format at the same time, learning a shared musical representation for both tasks. This novel architecture, which we call Cerberus, builds on the Chimera network for source separation by adding a third "head" for transcription. By training each head with different losses, we are able to jointly learn how to separate and transcribe up to 5 instruments in our experiments with a single network. We show that the two tasks are highly complementary with one another and when learned jointly, lead to Cerberus networks that are better at both separation and transcription and generalize better to unseen mixtures.},
archivePrefix = {arXiv},
arxivId = {1910.12621},
author = {Manilow, Ethan and Seetharaman, Prem and Pardo, Bryan},
eprint = {1910.12621},
file = {::},
title = {{Simultaneous Separation and Transcription of Mixtures with Multiple Polyphonic and Percussive Instruments}},
xurl = {http://arxiv.org/abs/1910.12621},
year = {2019}
}
@article{Li2019,
abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches to address the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multimodal tasks. We benchmark two existing MIR tasks (multipitch analysis and score-informed source separation) on the dataset and compare them with other existing music audio datasets. In addition, we consider two novel multimodal MIR tasks (visually informed multipitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measurements and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.},
archivePrefix = {arXiv},
arxivId = {1612.08727},
author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
xdoi = {10.1109/TMM.2018.2856090},
eprint = {1612.08727},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
title = {{Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications}},
year = {2019}
}
@article{Kalchbrenner:2016:TCN:CORR,
abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
archivePrefix = {arXiv},
arxivId = {1610.10099},
author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and van den Oord, Aaron and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1610.10099},
file = {::},
journal = {CoRR},
title = {{Neural Machine Translation in Linear Time}},
xurl = {http://arxiv.org/abs/1610.10099},
volume = {abs/1610.1},
year = {2016}
}
@article{Sajjadi2016,
abstract = {In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.},
archivePrefix = {arXiv},
arxivId = {1606.03141},
author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
eprint = {1606.03141},
file = {::},
journal = {Ieee Icip},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {jun},
title = {{Mutual Exclusivity Loss for Semi-Supervised Deep Learning}},
xurl = {http://arxiv.org/abs/1606.03141},
year = {2016}
}
@article{Haeusser2017,
abstract = {In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unlabeled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. “Associations” are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.},
archivePrefix = {arXiv},
arxivId = {1706.00909},
author = {Haeusser, Philip and Mordvintsev, Alexander and Cremers, Daniel},
xdoi = {10.1109/CVPR.2017.74},
eprint = {1706.00909},
file = {::},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
pages = {626--635},
title = {{Learning by association a versatile semi-supervised training method for neural networks}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Turpault2019,
abstract = {Deep neural networks are particularly useful to learn relevant representations from data. Recent studies have demonstrated the potential of unsupervised representation learning for ambient sound analysis using various flavors of the triplet loss. They have compared this approach to supervised learning. However, in real situations, it is common to have a small labeled dataset and a large unlabeled one. In this paper, we combine unsupervised and supervised triplet loss based learning into a semi-supervised representation learning approach. We propose two flavors of this approach, whereby the positive samples for those triplets whose anchors are unlabeled are obtained either by applying a transformation to the anchor, or by selecting the nearest sample in the training set. We compare our approach to supervised and unsupervised representation learning as well as the ratio between the amount of labeled and unlabeled data. We evaluate all the above approaches on an audio tagging task using the DCASE 2018 Task 4 dataset, and we show the impact of this ratio on the tagging performance.},
address = {Brighton, UK},
annote = {- ambient sound analysis
- combine unsupervised and supervised triplet loss to semi-supervised triplet loss
- positive example takne from unlabeled
- for example by taking nearest sample 
- tested on DCASE2018 task 4 audio tagging
- compared to super und unsupervised versions
- two strategies for taking postive: transform acnhor or taking nearest sample in input space -{\textgreater} input space closeness should lead to embedding closeness -{\textgreater} transformation is better strategy, see results
- semi-supervised better than unsupservised
- transform better than nearset
- un{\"{o}}abeled only helpful up to a certain point
- semi-supervsied outperforms supervised baseline
- loss using different examples, not same data but unsupervised statistics -{\textgreater} npt part of classification loss, just for embedding},
author = {Turpault, Nicolas and Serizel, Romain and Vincent, Emmanuel},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683774},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {may},
pages = {760--764},
publisher = {IEEE},
title = {{Semi-supervised Triplet Loss Based Learning of Ambient Audio Embeddings}},
xurl = {https://ieeexplore.ieee.org/document/8683774/},
year = {2019}
}
@inproceedings{Diment2013,
abstract = {instrument recognition. The conventional supervised approaches normally rely on annotated data to train the classifier. This implies performing costly manual annotations of the training data. The SSL methods enable utilising the additional unannotated data, which is significantly easier to obtain, allowing the overall development cost maintained at the same level while notably improving the performance. The implemented classifier incorporates the Gaussian mixture model-based SSL scheme utilising the iterative EM-based algorithm, as well as the extensions facilitating a simpler convergence criteria. The evaluation is performed on a set of nine instruments while training on a dataset, in which the relative size of the labelled data is as little as 15{\%}. It yields a noteworthy absolute performance gain of 16{\%} compared to the performance of the initial supervised models.},
address = {Marrakech, Morocco},
annote = {* GMM
* fully supervised for showing upper limit
* first estimate model parameters with labeled data
* then apply to unlabeled with uncertainty weighting scheme
* performance gain with unlabeled data},
author = {Diment, Aleksandr and Heittola, Toni and Virtanen, Tuomas},
booktitle = {Proceedings of the 21st European Signal Processing Conference (EUSIPCO 2013)},
file = {::},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
publisher = {IEEE},
title = {{Semi-supervised learning for musical instrument recognition}},
year = {2013}
}
@inproceedings{Poria2013,
abstract = {Music genres can be seen as categorical descriptions used to classify music basing on various characteristics such as instrumentation, pitch, rhythmic structure, and harmonic contents. Automatic music genre classification is important for music retrieval in large music collections on the web. We build a classifier that learns from very few labeled examples plus a large quantity of unlabeled data, and show that our methodology outperforms existing supervised and unsupervised approaches. We also identify salient features useful for music genre classification. We achieve 97.1{\%} accuracy of 10-way classification on real-world audio collections.},
address = {Berlin, Heidelberg},
annote = {- 10 genres from western music
- add large amount of unlabeled data
- use short-time, long-time and beat features
- short: MFCC, spectral centroid, roll-off, flux, root mean square, compactness, domain zero crossing
- long: aggreate short time -{\textgreater} derivate, std dev, running mean, derivative of running mean
- beat: beat histogram, bpm, beat sum, strongest beat
- extracted with Jaudio toolkit (Java)
- fuzzy training classifier for mapping 10 class problem to 2-way or 3-way problem
- first extract features -{\textgreater} different combinations tested
- fuzzy clustering: ignore labled and create clusters according to number of classes, fuzzy means one point can be assigned to more than one label
- Mapping: identify classes, majority voting performed for each cluster by using labeled data
- hard clustering: remove fuzziness and assign each item to a class
- 97{\%} accuracy with 10 fold cv
- spectral centroid and mfcc most important (short and long term)},
author = {Poria, Soujanya and Gelbukh, Alexander and Hussain, Amir and Bandyopadhyay, Sivaji and Howard, Newton},
booktitle = {Carrasco-Ochoa J.A., Mart{\'{i}}nez-Trinidad J.F., Rodr{\'{i}}guez J.S., di Baja G.S. (eds) Pattern Recognition. MCPR 2013. Lecture Notes in Computer Science},
xdoi = {10.1007/978-3-642-38989-4_26},
file = {::},
isbn = {978-3-642-38989-4},
issn = {03029743},
keywords = {semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
pages = {254--263},
publisher = {Springer Berlin Heidelberg},
title = {{Music Genre Classification: A Semi-supervised Approach}},
xurl = {http://link.springer.com/10.1007/978-3-642-38989-4{\_}26},
volume = {7914 LNCS},
year = {2013}
}
@inproceedings{Liao2013,
abstract = {YouTube is a highly visited video sharing website where over one billion people watch six billion hours of video every month. Improving accessibility to these videos for the hearing impaired and for search and indexing purposes is an excellent application of automatic speech recognition. However, YouTube videos are extremely challenging for automatic speech recognition systems. Standard adapted Gaussian Mixture Model (GMM) based acoustic models can have word error rates above 50{\%}, making this one of the most difficult reported tasks. Since 2009, YouTube has provided automatic generation of closed captions for videos detected to have English speech; the service now supports ten different languages. This paper describes recent improvements to the original system, in particular the use of owner-uploaded video transcripts to generate additional semi-supervised training data and deep neural networks acoustic models with large state inventories. Applying an “island of confidence” filtering heuristic to select useful training segments, and increasing the model size by using 44,526 context dependent states with a low-rank final layer weight matrix approximation, improved performance by about 13{\%} relative compared to previously reported sequence trained DNN results for this task.},
author = {Liao, Hank and McDermott, Erik and Senior, Andrew},
booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding},
xdoi = {10.1109/ASRU.2013.6707758},
file = {::},
isbn = {978-1-4799-2756-2},
keywords = {Large vocabulary speech recognition,audio indexing,deep learning,deep neural networks,semi{\_}supervised{\_}learning},
mendeley-tags = {semi{\_}supervised{\_}learning},
month = {dec},
pages = {368--373},
publisher = {IEEE},
title = {{Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription}},
xurl = {http://ieeexplore.ieee.org/document/6707758/},
year = {2013}
}
@article{Iversen2008,
abstract = {The ability to perceive a musical beat (and move in synchrony with it) seems widespread, but we currently lack normative data on the distribution of this ability in musically untrained individuals. To aid in the survey of beat processing abilities in the general population, as well as to attempt to identify and differentiate impairments in beat processing, we have developed a psychophysical test called the Beat Alignment Test (BAT). The BAT is intended to complement existing tests of rhythm processing by directly examining beat perception in isolation from beat synchronization. The goals of the BAT are 1) to study the distribution of beat-based processing abilities in the normal population and 2) to provide a way to search for “rhythm deaf” individuals, who have trouble with beat processing in music though they are not tone deaf. The BAT is easily implemented and it is our hope that it is widely adopted. Data from a pilot study of 30 individuals is presented.},
author = {Iversen, John R. and Patel, Aniruddh D.},
file = {::},
isbn = {9784990420802},
journal = {Proceedings of the 10th International Conference on Music Perception and Cognition},
keywords = {[Electronic Manuscript]},
number = {Icmpc 10},
pages = {465--468},
title = {{The Beat Alignment Test (BAT): Surveying beat processing abilities in the general population}},
xurl = {http://www.nsi.edu/users/iversen/pubs/Iversen{\_}Patel{\_}2008{\_}ICMPC10{\_}BAT{\_}paper.pdf},
year = {2008}
}
@article{Goto,
author = {Goto, Masataka},
file = {::},
journal = {Ismir2002.Ismir.Net},
pages = {1--2},
title = {{ISMIR2002RWCMDBgoto.pdf}},
xurl = {http://ismir2002.ismir.net/proceedings/03-SP04-1.pdf}
}
@article{Krebs:2013:Beat:ISMIR,
abstract = {Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pat- tern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observa- tion model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic pat- terns and evaluating beat and downbeat tracking, 697 ball- room dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces oc- tave errors (detection of half or double tempo) and sub- stantially improves downbeat tracking.},
author = {Krebs, Florian and B{\"{o}}ck, Sebastian and Widmer, Gerhard},
file = {::},
journal = {Ismir},
pages = {227--232},
title = {{Rhythmic Pattern Modeling for Beat and Downbeat Tracking in Musical Audio.}},
year = {2013}
}
@article{DiGiorgi2013,
abstract = {Chords and keys are among the most exhaustive descriptors of songs. In this study we focus on chord and key sequence recognition from an audio signal, in the context of pop and rock music. The system exploits a set of novel probabilistic models that describe the relationship between different aspects of music and their temporal evolution. These models are based on a set of parameters with a musical meaning. The models include two diatonic key modes, Dorian and Mixolydian, besides major and minor modes previously considered in the literature. These four key modes are the most used in western pop and rock music. In order to provide a compact representation of the chord and key sequences, three novel time-varying harmony- based features are here introduced. Given the importance of emotion characterization in music, the three features are here related to the mood perceived in songs. The method outperforms the state-of-the-art in both chord and key recognition tasks. In order to better train our parameters, we create annotations of chords and keys for a new dataset of 62 songs from the first five Robbie Williams' albums.},
author = {{Di Giorgi}, Bruno and Zanoni, Massimiliano and Sarti, Augusto and Tubaro, Stefano},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Giorgi et al. - 2013 - Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony.pdf:pdf},
isbn = {9783800735433},
journal = {Proceedings of the 8th International Workshop on Multidimensional Systems},
pages = {145--150},
title = {{Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony}},
xurl = {http://ieeexplore.ieee.org/document/6623838/},
year = {2013}
}
@article{Gouyon2006,
abstract = {We report on the tempo induction contest organized during the International Conference on Music Information Retrieval (ISMIR 2004) held at the University Pompeu Fabra in Barcelona, Spain, in October 2004. The goal of this contest was to evaluate some state-of-the-art algorithms in the task of inducing the basic tempo (as a scalar, in beats per minute) from musical audio signals. To our knowledge, this is the first published large scale cross-validation of audio tempo induction algorithms. Participants were invited to submit algorithms to the contest organizer, in one of several allowed formats. No training data was provided. A total of 12 entries (representing the work of seven research teams) were evaluated, 11 of which are reported in this document. Results on the test set of 3199 instances were returned to the participants before they were made public. Anssi Klapuri's algorithm won the contest. This evaluation shows that tempo induction algorithms can reach over 80{\%} accuracy for music with a constant tempo, if we do not insist on finding a specific metrical level. After the competition, the algorithms and results were analyzed in order to discover general lessons for the future development of tempo induction systems. One conclusion is that robust tempo induction entails the processing of frame features rather than that of onset lists. Further, we propose a new "redundant" approach to tempo induction, inspired by knowledge of human perceptual mechanisms, which combines multiple simpler methods using a voting mechanism. Machine emulation of human tempo induction is still an open issue. Many avenues for future work in audio tempo tracking are highlighted, as for instance the definition of the best rhythmic features and the most appropriate periodicity detection method. In order to stimulate further research, the contest results, annotations, evaluation software and part of the data are available at http://ismir2004.ismir.net/ISMIR{\_}Contest.html},
author = {Gouyon, Fabien and Klapuri, Anssi and Dixon, Simon and Alonso, Miguel and Tzanetakis, George and Uhle, Christian and Cano, Pedro},
xdoi = {10.1109/TSA.2005.858509},
file = {::},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Benchmark,Evaluation,Tempo induction},
number = {5},
pages = {1832--1844},
title = {{An experimental comparison of audio tempo induction algorithms}},
volume = {14},
year = {2006}
}
@article{Davies2009,
abstract = {A fundamental research topic in music information retrieval is the automatic extraction of beat locations frommusic signals. In this paper we address the under-explored topic of beat tracking evaluation. We present a review of existing evaluation models and, given their strengths and weaknesses, we propose a new method based on a novel visualisation for beat tracking performance, the beat error histogram. To investigate the properties of evaluation methods we undertake a large scale beat tracking experiment. We conduct experiments using a new annotated test database which we make available to the research community. We demonstrate that the choice of evaluation method can have a significant impact on the relative performance of different beat tracking algorithms. On this basis we make a set of recommendations for comparative beat tracking experiments.},
author = {Davies, Matthew E. P. and Degara, Norberto and Plumbley, Mark D},
file = {::},
journal = {Audio},
number = {October},
pages = {17},
title = {{Evaluation Methods for Musical Audio Beat Tracking Algorithms}},
xurl = {http://citeseerx.ist.psu.edu/viewdoc/download?xdoi=10.1.1.152.6936{\&}rep=rep1{\&}type=pdf},
year = {2009}
}
@article{Holzapfel2012,
abstract = {In this paper, we propose a method that can identify challenging music samples for beat tracking without ground truth. Our method, motivated by the machine learning method “selective sampling”, is based on the measurement of mutual agreement between beat sequences. In calculating this mutual agreement we show the critical inﬂuence of different evaluation measures. Using our approach we demonstrate how to compile a new evaluation dataset comprised of difﬁcult excerpts for beat tracking and examine this difﬁculty in the context of perceptual and musical properties. Based on tag analysis we indicate the musical properties where future advances in beat tracking research would be most proﬁtable and where beat tracking is too difﬁcult to be attempted. Finally, we demonstrate how our mutual agreement method can be used to improve beat tracking accuracy on large music collections.},
author = {Holzapfel, Andr{\'{e}} and Davies, Matthew E P and Zapata, Jos{\'{e}} R. and Oliveira, Jo{\~{a}}o Lobato and Gouyon, Fabien},
xdoi = {10.1109/TASL.2012.2205244},
file = {::},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Beat tracking,evaluation,ground truth annotation,selective sampling},
month = {nov},
number = {9},
pages = {2539--2548},
title = {{Selective Sampling for Beat Tracking Evaluation}},
xurl = {http://ieeexplore.ieee.org/document/6220849/},
volume = {20},
year = {2012}
}
@article{Jenckel2018,
author = {Jenckel, Martin and Parkala, Sourabh Sarvotham and Bukhari, Syed Saqib and Dengel, Andreas},
xdoi = {10.5220/0006592703880393},
file = {::},
isbn = {9789897582769},
journal = {Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
keywords = {Document Analysis,Fuzzy Ground Truth,LSTM,OCR},
number = {Icpram},
pages = {388--393},
title = {{Impact of training LSTM-RNN with fuzzy ground truth}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Hestermann:2019:SelectiveHearing:ICSA,
author = {Hestermann, Simon and Lukashevich, Hanna and Sladeczek, Christoph},
booktitle = {Proceedings of the 5th International Conference on Spatial Audio (ICSA)},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Deep Neural Network Approaches for Selective Hearing based on Spatial Data Simulation}},
year = {2019}
}
@article{Hoyer:2004:NMF:JMLR,
author = {Hoyer, Patrik O},
file = {:C$\backslash$:/Users/abr/Downloads/hoyer04a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {idmt},
mendeley-tags = {idmt},
pages = {1457--1469},
title = {{Non-negative Matrix Factorization with Sparseness Constraints}},
volume = {5},
year = {2004}
}
@inproceedings{Weiss:2015:Complexity:ICASSP,
address = {Brisbane, Australia},
author = {Wei{\ss}, Christof and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/Downloads/2015{\_}WeissMueller{\_}TonalComplexity{\_}ICASSP (2).pdf:pdf},
isbn = {9781467369978},
pages = {688--692},
title = {{Tonal Complexity Features for Style Classification of Classical Music}},
year = {2015}
}
@inproceedings{Cano:2019:SelectiveHearing:MMSP,
address = {Kuala Lumpur, Malaysia},
author = {Cano, Estefan{\'{i}}a and Lukashevich, Hanna},
booktitle = {Proceedings of the IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)},
file = {:C$\backslash$:/Users/abr/Desktop/Selective{\_}Hearing(3).pdf:pdf},
isbn = {9781728118178},
keywords = {abt-md,idmt},
mendeley-tags = {abt-md,idmt},
title = {{Selective Hearing : A Machine Listening Perspective}},
year = {2019}
}
@inproceedings{Grollmisch:2019:ISA:EUSIPCO,
address = {A Coruna, Spain},
author = {Grollmisch, Sascha and Abe{\ss}er, Jakob and Liebetrau, Judith and Lukashevich, Hanna},
booktitle = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grollmisch et al. - 2019 - Sounding Industry Challenges and Datasets for Industrial Sound Analysis (ISA).pdf:pdf},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Sounding Industry: Challenges and Datasets for Industrial Sound Analysis (ISA)}},
year = {2019}
}
@inproceedings{Mimilakis:2019:CrossVersion:MML,
author = {Mimilakis, Stylianos I. and Wei{\ss}, Christof and Arifi-M{\"{u}}ller, Vlora and Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 12th International Workshop on Machine Learning and Music (MML)},
file = {::},
keywords = {idmt,opera,singing voice detection,supervised deep learning},
mendeley-tags = {idmt},
title = {{Cross-Version Singing Voice Detection in Opera Recordings : Challenges for Supervised Learning}},
year = {2019}
}
@book{Ghahramani:2004:UL:BOOK,
abstract = {Abstract We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised ... $\backslash$n},
author = {Ghahramani, Zoubin},
file = {::},
title = {{Unsupervised Learning}},
year = {2004}
}
@article{London:2019:BeatTempo:MP,
author = {London, Justin and Burger, Birgitta and Thompson, Marc and Hildreth, Molly and Wilson, Johanna and Schally, Nick and Toiviainen, Petri},
file = {::},
journal = {Music Perception},
keywords = {absolute ver-,after only a few,ambiguous,beat salience,ever,keen sense of whether,notes or drum,perceptual sharpening,rhythm,strokes we have a,sus relative tempo judgment,tempo,the},
number = {1},
pages = {26--41},
title = {{Motown, Disco, and Drumming: An Exploration of the Relationship between Beat Salience, Melodic Structure, and Perceived Tempo}},
volume = {37},
year = {2019}
}
@article{Thakur:2019:DeepMetric:JASA,
abstract = {This paper proposes multiscale convolutional neural network (CNN)-based deep metric learning for bioacoustic classification, under low training data conditions. The proposed CNN is characterized by the utilization of four different filter sizes at each level to analyze input feature maps. This multiscale nature helps in describing different bioacoustic events effectively: smaller filters help in learning the finer details of bioacoustic events, whereas, larger filters help in analyzing a larger context leading to global details. A dynamic triplet loss is employed in the proposed CNN architecture to learn a transformation from the input space to the embedding space, where classification is performed. The triplet loss helps in learning this transformation by analyzing three examples, referred to as triplets, at a time where intra-class distance is minimized while maximizing the inter-class separation by a dynamically increasing margin. The number of possible triplets increases cubically with the dataset size, making triplet loss more suitable than the softmax cross-entropy loss in low training data conditions. Experiments on three different publicly available datasets show that the proposed framework performs better than existing bioacoustic classification frameworks. Experimental results also confirm the superiority of the triplet loss over the cross-entropy loss in low training data conditions},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10713v2},
author = {Thakur, Anshul and Thapar, Daksh and Rajan, Padmanabhan and Nigam, Aditya},
xdoi = {10.1121/1.5118245},
eprint = {arXiv:1903.10713v2},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {534--547},
title = {{Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss}},
volume = {146},
year = {2019}
}
@article{Park:2019:Transformer:ARXIV,
abstract = {Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.},
archivePrefix = {arXiv},
arxivId = {1907.02698},
author = {Park, Jonggwon and Choi, Kyoyun and Jeon, Sungwook and Kim, Dokyun and Park, Jonghun},
eprint = {1907.02698},
file = {::},
journal = {ArXiV},
title = {{A Bi-directional Transformer for Musical Chord Recognition}},
xurl = {http://arxiv.org/abs/1907.02698},
year = {2019}
}
@inproceedings{Eck:2007:BeatTracking:ICASSP,
abstract = {We introduce a novel method for estimating beat from digital audio. We compute autocorrelation such that the distribution of energy in phase space is preserved in a so-called autocorrelation phase matrix (APM). We estimate beats by computing individual APMs over short overlapping segments of an onset trace derived from the audio. Then an adaptation of Viterbi decoding is used to search the APMs for metrical combinations of points that change smoothly over time in the lag/phase plane. Because small temporal perturbations are seen as local movements on the APM, the Viterbi search can be bounded using a small 2D Gaussian window. The resulting algorithm jointly estimates tempo, meter and beat. As is always the case with Viterbi decoding, an online version is possible although best performance is achieved offline. We report results on an annotated dataset of 60-second musical segments},
address = {Honolulu, Hawaii, USA},
author = {Eck, Douglas},
booktitle = {Proceedings of the 32nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2007.367319},
file = {::},
isbn = {1424407281},
issn = {15206149},
keywords = {Autocorrelation phase matrix,Beat induction,Correlation,Viterbi decoding,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1313--1316},
title = {{Beat tracking using an autocorrelation phase matrix}},
volume = {4},
year = {2007}
}
@article{Qandour2014,
abstract = {Recent developments in Wireless Sensor Networks (WSNs) have led to their use in remote data acquisition and automatic data analysis applications, which have proven to be an invaluable tool in a diverse range of fields including biosecurity. Further indications have been found that honeybee health can be monitored and determined through the use of acoustic analysis. In this paper, we present a system that has the ability to remotely detect the presence of pest infestation on a colony of honeybees by comparing the acoustic fingerprint of a hive to a fingerprint of known status. This will aid the goals of increasing surveillance programs by reducing the labour time and costs that are associated with managing and maintaining monitoring programs. Other benefits of the system proposed in this article include the ability to make available a collection of deterministic, standardised and nondiscriminatory statistical data for the purpose of research into determining the causes of colony collapse disorder.},
author = {Qandour, Amro and Ahmad, Iftekhar and Habibi, Daryoush and Leppard, Mark},
issn = {08146039},
journal = {Acoustics Australia},
keywords = {bee monitoring},
mendeley-tags = {bee monitoring},
number = {3},
pages = {204--209},
title = {{Remote beehive monitoring using acoustic signals}},
volume = {42},
year = {2014}
}
@inproceedings{Boeck:2014:BeatTracking:ISMIR,
abstract = {In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possi- ble beat positions. It chooses the model with the most ap- propriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27{\%} over existing state- of-the-art methods. Under certain conditions the system is able to match even human tapping performance.},
address = {Taipei, Taiwan},
author = {B{\"{o}}ck, S and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{o}}ck, Krebs, Widmer - 2014 - A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music Styles.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {603--608},
title = {{A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music Styles}},
xurl = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T108{\_}367{\_}Paper.pdf},
year = {2014}
}
@inproceedings{Davies:2019:BeatTracking:EUSIPCO,
address = {A Coru{\~{n}}a, Spain},
author = {Davies, Matthew E. P. and B{\"{o}}ck, Sebastia},
booktitle = {Proceedings of the 27th European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davies, B{\"{o}}ck - 2019 - Temporal convolutional networks for musical audio beat tracking.pdf:pdf},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
title = {{Temporal convolutional networks for musical audio beat tracking}},
year = {2019}
}
@inproceedings{Fuentes:2019:BeatTracking:ICASSP,
address = {Brighton, UK},
author = {Fuentes, Magdalena and McFee, Brian and Crayencour, H{\'{e}}l{\`{e}}ne C. and Essid, Slim and Bello, Juan Pablo},
booktitle = {Proceedings of the 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682870},
file = {::},
isbn = {9781479981311},
issn = {15206149},
keywords = {Convolutional-Recurrent Neural Networks,Deep Learning,Downbeat Tracking,Music Structure,Skip-Chain Conditional Random Fields,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {481--485},
title = {{A Music Structure Informed Downbeat Tracking System Using Skip-chain Conditional Random Fields and Deep Learning}},
year = {2019}
}
@inproceedings{Fiocchi:2018:BeatTracking:EUSIPCO,
address = {Rome, Italy},
author = {Fiocchi, Davide and Buccoli, Michele and Zanoni, Massimiliano and Antonacci, Fabio and Sarti, Augusto},
booktitle = {Proceedings of the 26th European Signal Processing Conference (EUSIPCO)},
file = {::},
isbn = {9789082797015},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1915--1919},
publisher = {EURASIP},
title = {{Beat Tracking using Recurrent Neural Network: a Transfer Learning Approach}},
xurl = {https://www.politesi.polimi.it/bitstream/10589/139073/1/tesi.pdf},
year = {2018}
}
@inproceedings{Cheng:2018:BeatTracking:EUSIPCO,
address = {Rome, Italy},
author = {Cheng, Tian and Fukayama, Satoru and Goto, Masataka},
booktitle = {Proceedings of the 28th European Signal Processing Conference (EUSIPCO)},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {1905--1909},
title = {{Convolving Gaussian Kernels for RNN-Based Beat Tracking}},
year = {2018}
}
@article{Durand:2017:DownbeatTracking:IEEE_TASLP,
abstract = {In this paper, we present a novel state of the art system for automatic downbeat tracking from music signals. The audio signal is first segmented in frames which are synchronized at the tatum level of the music. We then extract different kind of features based on harmony, melody, rhythm and bass content to feed convolutional neural networks that are adapted to take advantage of each feature characteristics. This ensemble of neural networks is combined to obtain one downbeat likelihood per tatum. The downbeat sequence is finally decoded with a flexible and efficient temporal model which takes advantage of the metrical continuity of a song. We then perform an evaluation of our system on a large base of 9 datasets, compare its performance to 4 other published algorithms and obtain a significant increase of 16.8 percent points compared to the second best system, for altogether a moderate cost in test and training. The influence of each step of the method is studied to show its strengths and shortcomings.},
author = {Durand, Simon and Bello, Juan Pablo and David, Bertrand and Richard, Gael},
xdoi = {10.1109/TASLP.2016.2623565},
file = {::},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Convolutional neural networks,beat{\_}tracking,downbeat tracking,music information retrieval,music signal processing},
mendeley-tags = {beat{\_}tracking},
number = {1},
pages = {72--85},
title = {{Robust Downbeat Tracking Using an Ensemble of Convolutional Networks}},
volume = {25},
year = {2017}
}
@inproceedings{Durand:2016:DownbeatTracking:ICASSP,
abstract = {We define a novel system for the automatic estimation of downbeat positions from audio music signals. New rhythm and melodic fea- tures are introduced and feature adapted convolutional neural net- works are used to take advantage of their specificity. Indeed, invari- ance to melody transposition, chroma data augmentation and length- specific rhythmic patterns prove to be useful to learn downbeat like- lihood. After the data is segmented in tatums, complementary fea- tures related to melody, rhythm and harmony are extracted and the likelihood of a tatum being at a downbeat position is computed with the aforementioned neural networks. The downbeat sequence is then extracted with a flexible temporal hidden Markov model. We then show the efficiency and robustness of our approach with a compara- tive evaluation conducted on 9 datasets.},
address = {Shanghai, China},
author = {Durand, Simon and Bello, Juan P. and David, Bertrand and Richard, Gael},
booktitle = {Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2016.7471684},
file = {::},
isbn = {9781479999880},
issn = {15206149},
keywords = {Convolutional Neural Networks,Downbeat Tracking,Music Information Retrieval,Music Signal Processing,beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {296--300},
title = {{Feature adapted convolutional neural networks for downbeat tracking}},
year = {2016}
}
@inproceedings{Krebs:2016:DownbeatTracking:ISMIR,
abstract = {In this paper, we propose a system that extracts the down-beat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results.},
address = {New York, NY, USA},
author = {Krebs, Florian and B{\"{o}}ck, Sebastian and Dorfer, Matthias and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {129--135},
title = {{Downbeat Tracking Using Beat-Synchronous Features and Recurrent Neural Networks}},
year = {2016}
}
@inproceedings{Boeck:2016:JointBeatDownbeat:ISMIR,
abstract = {In this paper we present a novel method for jointly extract-ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro-grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down-beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.},
address = {New York, NY, USA},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {255--261},
title = {{Joint Beat and Downbeat Tracking with Recurrent Neural Networks}},
year = {2016}
}
@inproceedings{Krebs:2015:TempoMeterTracking:ISMIR,
abstract = {Dynamic Bayesian networks (e.g., Hidden Markov Mod-els) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhyth-mic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic pa-rameters from a piece of music. While this allows the mutual dependencies between these parameters to be ex-ploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We in-corporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets.},
address = {M{\'{a}}laga, Spain},
author = {Krebs, Florian and Sebastian, B and Widmer, Gerhard},
booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {72--78},
title = {{An Efficient State-Space Model for Joint Tempo and Meter Tracking}},
year = {2015}
}
@article{Kong:2019:AttentionWeaklyLabel:IEEE_TASLP,
abstract = {Audio tagging is the task of predicting the presence or absence of sound classes within an audio clip. Previous work in audio classification focused on relatively small datasets limited to recognising a small number of sound classes. We investigate audio tagging on AudioSet, which is a dataset consisting of over 2 million audio clips and 527 classes. AudioSet is weakly labelled, in that only the presence or absence of sound classes is known for each clip, while the onset and offset times are unknown. To address the weakly-labelled audio classification problem, we propose attention neural networks as a way to attend the the most salient parts of an audio clip. We bridge the connection between attention neural networks and multiple instance learning (MIL) methods, and propose decision-level and feature-level attention neural networks for audio tagging. We investigate attention neural networks modelled by different functions, depths and widths. Experiments on AudioSet show that the feature-level attention neural network achieves a state-of-the-art mean average precision (mAP) of 0.369, outperforming the best multiple instance learning (MIL) method of 0.317 and Google's deep neural network baseline of 0.314. In addition, we discover that the audio tagging performance on AudioSet embedding features has a weak correlation with the number of training examples and the quality of labels of each sound class.},
author = {Kong, Qiuqiang and Yu, Changsong and Xu, Yong and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.},
xdoi = {10.1109/taslp.2019.2930913},
file = {:C$\backslash$:/Users/abr/Downloads/08777125.pdf:pdf},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {11},
pages = {1--1},
publisher = {IEEE},
title = {{Weakly Labelled AudioSet Tagging with Attention Neural Networks}},
volume = {27},
year = {2019}
}
@article{Sharma:2019:SoundClassification:ARXIV,
abstract = {In this paper, we propose a model for the Environment Sound Classification Task (ESC) that consists of multiple feature channels given as input to a Deep Convolutional Neural Network (CNN). The novelty of the paper lies in using multiple feature channels consisting of Mel-Frequency Cepstral Coefficients (MFCC), Gammatone Frequency Cepstral Coefficients (GFCC), the Constant Q-transform (CQT) and Chromagram. Such multiple features have never been used before for signal or audio processing. Also, we employ a deeper CNN (DCNN) compared to previous models, consisting of 2D separable convolutions working on time and feature domain separately. The model also consists of max pooling layers that downsample time and feature domain separately. We use some data augmentation techniques to further boost performance. Our model is able to achieve state-of-the-art performance on all three benchmark environment sound classification datasets, i.e. the UrbanSound8K (98.60{\%}), ESC-10 (97.25{\%}) and ESC-50 (95.50{\%}). To the best of our knowledge, this is the first time that a single environment sound classification model is able to achieve state-of-the-art results on all three datasets and by a considerable margin over the previous models. For ESC-10 and ESC-50 datasets, the accuracy achieved by the proposed model is beyond human accuracy of 95.7{\%} and 81.3{\%} respectively.},
archivePrefix = {arXiv},
arxivId = {1908.11219},
author = {Sharma, Jivitesh and Granmo, Ole-Christoffer and Goodwin, Morten},
eprint = {1908.11219},
file = {::},
journal = {ArXiv pre-prints},
number = {8},
pages = {1--11},
title = {{Environment Sound Classification using Multiple Feature Channels and Deep Convolutional Neural Networks}},
xurl = {http://arxiv.org/abs/1908.11219},
volume = {14},
year = {2019}
}
@inproceedings{Kosmider:2019:DeviceCalibration:DCASE,
author = {Kosmider, Michal},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
title = {{Calibrating Neural Networks for Secondary Recording Devices}},
year = {2019}
}
@article{Liu:2019:EnvironmentSurveillance:ARXIV,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167},
author = {Li, Yuan and Cheng, Zhongwei and Liu, Jie and Yassin, Bourhan and Nan, Zhe and Luo, Jiebo},
eprint = {arXiv:1502.03167},
file = {::},
isbn = {9781510810587},
keywords = {acoustic surveillance,audio classi,cation,neural networks},
title = {{AI for Earth: Rainforest Conservation by Acoustic Surveillance}},
year = {2019}
}
@article{Okamoto:2019:EnvironmentalSoundSynthesis:ARXIV,
abstract = {Synthesizing and converting environmental sounds have the potential for many applications such as supporting movie and game production, data augmentation for sound event detection and scene classification. Conventional works on synthesizing and converting environmental sounds are based on a physical modeling or concatenative approach. However, there are a limited number of works that have addressed environmental sound synthesis and conversion with statistical generative models; thus, this research area is not yet well organized. In this paper, we review problem definitions, applications, and evaluation methods of environmental sound synthesis and conversion. We then report on environmental sound synthesis using sound event labels, in which we focus on the current performance of statistical environmental sound synthesis and investigate how we should conduct subjective experiments on environmental sound synthesis.},
archivePrefix = {arXiv},
arxivId = {1908.10055},
author = {Okamoto, Yuki and Imoto, Keisuke and Komatsu, Tatsuya and Takamichi, Shinnosuke and Yagyu, Takumi and Yamanishi, Ryosuke and Yamashita, Yoichi},
eprint = {1908.10055},
file = {::},
title = {{Overview of Tasks and Investigation of Subjective Evaluation Methods in Environmental Sound Synthesis and Conversion}},
xurl = {http://arxiv.org/abs/1908.10055},
year = {2019}
}
@inproceedings{Perez-Lopez:2019:LocalizationDetectionDCASE,
abstract = {This work describes and discusses an algorithm submitted to the Sound Event Localization and Detection Task of DCASE2019 Challenge. The proposed methodology relies on parametric spatial audio analysis for source localization and detection, combined with a deep learning-based monophonic event classifier. The evaluation of the proposed algorithm yields overall results comparable to the baseline system. The main highlight is a reduction of the localization error on the evaluation dataset by a factor of 2.6, compared with the baseline performance.},
archivePrefix = {arXiv},
arxivId = {1908.10133},
author = {P{\'{e}}rez-L{\'{o}}pez, Andr{\'{e}}s and Fonseca, Eduardo and Serra, Xavier},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
eprint = {1908.10133},
file = {::},
number = {October},
title = {{A hybrid parametric-deep learning approach for sound event localization and detection}},
year = {2019}
}
@inproceedings{Zhang:2018:Mixup:ICLR,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By xdoing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1710.09412},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:pdf},
title = {{mixup: Beyond Empirical Risk Minimization}},
year = {2018}
}
@article{Qiao:2019:SoundClassification:ARXIV,
abstract = {Environmental Sound Classification (ESC) is an important and challenging problem, and feature representation is a critical and even decisive factor in ESC. Feature representation ability directly affects the accuracy of sound classification. Therefore, the ESC performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. In this paper, we propose a subspectrogram segmentation based ESC classification framework. In addition, we adopt the proposed Convolutional Recurrent Neural Network (CRNN) and score level fusion to jointly improve the classification accuracy. Extensive truncation schemes are evaluated to find the optimal number and the corresponding band ranges of sub-spectrograms. Based on the numerical experiments, the proposed framework can achieve 81.9{\%} ESC classification accuracy on the public dataset ESC-50, which provides 9.1{\%} accuracy improvement over traditional baseline schemes.},
archivePrefix = {arXiv},
arxivId = {1908.05863},
author = {Qiao, Tianhao and Zhang, Shunqing and Zhang, Zhichao and Cao, Shan and Xu, Shugong},
eprint = {1908.05863},
file = {::},
title = {{Sub-Spectrogram Segmentation for Environmental Sound Classification via Convolutional Recurrent Neural Network and Score Level Fusion}},
xurl = {http://arxiv.org/abs/1908.05863},
year = {2019}
}
@inproceedings{Zhang:2019:LocalizationDetection:DCASE,
author = {Zhang, Jingyang and Ding, Wenhao and He, Liang},
file = {::},
pages = {1--5},
title = {{Data Augmentation and Prior Knowledge-Based Regularization for Sound Event Localization and Detection}},
year = {2019}
}
@inproceedings{Xue:2019:DetectionLocalization:DCASE,
abstract = {Joint sound event detection (SED) and sound source localization (SSL) is essential since it provides both the temporal and spatial information of the events that appear in an acoustic scene. Although the problem can be tackled by designing a system based on the deep neural networks (DNNs) and fundamental spectral and spatial features, in this paper, we largely leverage the conventional microphone array signal processing techniques to generate more comprehensive representations for both SED and SSL, and to perform post-processing such that stable SED and SSL results can be obtained. Specifically, the features extracted from signals of multiple beams are utilized, which orient towards different directions of arrival (DOAs), and are formed according to the estimated steering vector of each DOA. Smoothed cross-power spectra (CPS) are computed based on the signal presence probability (SPP), and are used both as the input features of the DNNs, and for estimating the steering vectors of different DOAs. A triple-task learning scheme is developed, which jointly exploits the classification and regression based criterion for DOA estimation, and uses the classification based criterion as a regularization for the DNN. Experimental results demonstrate that the proposed method yields substantial improvements compared with the baseline method for the task 3 of the DCASE challenge 2019.},
author = {Xue, Wei and Tong, Ying and Zhang, Chao and Ding, Guohong},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
keywords = {Deep neural networks,Index Terms-Sound event detection,Sound source localiza-tion},
title = {{Multi-Beam and Multi-Task Learning for Joint Sound Event Detection and Localization}},
xurl = {http://dcase.community/documents/challenge2019/technical{\_}reports/DCASE2019{\_}Xue{\_}91.pdf},
year = {2019}
}
@article{Srivastava:2014:Dropout:JMLR,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{Ioffe:2015:BatchNorm:ICML,
address = {Lille, France},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
xdoi = {10.1080/17512786.2015.1058180},
file = {::},
issn = {17512794},
pages = {448--456},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{Parisi:2019:ContinualLearning:NN,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.07569v4},
author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
xdoi = {10.1016/j.neunet.2019.01.012},
eprint = {arXiv:1802.07569v4},
file = {:C$\backslash$:/Users/abr/Downloads/1802.07569.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
pages = {54--71},
title = {{Continual lifelong learning with neural networks: A review}},
volume = {113},
year = {2019}
}
@inproceedings{Boeck:2016:BeatTracking:ISMIR,
abstract = {In this paper we present a novel method for jointly extract-ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro-grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down-beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.},
address = {New York, NY, USA},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International So- ciety for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {beat{\_}tracking},
mendeley-tags = {beat{\_}tracking},
pages = {255--261},
title = {{Joint Beat and Downbeat Tracking with Recurrent Neural Networks}},
year = {2016}
}
@article{Delphin-Poulat:2019:SED:DCASE,
author = {Delphin-Poulat, Lionel and Plapous, Cyril},
file = {::},
pages = {2018--2020},
title = {{Mean Teacher with Data Augmentation for DCASE 2019 Task 4}},
year = {2019}
}
@article{Shi:2019:SED:DCASE,
annote = {Ensemble of 3 semisupervised learning algorithms

CRNN as core model},
archivePrefix = {arXiv},
arxivId = {1903.03825},
author = {Shi, Ziqiang},
eprint = {1903.03825},
file = {::},
pages = {9--10},
title = {{Hodgepodge: Sound Event Detection based on Ensemble of Semi-Supervised Learning Methods}},
xurl = {http://arxiv.org/abs/1903.03825},
year = {2019}
}
@article{Cances:2019:SED:DCASE,
author = {Cances, L{\'{e}}o and Pellegrini, Thomas and Guyot, Patrice},
file = {::},
pages = {2--5},
title = {{Multi-Task Learning and Post Processing Optimization for Sound Event Detection}},
year = {2019}
}
@article{Lin:2019:SED:DCASE,
author = {Lin, Liwei and Wang, Xiangdong},
file = {::},
pages = {1--5},
title = {{Guided Learning Convolution System for DCASE 2019 Task 4}},
year = {2019}
}
@inproceedings{HungYang:2018:FrameIRTimbrePitch:ISMIR,
abstract = {Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that xdoing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at $\backslash$xurl{\{}https://biboamy.github.io/instrument-recognition/{\}}.},
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {1806.09587},
author = {Hung, Yun-Ning and Yang, Yi-Hsuan},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {1806.09587},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hung, Yang - 2018 - Frame-Level Instrument Recognition by Timbre and Pitch(2).pdf:pdf},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
pages = {135--142},
title = {{Frame-level Instrument Recognition by Timbre and Pitch}},
xurl = {http://arxiv.org/abs/1806.09587},
year = {2018}
}
@inproceedings{Sumi:2008:ChordBass:ISMIR,
address = {Philadelphia, PA, USA},
author = {Sumi, Kouhei and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Komatani, Kazunori and Ogata, Tetsuya and Okuno, Hiroshi G.},
booktitle = {Proceedings of the 9th International Conference of Music Information Retrieval (ISMIR)},
file = {::},
keywords = {bass{\_}transcription,chord{\_}transcription},
mendeley-tags = {bass{\_}transcription,chord{\_}transcription},
pages = {39--44},
title = {{Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation}},
year = {2008}
}
@article{Stoller:2018:WaveUNet:ISMIR,
abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
archivePrefix = {arXiv},
arxivId = {1806.03185},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
eprint = {1806.03185},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stoller, Ewert, Dixon - 2018 - Wave-U-Net A Multi-Scale Neural Network for End-to-End Audio Source Separation.pdf:pdf},
title = {{Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation}},
xurl = {http://arxiv.org/abs/1806.03185},
year = {2018}
}
@article{Xie2019,
abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than {\$}30\backslash{\%}{\$} of the error rates of state-of-the-art methods: going from 7.66{\%} to 5.27{\%} and from 3.53{\%} to 2.46{\%} respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36{\%} to 79.04/94.45{\%} when compared to AutoAugment.},
archivePrefix = {arXiv},
arxivId = {1904.12848},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
eprint = {1904.12848},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2019 - Unsupervised Data Augmentation.pdf:pdf},
month = {apr},
title = {{Unsupervised Data Augmentation}},
xurl = {http://arxiv.org/abs/1904.12848},
year = {2019}
}
@inproceedings{Arora2017,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)},
xdoi = {10.1109/MMSP.2017.8122258},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario(4).pdf:pdf},
isbn = {978-1-5090-3649-3},
month = {oct},
pages = {1--6},
publisher = {IEEE},
title = {{A study on transfer learning for acoustic event detection in a real life scenario}},
xurl = {http://ieeexplore.ieee.org/document/8122258/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Koutini:2019:ASC:DCASE,
author = {Koutini, Khaled and Eghbal-zadeh, Hamid and Widmer, Gerhard and Kepler, Johannes},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
pages = {1--5},
title = {{CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs}},
year = {2019}
}
@inproceedings{Chen:2019:ASC:DCASE,
author = {Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling.pdf:pdf},
title = {{Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling}},
year = {2019}
}
@inproceedings{Seo:2019:ASC:DCASE,
annote = {Ensemble approach, computationally too expensive},
author = {Seo, Hyeji and Park, Jihwan and Park, Yongjin},
booktitle = {Challange on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {::},
pages = {3--6},
title = {{Acoustic Scene Classification using Various Pre-Processed Features and Convolutional Neural Networks}},
year = {2019}
}
@article{Kim:2019:Learning:NCA,
abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g. music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
author = {Kim, Jaehun and Urbano, Juli{\'{a}}n and Liem, Cynthia C.S. and Hanjalic, Alan},
xdoi = {10.1007/s00521-019-04076-1},
file = {::},
isbn = {0123456789},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Multitask learning,Music Information Retrieval,Representation learning},
title = {{One deep music representation to rule them all? A comparative analysis of different representation learning strategies}},
volume = {1},
year = {2019}
}
@article{Zheng2017,
author = {Zheng, Nan-ning and Liu, Zi-yi and Ren, Peng-ju and Ma, Yong-qiang and Chen, Shi-tao},
file = {::},
keywords = {10,1631,1700053,causal model,cognitive computing,cognitive mapping,xdoi,dx,fitee,http,human-machine collaboration,hybrid-augmented intelligence,intuitive,org,reasoning,self-driving cars,visual scene understanding},
number = {2},
pages = {153--179},
title = {{Hybrid-augmented intelligence :}},
volume = {18},
year = {2017}
}
@article{Ahn2013,
author = {von Ahn, Luis},
xdoi = {10.1098/rsta.2012.0383},
file = {::},
issn = {1364-503X, 1471-2962},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {artificial intelligence},
number = {1987},
title = {{Augmented intelligence: the Web and human intelligence}},
xurl = {http://rsta.royalsocietypublishing.org.libproxy1.nus.edu.sg/content/371/1987/20120383{\%}5Cnhttp://rsta.royalsocietypublishing.org.libproxy1.nus.edu.sg/content/371/1987/20120383.full.pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23419852},
volume = {371},
year = {2013}
}
@article{Pan2016,
abstract = {With the popularization of the Internet, permeation of sensor networks, emergence of big data, increase in size of the information community, and interlinking and fusion of data and information throughout human society, physical space, and cyberspace, the information environment related to the current development of artificial intelligence (AI) has profoundly changed. AI faces important adjustments, and scientific foundations are confronted with new breakthroughs, as AI enters a new stage: AI 2.0. This paper briefly reviews the 60-year developmental history of AI, analyzes the external environment promoting the formation of AI 2.0 along with changes in goals, and describes both the beginning of the technology and the core idea behind AI 2.0 development. Furthermore, based on combined social demands and the information environment that exists in relation to Chinese development, suggestions on the development of AI 2.0 are given.},
author = {Pan, Yunhe},
xdoi = {10.1016/J.ENG.2016.04.018},
file = {::},
issn = {20958099},
journal = {Engineering},
keywords = {Artificial intelligence 2.0,Autonomous-intelligent system,Big data,Cross-media,Crowd intelligence,Human-machine hybrid-augmented intelligence},
number = {4},
pages = {409--413},
publisher = {Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company},
title = {{Heading toward Artificial Intelligence 2.0}},
xurl = {http://dx.xdoi.org/10.1016/J.ENG.2016.04.018},
volume = {2},
year = {2016}
}
@inproceedings{Maksimovic:2019:AESForensics,
author = {Maksimovi{\'{c}}, Milica and Cuccovillo, Luca and Aichroth, Patrick},
booktitle = {AES International Conference on Audio Forensics},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Copy-Move Forgery Detection and Localization via Partial Audio Matching}},
year = {2019}
}
@inproceedings{Grollmisch:2019:S2S:SMC,
address = {M{\'{a}}laga, Spain},
author = {Grollmisch, Sascha and Cano, Estefan{\'{i}}a},
booktitle = {Proceedings of the Sound {\&} Music Computing Conference (SMC)},
file = {::},
keywords = {idmt},
mendeley-tags = {idmt},
pages = {3},
title = {{Automatic Chord Recognition in Music Education Applications}}
}
@inproceedings{Cuccovillo:2019:AESForensics,
author = {Cuccovillo, Luca and Aichroth, Patrick},
booktitle = {AES International Conference on Audio Forensics},
keywords = {idmt},
mendeley-tags = {idmt},
title = {{Inverse Decoding of PCM {\{}{\$}A{\$}-law{\}} and {\{}{\$}\mu{\$}-law{\}}}},
year = {2019}
}
@article{Luo:2018:ConvTasNet:ARXIV,
abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.},
archivePrefix = {arXiv},
arxivId = {1809.07454},
author = {Luo, Yi and Mesgarani, Nima},
xdoi = {10.1109/TASLP.2019.2915167},
eprint = {1809.07454},
file = {::},
journal = {arXiv},
keywords = {speaker{\_}separation},
mendeley-tags = {speaker{\_}separation},
pages = {1--12},
title = {{Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation}},
xurl = {http://arxiv.org/abs/1809.07454{\%}0Ahttp://dx.xdoi.org/10.1109/TASLP.2019.2915167},
year = {2018}
}
@article{Taenzer:2019:InvestigatingInstFamRecognition:ISMIR,
abstract = {Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation.},
author = {Taenzer, Michael and Abe{\ss}er, Jakob and Mimilakis, Stylianos Ioannis and Wei{\ss}, Christof and M{\"{u}}ller, Meinard and Lukashevich, Hanna},
file = {::},
keywords = {instrument{\_}recognition,isad,md},
mendeley-tags = {instrument{\_}recognition,isad,md},
title = {{Investigating CNN-Based Instrument Family Recognition for Western Classical Music Recordings}},
year = {2019}
}
@techreport{Cao:2019:EventLocalizationDetection:DCASE,
author = {Cao, Yin and Iqbal, Turab and Kong, Qiuqiang and Galindo, Miguel B. and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Detection and Classification of Acoustic Scenes and Events 2019},
file = {::},
keywords = {event{\_}detection,event{\_}localization,machine{\_}listening},
mendeley-tags = {event{\_}detection,event{\_}localization,machine{\_}listening},
title = {{Two-Stage Sound Event Localization and Detection using Intensity Vector and Generalized Cross-Correlation}},
year = {2019}
}
@book{Pfleiderer:2017:Jazzomat:BOOK,
editor = {Pfleiderer, Martin and Frieler, Klaus and Abe{\ss}er, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - Inside the Jazzomat - New Perspectives for Jazz Research.pdf:pdf},
publisher = {Schott Campus},
title = {{Inside the Jazzomat - New Perspectives for Jazz Research}},
year = {2017}
}
@inproceedings{Hawthorne:2018:PianoTranscription:ISMIR,
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11153v1},
author = {Hawthorne, Curtis and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Roberts, Adam},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {arXiv:1710.11153v1},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawthorne et al. - 2018 - Onsets and frames Dual-objective piano transcription.pdf:pdf},
title = {{Onsets and frames: Dual-objective piano transcription}},
year = {2018}
}
@inproceedings{Bittner:2014:MedleyDB:ISMIR,
address = {Taipei, Taiwan},
author = {Bittner, Rachel M. and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {155--160},
title = {{MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research}},
year = {2014}
}
@inproceedings{Kum:2016:Melody:ISMIR,
address = {New York City, USA},
author = {Kum, Sangeun and Oh, Changheun and Nam, Juhan},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kum, Oh, Nam - 2016 - Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks(2).pdf:pdf},
pages = {819--825},
title = {{Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks}},
year = {2016}
}
@article{Kum:2017:MelodyCNN:PREPRINTS,
author = {Kum, Sangeun and Nam, Juhan},
xdoi = {10.20944/preprints201711.0027.v1},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kum, Nam - 2017 - Classification-Based Singing Melody Extraction Using Deep Convolutional Neural Networks(2).pdf:pdf},
journal = {Preprints},
title = {{Classification-Based Singing Melody Extraction Using Deep Convolutional Neural Networks}},
year = {2017}
}
@inproceedings{Rigaud:2016:Transcription:ISMIR,
abstract = {This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f 0 es-timation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The perfor-mance of the melody f 0 estimation system is compared with a state-of-the-art method and exhibits highest accu-racy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody tran-scription is presented.},
address = {New York, NY, USA},
author = {Rigaud, Francois and Radenen, Mathieu},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {737--743},
title = {{Singing Voice Melody Transcription using Deep Neural Networks}},
year = {2016}
}
@inproceedings{Balke:2018:MelodyTranscription:ICASSP,
address = {New Orleans, USA},
author = {Balke, Stefan and Dittmar, Christian and Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Balke et al. - 2018 - Data-Driven Solo Voice Enhancement for Jazz Music Retrieval(2).pdf:pdf},
pages = {196--200},
title = {{Data-Driven Solo Voice Enhancement for Jazz Music Retrieval}},
year = {2018}
}
@article{Bittner:2018:MultitaskLearning:ARXIV,
archivePrefix = {arXiv},
arxivId = {1809.00381},
author = {Bittner, Rachel M. and McFee, Brian and Bello, Juan P.},
eprint = {1809.00381},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner, McFee, Bello - 2018 - Multitask Learning for Fundamental Frequency Estimation in Music(2).pdf:pdf},
journal = {CoRR},
title = {{Multitask Learning for Fundamental Frequency Estimation in Music}},
volume = {abs/1809.0},
year = {2018}
}
@inproceedings{Doras:2019:UNet:MMRP,
address = {Milano, Italy},
author = {Doras, Guillaume and Esling, Philippe and Peeters, Geoffroy},
booktitle = {Proceedings of the International Workshop on Multilayer Music Representation and Processing (MMRP)},
xdoi = {10.1109/MMRP.2019.00020},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doras, Esling, Peeters - 2019 - On the use of U-Net for dominant melody estimation in polyphonic music(2).pdf:pdf},
isbn = {9781728116495},
pages = {66--70},
title = {{On the use of U-Net for dominant melody estimation in polyphonic music}},
year = {2019}
}
@inproceedings{Bittner:2017:DeepSaliency:ISMIR,
address = {Suzhou, China},
author = {Bittner, Rachel M. and McFee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P.},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bittner et al. - Unknown - DEEP SALIENCE REPRESENTATIONS FOR F 0 ESTIMATION IN POLYPHONIC MUSIC(2).pdf:pdf},
pages = {63--70},
title = {{Deep Salience Representations for F0 Estimation in Polyphonic Music}},
year = {2017}
}
@inproceedings{Abesser:2018:BassSaliency:ISMIR,
address = {Paris, France},
author = {Abe{\ss}er, Jakob and Balke, Stefan and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
pages = {306--312},
title = {{Improving Bass Saliency Estimation Using Label Propagation and Transfer Learning}},
xurl = {http://ismir2018.ircam.fr/doc/pdfs/143{\_}Paper.pdf},
year = {2018}
}
@inproceedings{Park:2017:Melody:ICASSP,
address = {New Orleans, USA},
author = {Park, Hyunsin and Yoo, Chang D.},
booktitle = {Proceedings of the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Yoo - 2017 - Melody Extraction and Detection through LSTM-RNN with Harmonic Sum Loss(2).pdf:pdf},
pages = {2766--2770},
title = {{Melody Extraction and Detection through LSTM-RNN with Harmonic Sum Loss}},
year = {2017}
}
@inproceedings{Abesser:2017:BassTranscription:AES,
address = {Erlangen, Germany},
author = {Abe{\ss}er, Jakob and Balke, Stefan and Frieler, Klaus and Pfleiderer, Martin and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the AES Conference on Semantic Audio},
xdoi = {10.1016/S0022-328X(00)00444-7},
file = {::},
pages = {202--209},
title = {{Deep Learning for Jazz Walking Bass Transcription}},
year = {2017}
}
@inproceedings{Salamon:2017:AnalysisSynthesis:ISMIR,
address = {Suzhou, China},
author = {Salamon, Justin and Bittner, Rachel M. and Bonada, Jordi and Bosch, Juan J. and G{\'{o}}mez, Emilia and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {::},
keywords = {f0{\_}tracking,synthesis},
mendeley-tags = {f0{\_}tracking,synthesis},
pages = {71--78},
title = {{An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets}},
year = {2017}
}
@article{Ryynaenen:2008:Transcription:CMJ,
author = {Ryyn{\"{a}}nen, Matti P. and Klapuri, Anssi P.},
file = {::},
journal = {Computer Music Journal},
keywords = {bass{\_}transcription,melody{\_}transcription},
mendeley-tags = {bass{\_}transcription,melody{\_}transcription},
number = {3},
pages = {72--86},
title = {{Automatic Transcription of Melody , Bass Line , and Chords in Polyphonic Music}},
volume = {32},
year = {2008}
}
@inproceedings{Zeghidour:2016:JointLearning:INTERSPEECH,
address = {San Francisco, CA, USA},
author = {Zeghidour, Neil and Synnaeve, Gabriel and Usunier, Nicolas and Dupoux, Emmanuel},
booktitle = {Proceedings of the 17th Annual Conference of the International Speech Communication Association (Interspeech)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeghidour et al. - 2016 - Joint Learning of Speaker and Phonetic Similarities with Siamese Networks(3).pdf:pdf},
pages = {1295--1299},
title = {{Joint Learning of Speaker and Phonetic Similarities with Siamese Networks.}},
year = {2016}
}
@inproceedings{Schroff:2015:FaceNet:CVPR,
address = {Boston, MA, USA},
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schroff, Philbin - Unknown - FaceNet A Unified Embedding for Face Recognition and Clustering(2).pdf:pdf},
pages = {815--823},
publisher = {{\{}IEEE{\}} Computer Society},
title = {{FaceNet: {\{}A{\}} unified embedding for face recognition and clustering}},
year = {2015}
}
@inproceedings{Taigman:2014:DeepFace:CVPR,
address = {Columbus, OH, USA},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc' Aurelio and Wolf, Lior},
booktitle = {Proceeedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taigman et al. - Unknown - DeepFace Closing the Gap to Human-Level Performance in Face Verification(2).pdf:pdf},
keywords = {siamese{\_}networks},
mendeley-tags = {siamese{\_}networks},
pages = {1701--1708},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@inproceedings{Koch:2015:SiameseOneShot:JLMR,
address = {Lille, France},
author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koch - 2011 - Siamese Neural Networks for One-shot Image Recognition(2).pdf:pdf},
keywords = {siamese{\_}networks},
mendeley-tags = {siamese{\_}networks},
title = {{Siamese Neural Networks for One-shot Image Recognition}},
year = {2015}
}
@inproceedings{Jansen:2018:UnsupervisedLearning:ICASSP,
address = {Calgary, AB, Canada},
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P W and Hershey, Shawn and Liu, Jiayang and Moore, R Channing and Saurous, Rif A},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jansen et al. - Unknown - Unsupervised learning of semantic audio representations(2).pdf:pdf},
pages = {126--130},
publisher = {EE},
title = {{Unsupervised Learning of Semantic Audio Representations}},
year = {2018}
}
@article{Balke:2019:SoftAttention:ISMIR,
abstract = {Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.},
archivePrefix = {arXiv},
arxivId = {1906.10996},
author = {Balke, Stefan and Dorfer, Matthias and Carvalho, Luis and Arzt, Andreas and Widmer, Gerhard},
eprint = {1906.10996},
file = {::},
journal = {arXiv},
title = {{Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music Retrieval}},
xurl = {http://arxiv.org/abs/1906.10996},
year = {2019}
}
@article{Roberts2018,
abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint = {1803.05428},
title = {{A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music}},
year = {2018}
}
@article{Cances:2019:PostProcessing:ARXIV,
abstract = {Sound event detection (SED) aims at identifying audio events (audio tagging task) in recordings and then locating them temporally (localization task). This last task ends with the segmentation of the frame-level class predictions, that determines the onsets and offsets of the audio events. Yet, this step is often overlooked in scientific publications. In this paper, we focus on the post-processing algorithms used to identify the audio event boundaries. Different post-processing steps are investigated, through smoothing, thresholding, and optimization. In particular, we evaluate different approaches for temporal segmentation, namely statistic-based and parametric methods. Experiments are carried out on the DCASE 2018 challenge task 4 data. We compared post-processing algorithms on the temporal prediction curves of two models: one based on the challenge's baseline and a Multiple Instance Learning (MIL) model. Results show the crucial impact of the post-processing methods on the final detection score. Statistic-based methods yield a 22.9{\%} event-based F-score on the evaluation set with our MIL model. Moreover, the best results were obtained using class-dependent parametric methods with 32.0{\%} F-score.},
archivePrefix = {arXiv},
arxivId = {1906.06909},
author = {Cances, Leo and Guyot, Patrice and Pellegrini, Thomas},
eprint = {1906.06909},
file = {::},
title = {{Evaluation of post-processing algorithms for polyphonic sound event detection}},
xurl = {http://arxiv.org/abs/1906.06909},
year = {2019}
}
@article{Luo:2019:TimbrePitchDisentanglement:ARXIV,
abstract = {In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model efficacy by latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high accuracy when tested on our synthesized sounds, which verifies the model performance of controllable realistic timbre and pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single autoencoder architecture, which is evaluated by measuring the shift in posterior of instrument classification. Our in depth evaluation confirms the model ability to successfully disentangle timbre and pitch.},
archivePrefix = {arXiv},
arxivId = {1906.08152},
author = {Luo, Yin-Jyun and Agres, Kat and Herremans, Dorien},
eprint = {1906.08152},
file = {::},
title = {{Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders}},
xurl = {http://arxiv.org/abs/1906.08152},
year = {2019}
}
@inproceedings{Elowsson:2019:PitchChromaCNN:ISMIR,
author = {Elowsson, Anders and Friberg, Anders},
booktitle = {ISMIR 2019},
file = {::},
title = {{Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN}},
year = {2019}
}
@article{Kim:2019:AdversarialLearning:ARXIV,
abstract = {Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements on transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.},
archivePrefix = {arXiv},
arxivId = {1906.08512},
author = {Kim, Jong Wook and Bello, Juan Pablo},
eprint = {1906.08512},
file = {::},
journal = {arXiv},
title = {{Adversarial Learning for Improved Onsets and Frames Music Transcription}},
xurl = {http://arxiv.org/abs/1906.08512},
year = {2019}
}
@article{Defferrard:2016:FMA:ARXIV,
abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma},
archivePrefix = {arXiv},
arxivId = {1612.01840},
author = {Defferrard, Micha{\"{e}}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
eprint = {1612.01840},
file = {::},
journal = {arXiv},
title = {{FMA: A Dataset For Music Analysis}},
xurl = {http://arxiv.org/abs/1612.01840},
year = {2016}
}
@inproceedings{Driedger:2019:BeatTapping:ISMIR,
address = {Delft, The Netherlands},
author = {Driedger, Jonathan and Schreiber, Hendrik and de Has, W. Bas and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/Downloads/2019{\_}DriedgerSHM{\_}BeatAnnotationCorrection{\_}ISMIR.pdf:pdf},
title = {{Towards Automatically Correcting Tapped Beat Annotations for Music Recordings}},
year = {2019}
}
@article{Waddell:2019:MusicLearning:FIICT,
author = {Waddell, George and Williamon, Aaron},
xdoi = {10.3389/fict.2019.00011},
file = {::},
issn = {2297-198X},
journal = {Frontiers in ICT},
keywords = {learning,music{\_}learning,musicians,performance,technology,technology acceptance model},
mendeley-tags = {music{\_}learning},
number = {May},
pages = {1--14},
title = {{Technology Use and Attitudes in Music Learning}},
xurl = {https://www.frontiersin.org/article/10.3389/fict.2019.00011/full},
volume = {6},
year = {2019}
}
@article{Rusu:2016:ProgressiveNN:ARXIV,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rusu et al. - 2016 - Progressive Neural Networks.pdf:pdf},
journal = {ArXiv pre-prints},
title = {{Progressive Neural Networks}},
xurl = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@phdthesis{Fayek:2019:ProgressiveLearning:PHD,
author = {Fayek, Haytham M.},
file = {:C$\backslash$:/Users/abr/Downloads/Fayek.pdf:pdf},
keywords = {progressive{\_}learning},
mendeley-tags = {progressive{\_}learning},
school = {Petronas University of Technology},
title = {{Continual Deep Learning via Progressive Learning}},
type = {PhD Thesis},
year = {2019}
}
@article{Oord:2016:Wavenet:ARXIV,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
pages = {1--15},
title = {{WaveNet: A Generative Model for Raw Audio}},
xurl = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Bai:2018:ConvRecurrent:ARXIV,
abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
archivePrefix = {arXiv},
arxivId = {1803.01271},
author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
eprint = {1803.01271},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling(2).pdf:pdf},
month = {mar},
title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
xurl = {http://arxiv.org/abs/1803.01271},
year = {2018}
}
@inproceedings{Humphrey:2018:InstrumentRecognition:ISMIR,
abstract = {Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music informa-tion retrieval. While there has been significant progress in developing predictive models for this and related classifi-cation tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset con-tains 20,000 examples of Creative Commons-licensed mu-sic available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the data-set was sampled and annotated, and compare its character-istics to similar, previous data-sets. Finally, we present ex-perimental results and baseline model performance to mo-tivate future work.},
address = {Paris, France},
author = {Humphrey, Eric J and Durand, Simon and Mcfee, Brian},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Humphrey, Durand, Mcfee - 2018 - OpenMIC-2018 An Open Data-set for Multiple Instrument Recognition(2).pdf:pdf},
pages = {438--444},
title = {{OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition}},
xurl = {http://bmcfee.github.io/papers/ismir2018{\_}openmic.pdf},
year = {2018}
}
@article{Choi:2019:DrumTranscription:ISMIR,
archivePrefix = {arXiv},
arxivId = {1906.03697v1},
author = {Choi, Keunwoo and Cho, Kyunghyun},
eprint = {1906.03697v1},
file = {::},
journal = {arXiv},
keywords = {drum{\_}transcription,unsupervised{\_}learning},
mendeley-tags = {drum{\_}transcription,unsupervised{\_}learning},
title = {{Deep unsupervised drum transcription}},
year = {2019}
}
@article{Kruspe:2019:ARXIV,
abstract = {Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single class. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated via softmax. We solve this problem by introducing a "null class" centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched test set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples' distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.},
archivePrefix = {arXiv},
arxivId = {1906.00820},
author = {Kruspe, Anna},
eprint = {1906.00820},
file = {::},
journal = {ArXiv pre-prints},
keywords = {machine{\_}learning},
mendeley-tags = {machine{\_}learning},
month = {jun},
title = {{One-Way Prototypical Networks}},
xurl = {http://arxiv.org/abs/1906.00820},
year = {2019}
}
@inproceedings{Pons:2018:LargeScaleTagging:ISMIR,
abstract = {The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models - using waveforms as input with very small convolutional filters; and models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios.},
address = {Paris},
author = {Pons, Jordi and Nieto, Oriol and Prockup, Matthew and Schmidt, Erik and Ehmann, Andreas and Serra, Xavier},
booktitle = {Proceedings of the International Conference on Music Information Retrieval},
file = {:C$\backslash$:/Users/abr/Downloads/191{\_}Paper (1).pdf:pdf},
keywords = {tagging},
mendeley-tags = {tagging},
pages = {637--644},
title = {{End-to-end learning for music audio tagging at scale}},
year = {2018}
}
@article{Schoneich2015,
abstract = {From human language to birdsong and the chirps of insects, acoustic communication is based on amplitude and frequency modulation of sound signals. Whereas frequency processing starts at the level of the hearing organs, temporal features of the sound amplitude such as rhythms or pulse rates require processing by central auditory neurons. Besides several theoretical concepts, brain circuits that detect temporal features of a sound signal are poorly understood. We focused on acoustically communicating field crickets and show how five neurons in the brain of females form an auditory feature detector circuit for the pulse pattern of the male calling song. The processing is based on a coincidence detector mechanism that selectively responds when a direct neural response and an intrinsically delayed response to the sound pulses coincide. This circuit provides the basis for auditory mate recognition in field crickets and reveals a principal mechanism of sensory processing underlying the perception of temporal patterns.},
author = {Sch{\"{o}}neich, Stefan and Kostarakos, Konstantinos and Hedwig, Berthold},
xdoi = {10.1126/sciadv.1500325},
file = {::},
journal = {Science Advances},
keywords = {KISH,auditory feature detection},
mendeley-tags = {KISH,auditory feature detection},
number = {8},
pages = {e1500325},
title = {{An auditory feature detection circuit for sound pattern recognition}},
volume = {1},
year = {2015}
}
@inproceedings{Tan:2019:TCNN:ICASSP,
address = {Brighton, UK},
author = {Tan, Ke and Wang, Deliang},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {9781538646588},
keywords = {time{\_}series{\_}analysis},
mendeley-tags = {time{\_}series{\_}analysis},
pages = {6865--6869},
title = {{TCNN: Temporal Convolutional Neural Network for Real-Time Speech Enhancement in the Time Domain}},
year = {2019}
}
@article{Koops:2019:harmony_subjectivity:journal,
abstract = {Reference annotation datasets containing harmony annotations are at the core of a wide range of studies in music information retrieval (MIR) and related fields. The majority of these datasets contain single reference annotations describing the harmony of each piece. Nevertheless, studies showing differences among annotators in many other MIR tasks make the notion of a single ‘ground-truth' reference annotation a tenuous one. In this paper, we introduce and analyse the Chordify Annotator Subjectivity Dataset (CASD) containing chord labels for 50 songs from 4 expert annotators in order to gain a better understanding of the differences between annotators in their chord label choice. Our analysis reveals that annotators use distinct chord-label vocabularies, with low chord-label overlap across all annotators. Between annotators, we find only 73 percent overlap on average for the traditional major–minor vocabulary and 54 percent overlap for the most complex chord labels. A factor analysis reveals the relative importance of triads, sevenths, inversions and other musical factors for each annotator on their choice of chord labels and reported difficulty of the songs. Our results further substantiate the existence of a harmonic ‘subjectivity ceiling': an upper bound for evaluations in computational harmony research. Current state-of-the-art chord-estimation systems perform beyond this subjectivity ceiling by about 10 percent. This suggests that current ACE algorithms are powerful enough to tune themselves to particular annotators' idiosyncrasies. Overall, our results show that annotator subjectivity is an important factor in harmonic transcriptions, which should inform future studies into harmony perception and computational models of harmony.},
author = {Koops, Hendrik Vincent and de Haas, W. Bas and Burgoyne, John Ashley and Bransen, Jeroen and Kent-Muller, Anna and Volk, Anja},
xdoi = {10.1080/09298215.2019.1613436},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {Annotator subjectivity,annotator subjectivity,harmony,inter-rater,inter-rater agreement},
month = {may},
number = {0},
pages = {1--21},
publisher = {Taylor {\&} Francis},
title = {{Annotator subjectivity in harmony annotations of popular music}},
xurl = {https://www.tandfonline.com/xdoi/full/10.1080/09298215.2019.1613436},
volume = {0},
year = {2019}
}
@inproceedings{Schreiber:2018:TempoEstimation:ISMIR,
abstract = {We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional mel-spectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system's CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances.},
address = {Paris, France},
author = {Schreiber, Hendrik and M{\"{u}}ller, Meinard},
booktitle = {ISMIR, International Society for Music Information Retrieval Conference},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schreiber, M{\"{u}}ller - 2018 - A Single-Step Approach to Musical Tempo Estimation using a Convolutional Neural Network.pdf:pdf},
pages = {98--105},
title = {{a Single-Step Approach To Musical Tempo Estimation Using a Convolutional Neural Network}},
year = {2018}
}
@article{Essid:2006:InstrumentRecognition:IEEE_TASLP,
abstract = {Journal Helia started its "mission" nearly 36 years ago, very unpreten- tiously, as an information bulletin FAO Research Network on Sunflower, with the founders' intention to allow a quick and easy exchange of knowledge gained from experimental field trials of improving and breeding important oilseed crop species, such as sunflower. With the time passing, the number of scien- tists gathered around the project development and promotion of sunflower growing under the FAO Research Network on Sunflower has risen slowly but steadily, while the projects have become more extensive, complex and serious. The abundance of scientific research results, realized in the framework of a research network, determined the direction of the evolution journal Helia in scientific journal with internationally recognized quality, peer-reviewed papers and its relatively high ranking in the scientific society. Since the beginning of publishing in Serbia (1990th), the Journal pub- lished by the Institute of Field and Vegetable Crops, as the publisher and edito- rial office headquarters, was under the auspices of F.A.O. and ISA until the 2006th when the editorial office was transferred to the Serbian Academy of Sci- ences and Arts; Branch in Novi Sad, which also assumed the role of the main publisher, while the Institute remained co-publisher.Since 1990 a total of 24 volumes with 47 regular and two extraordinary numbers have been published in the scientific journal HELIA. That is a pretty impressive library with 6900 pages of printed material in 746 scientific papers in English. So far 2125 authors and co-authors of scientific papers from 43 countries from all continents have participated in publishing scientific papers in the journal All submitted manuscripts are subjected to anonymous international review (so-called "single-blind peer review", where the authors of the papers do not know who the reviewers are, but the reviewers know who the authors of the papers are) and published in the journal only after receiving a positive review by two independent reviewers and the final opinion of the editor. Regarding the impact factor of the ISJ Helia in the last 10 years, accord- ing to citation indicators of some papers published in the journal, it can be concluded that it has had relatively high levels over the past 10 years, with a trend of significant increase in 2011 and 2012. Focusing on that parameter, and relatively high two-and five-year impact factors in 2011 and 2012, we can be very satisfied about these trends, which have led to our journal being ranked near relatively influential journals on the global level.},
author = {Essid, Slim and Richard, Ga{\"{e}}l and David, Bertrand},
file = {:C$\backslash$:/Users/abr/Downloads/SE{\_}TSALP-06a.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {1401--1412},
title = {{Pairwise Classification Strategies}},
volume = {14},
year = {2006}
}
@inproceedings{Hung:2019:InstrumentRecognition:ICASSP,
address = {Brighton, UK},
author = {Hung, Yun-Ning and Chen, Yi-An and Yang, Yi-Hsuan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683426},
keywords = {instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
month = {may},
pages = {381--385},
title = {{Multitask Learning for Frame-level Instrument Recognition}},
xurl = {https://ieeexplore.ieee.org/document/8683426/},
year = {2019}
}
@inproceedings{McCallum:2019:Segmentation:ICASSP,
address = {Brighton, UK},
author = {McCallum, Matthew C.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683407},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCallum - 2019 - Unsupervised Learning of Deep Features for Music Segmentation.pdf:pdf},
isbn = {978-1-4799-8131-1},
keywords = {segmentation},
mendeley-tags = {segmentation},
month = {may},
pages = {346--350},
publisher = {IEEE},
title = {{Unsupervised Learning of Deep Features for Music Segmentation}},
xurl = {https://ieeexplore.ieee.org/document/8683407/},
year = {2019}
}
@inproceedings{Ren:2019:AttrousCNNAttention:ICASSP,
author = {Ren, Zhao and Kong, Qiuqiang and Han, Jing and Plumbley, Mark D and Schuller, Bjorn W.},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683434},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2019 - Attention-based Atrous Convolutional Neural Networks Visualisation and Understanding Perspectives of Acoustic Scenes.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {56--60},
title = {{Attention-based Atrous Convolutional Neural Networks: Visualisation and Understanding Perspectives of Acoustic Scenes}},
xurl = {https://ieeexplore.ieee.org/document/8683434/},
year = {2019}
}
@inproceedings{Phan:2019:EventDetection:ICASSP,
address = {Brighton, UK},
author = {Phan, Huy and Chen, Oliver Y. and Koch, Philipp and Pham, Lam and McLoughlin, Ian and Mertins, Alfred and Vos, Maarten De},
booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683064},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {51--55},
publisher = {IEEE},
title = {{Unifying Isolated and Overlapping Audio Event Detection with Multi-label Multi-task Convolutional Recurrent Neural Networks}},
xurl = {https://ieeexplore.ieee.org/document/8683064/},
year = {2019}
}
@inproceedings{Hou:2019:SoundEvent:ICASSP,
author = {Hou, Yuanbo and Kong, Qiuqiang and Li, Shengchen and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/icassp.2019.8683627},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {46--50},
title = {{Sound Event Detection with Sequentially Labelled Data Based on Connectionist Temporal Classification and Unsupervised Clustering}},
year = {2019}
}
@inproceedings{Wang:2019:MultipleInstanceLearning:ICASSP,
address = {Brighton, UK},
author = {Wang, Yun and Li, Juncheng and Metze, Florian},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682847},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {31--35},
title = {{A Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling}},
xurl = {https://ieeexplore.ieee.org/document/8682847/},
year = {2019}
}
@inproceedings{Kothinti:2019:SoundEvent:ICASSP,
address = {Brighton, UK},
author = {Kothinti, Sandeep and Imoto, Keisuke and Chakrabarty, Debmalya and Sell, Gregory and Watanabe, Shinji and Elhilali, Mounya},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682772},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {36--40},
title = {{Joint Acoustic and Class Inference for Weakly Supervised Sound Event Detection}},
xurl = {https://ieeexplore.ieee.org/document/8682772/},
year = {2019}
}
@inproceedings{Lin:2019:SingingVoiceSeparation:ICASSP,
address = {Brighton, UK},
author = {{Edward Lin}, Kin Wah and Goto, Masataka},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682958},
file = {::},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {251--255},
title = {{Zero-mean Convolutional Network with Data Augmentation for Sound Level Invariant Singing Voice Separation}},
xurl = {https://ieeexplore.ieee.org/document/8682958/},
year = {2019}
}
@inproceedings{Chou:2019:Attention:ICASSP,
address = {Brighton, UK},
author = {Chou, Szu-Yu and Cheng, Kai-Hsiang and Jang, Jyh-Shing Roger and Yang, Yi-Hsuan},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682558},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {26--30},
title = {{Learning to Match Transient Sound Events Using Attentional Similarity for Few-shot Sound Recognition}},
xurl = {https://ieeexplore.ieee.org/document/8682558/},
year = {2019}
}
@inproceedings{Pons:2019:FewData:ICASSP,
address = {Brighton, UK},
author = {Pons, Jordi and Serra, Joan and Serra, Xavier},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682591},
file = {::},
isbn = {978-1-4799-8131-1},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {16--20},
title = {{Training Neural Audio Classifiers with Few Data}},
xurl = {https://ieeexplore.ieee.org/document/8682591/},
year = {2019}
}
@inproceedings{Jati:2019:EventDetection:ICASSP,
address = {Brighton, UK},
author = {Jati, Arindam and Kumar, Naveen and Chen, Ruxin and Georgiou, Panayiotis},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682341},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
pages = {6--10},
title = {{Hierarchy-aware Loss Function on a Tree Structured Label Space for Audio Event Detection}},
xurl = {https://ieeexplore.ieee.org/document/8682341/},
year = {2019}
}
@inproceedings{Imoto:2019:EventDetection:ICASSP,
abstract = {The types of sound events that occur in a situation are limited, and some sound events are likely to co-occur; for instance, ``dishes'' and ``glass jingling.'' In this paper, we introduce a technique of sound event detection utilizing graph Laplacian regularization taking the sound event co-occurrence into account. To consider the co-occurrence of sound events in a sound event detection system, we first represent sound event occurrences as a graph whose nodes indicate the frequency of event occurrence and whose edges indicate the co-occurrence of sound events. We then utilize this graph structure for sound event modeling, which is optimized under an objective function with a regularization term considering the graph structure. Experimental results obtained using TUT Acoustic Scenes 2016 development and 2017 development datasets indicate that the proposed method improves the detection performance of sound events by 7.9 percentage points compared to that of the conventional CNN-BiGRU-based method in terms of the segment-based F1-score. Moreover, the results show that the proposed method can detect co-occurring sound events more accurately than the conventional method.},
address = {Brighton, UK},
author = {Imoto, Keisuke and Kyochi, Seisuke},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683708},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {may},
pages = {1--5},
title = {{Sound Event Detection Using Graph Laplacian Regularization Based on Event Co-occurrence}},
xurl = {https://ieeexplore.ieee.org/document/8683708/},
year = {2019}
}
@inproceedings{Shibata:2019:JointTranscription:ICASSP,
address = {Brighton, UK},
author = {Shibata, Kentaro and Nishikimi, Ryo and Fukayama, Satoru and Goto, Masataka and Nakamura, Eita and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8682817},
file = {::},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {236--240},
publisher = {IEEE},
title = {{Joint Transcription of Lead, Bass, and Rhythm Guitars Based on a Factorial Hidden Semi-markov Model}},
xurl = {https://ieeexplore.ieee.org/document/8682817/},
year = {2019}
}
@inproceedings{Kelz:2019:PianoTranscription:ICASSP,
abstract = {We investigate a late-fusion approach to piano transcription , combined with a strong temporal prior in the form of a handcrafted Hidden Markov Model (HMM). The network architecture under consideration is compact in terms of its number of parameters and easy to train with gradient descent and momentum. The network outputs are fused over time in the final stage to obtain note segmentations, with an HMM whose transition-and observation probabilities are chosen based on a model of attack decay sustain release (ADSR) envelope. The note segments are then subject to a final binary decision rule to reject too weak note segment hypotheses. 1. METHODS We would like to transcribe a polyphonic audio recording of a piano into a symbolic score. For each note sounding, we expect to obtain a tuple (s, e, n, v), denoting start, end, MIDI note number, and optionally, volume. 1.1 Deep convolutional neural network We employ a model with multiple outputs, predicting different note phases. A conceptual drawing is shown in Figure 1. The network input x t ∈ R c×b is a spectro-gram snippet, extending c context frames in time, and b bins in frequency. b is the number of bins resulting from passing a linear STFT through a filterbank with semi-logarithmically spaced, triangular filters, resulting in a resolution of approximately two bins per semitone. We choose c = 11, b = 144. The temporal resolution of the model is 50 [frames/s]. The target matrix y t ∈ {\{}0, 1{\}} 88×3 decomposes into vectors y on t , y f rm t , and y off t respectively, denoting the onset , the intermediate note phases, and the offset for each note for the center frame within the context window c.},
address = {Brighton, UK},
author = {Kelz, Rainer and Bock, Sebastian and Widmer, Gerhard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2019.8683582},
file = {::},
isbn = {978-1-4799-8131-1},
month = {may},
pages = {246--250},
publisher = {IEEE},
title = {{Deep Polyphonic ADSR Piano Note Transcription}},
xurl = {https://ieeexplore.ieee.org/document/8683582/},
year = {2019}
}
@book{Virtanen:2018:SoundSceneBook:BOOK,
address = {Cham, Switzerland},
xdoi = {10.1007/978-3-319-63450-0},
editor = {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Virtanen, Plumbley, Ellis - Unknown - Computational Analysis of Sound Scenes and Events.pdf:pdf},
publisher = {Springer International Publishing},
title = {{Computational Analysis of Sound Scenes and Events}},
xurl = {http://link.springer.com/10.1007/978-3-319-63450-0},
year = {2018}
}
@inproceedings{Virtanen:2019:MachineListeningTutorial:ICASSP,
address = {Brighton, UK},
author = {Virtanen, Tuomas and Mesaros, Annamaria and Heittola, Toni},
booktitle = {Tutorial at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
title = {{Detection and Classification of Acoustic Scenes and Events}},
year = {2019}
}
@inproceedings{Mueller:2019:MusicTutorial:ICASSP,
address = {Brighton, UK},
author = {M{\"{u}}ller, Meinard and Arzt, Andreas and Balke, Stefan},
booktitle = {Tutorial at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
isbn = {0780370414},
title = {{Cross-Modal Music Retrieval and Applications}},
year = {2019}
}
@book{Fletcher:1991:PhysicsMusicalInstruments:BOOK,
author = {Fletcher, Neville H. and Rossing, Thomas D.},
publisher = {Springer},
title = {{The Physics of Musical Instruments}},
year = {1991}
}
@article{Meddis:1986:SimulationNeuralTransduction:ASOC,
abstract = {A probabilistic model is described for transmitter release from hair cells, auditory neuron EPSP's, and discharge patterns. The model assumes that the release fraction of the transmitter is a function of stimulus intensity. It further assumes that some of this transmitter substance is taken back into the cell while some is irretrievably lost from the cleft. These assumptions differ from other recent models which propose multiple release sites, fixed release fractions, and no transmitter reuptake. The model produces realistic mammalian rate intensity functions, interval and period histograms, incremental responses, and adaptation effects. It mimics successfully the adaptation of successive EPSP amplitudes of the afferent neuron of the goldfish sacculus and offers a reinterpretation of the implications of these studies for hair cell synaptic mechanism.},
author = {Meddis, Ray},
file = {::},
journal = {The Journal of the Acoustical Society of America},
pages = {702--11},
title = {{Simulation of mechanical to neural transduction in the auditory receptor}},
year = {1986}
}
@inproceedings{Pielemeier:1996:TimeFreqAnalysisMusicalSignals:IEEE,
abstract = {The major time and frequency analysis methods that have been applied to music processing are traced and application areas described. Techniques are examined in the context of Cohen's class, facilitating comparison and the design of new approaches. A trumpet example illustrates most techniques. The impact of different analysis methods on pitch and timbre examination is shown. Analyses spanning Fourier series and transform, pitch synchronous analysis, heterodyne jilter, short-time Fourier transform (STFT), phase vocoder, constant-{\&} and wavelet transforms, the Wigner distribution, and the modal distribution are all covered. The limitations of windowing methods and their reliance on steady-state assumptions and infnite duration sinusoids to define frequency and amplitude are detailed. The Wigner distribution, in contrast, uses the analytic signal to define instantaneous frequency and power parameters. The modal distribution is shown to be a linear transformation of the Wigner distribution optimized for estimating those parameters for a musical signal model. Application areas consider analysis, resynthesis, transcription, and visualization. The more stringent requirements for time-frequency (TF) distributions in these applications are compared with the weaker requirements found in speech analysis and highlight the need for further theoretical research.},
author = {Pielemeier, William J and Wakefield, Gregory H and Simoni, Mary H},
booktitle = {Proceedings of the IEEE},
file = {::},
isbn = {00189219/96{\$}05.0},
title = {{Time-frequency analysis of musical signals}},
year = {1996}
}
@inproceedings{Hedelin:1981:ToneOrientedVoiceExcitedVocoder:ICASSP,
abstract = {An LPC base-band vocoder is developed. The novel feature concerns the coding of the base-band. A model is set up for the base-band as a set of modulated tones. Algorithms are presented for the extraction of amplitude and phase/frequency of the tones. Implementation aspects as well as simulation results are discussed. Total bit rates in the order of 3,2-4.8 kbits are possible where approximately one half of the bits represents the base-band coding. Experiments have shown the coder to be robust to background noi as.},
author = {Hedelin, Per},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
title = {{A Tone-Oriented Voice-Excited Vocoder}},
year = {1981}
}
@inproceedings{Almeida:1984:VarFreqSynHarmonicCoding:ICASSP,
abstract = {The Harmonic Coding concept has already shown its potential for efficiently coding speech. Previous implementations have usec a frame rate of one every 16 ms. This was mainly due to the fact that, with longer frames, even a nonstationary spectral model (of low order) cannot reproduce the zones of fast-varying pitch with the desirable quality. However, the high framing rate is a limitation, since it implies that fewer bits will be available for encoding each frame. A solution for this problem has been devised: the signal is synthesized in the time domain, as a superimposition of "harmonics" whose instantaneous frequency varies continuously along an interpolation curve, within each frame. In this way, fast pitch variations can be tracked with no difficulty. Experimental results are presented, confirming these facts. The integration of this synthesis scheme in a speech coder is discussed.},
author = {Almeida, L. and Silva, F.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/icassp.1984.1172489},
file = {::},
month = {mar},
pages = {437--440},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Variable-frequency synthesis: An improved harmonic coding scheme}},
year = {1984}
}
@inproceedings{Kareer2018,
abstract = {Knowing the number of sources present in a mixture is useful for many computer audition problems such as polyphonic music transcription, source separation, and speech enhancement. Most existing algorithms for these applications require the user to provide this number thereby limiting the possibility of complete automatization. In this paper we explore a few probabilistic and machine learning approaches for an autonomous source number estimation. We then propose an implementation of a multi-class classification method using convolutional neural networks for musical polyphony estimation. In addition, we use these results to improve the performance of an instrument classifier based on the same dataset. Our final classification results for both the networks, prove that this method is a promising starting point for further advancements in unsupervised source counting and separation algorithms for music and speech.},
address = {Milan, Italy},
author = {Kareer, Saarish and Basu, Sattwik},
booktitle = {Audio Engineering Society Convention 144},
file = {::},
title = {{Musical Polyphony Estimation}},
year = {2018}
}
@inproceedings{Kehling:2014:GuitarTranscription:DAFX,
address = {Erlangen, Germany},
author = {Kehling, Christian and Abe{\ss}er, Jakob and Dittmar, Christian and Schuller, Gerald},
booktitle = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
file = {::},
keywords = {guitar{\_}transcription,m2d},
mendeley-tags = {guitar{\_}transcription,m2d},
pages = {1--8},
title = {{Automatic Tablature Transcription of Electric Guitar Recordings by Estimation of Score- and Instrument-Related Parameters}},
year = {2014}
}
@article{Barbancho:2012:GuitarTranscription:IEEE_TASLP,
abstract = {In this paper, a system for the extraction of the tablature of guitar musical pieces using only the audio waveform is presented. The analysis of the inharmonicity relations between the fundamentals and the partials of the notes played is the main process that allows to estimate both the notes played and the string/fret combination that was used to produce that sound. A procedure to analyze chords will also be described. This proce- dure will also make use of the inharmonicity analysis to find the simultaneous string/fret combinations used to play each chord. The proposed method is suitable for any guitar type: classical, acoustic and electric guitars. The system performance has been evaluated on a series of guitar samples from the RWC instruments database and our own recordings.},
author = {Barbancho, Isabel and Tard{\'{o}}n, Lorenzo J. and Sammartino, Simone and Barbancho, Ana M.},
xdoi = {10.1109/TASL.2012.2191281},
file = {::},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Fret,guitar,inharmonicity,music analysis,music{\_}transcription,pitch estimation,string,string{\_}estimation,tablature},
mendeley-tags = {music{\_}transcription,string{\_}estimation},
number = {6},
pages = {1857--1868},
title = {{Inharmonicity-based method for the automatic generation of guitar tablature}},
volume = {20},
year = {2012}
}
@inproceedings{Cano:2009:MelodyLineSeparation:DAFX,
abstract = {We propose a system which separates saxophone melodies from composite recordings of saxophone, piano, and/or orchestra. The system is intended to produce an accompaniment sans saxophone suitable for rehearsal and practice purposes. A Melody Line Detection (MLD) algorithm is proposed as the starting point for a source separation implementation which incorporates known information about typical saxophone melody lines, acoustic characteristics and range of the saxophone in order to prevent and correct detection errors. By extracting reliable information about the soloist melody line, the system separates piano or orchestra accompaniments from the solo part. The system was tested with commercial recordings and a performance of 79.7{\%} of accurate detections was achieved. The accompaniment tracks obtained after source separation successfully remove most of the saxophone sound while preserving the original nature of the accompaniment track.},
author = {Cano, Estefan{\'{i}}a and Cheng, Corey},
file = {::},
title = {{Melody Line Detection and Source Separation in Classical Saxophone Recordings}}
}
@inproceedings{Virtanen:2000:HarmonicSeparationSinusoidalModel:ICASSP,
author = {Virtanen, Tuomas and Klapuri, Annsi},
booktitle = {in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {isad,partial{\_}tracking,source{\_}separation},
mendeley-tags = {isad,partial{\_}tracking,source{\_}separation},
title = {{Separation of Harmonic Sound Sources Using Sinusoidal Modeling}},
year = {2000}
}
@inproceedings{Bonada:2008:WideBandModel:DAFX,
abstract = {In this paper we propose a method to estimate and transform harmonic components in wide-band conditions, out of a single period of the analyzed signal. This method allows estimating harmonic parameters with higher temporal resolution than typical Short Time Fourier Transform (STFT) based methods. We also discuss transformations and synthesis strategies in such context, focusing on the human voice.},
author = {Bonada, Jordi},
booktitle = {Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
title = {{Wide-Band Harmonic Sinusoidal Modelling}},
year = {2008}
}
@incollection{Bonada:2011:SpectralProcessing:DAFX_book,
author = {Bonada, Jordi and Serra, Xavier and Amatriain, Xavier and Loscos, Alex},
booktitle = {DAFX: Digital Audio Effects},
chapter = {Spectral P},
edition = {Second Edi},
editor = {{Udo Z{\"{o}}lzer}},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {394--445},
title = {{Spectral Processing}},
year = {2011}
}
@inproceedings{Raspaud:2008:BinauralPartialTracking:DAFX,
abstract = {Partial tracking in sinusoidal models have been studied for over twenty years now, and have been enhanced, making it precise and useful to analyse noiseless harmonic sounds. However, such tools have always been used in a monophonic (single channel) context. A method is thus proposed to adapt the partial tracking to the case of binaural signals. This gives a tool to perform spectral analysis of such signals, keeping relevant information from both left and right channels. Moreover, azimuth (position in the horizontal plane) information for each partial is gained using interaural cues, such as interaural time differences (ITDs) and interaural level differences (ILDs). The azimuth information can then be used as an attribute or as a constraint in the binaural partial tracking algorithm. Finally, some classification results using the azimuth of partials are presented.},
address = {Espoo, Finland},
author = {Raspaud, Martin and Evangelista, Gianpaolo},
booktitle = {Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)},
isbn = {9789512295173},
title = {{Binaural partial tracking}},
year = {2008}
}
@incollection{Serra:1997:MusicalSoundModeling:MSP,
author = {Serra, Xavier},
booktitle = {Musical Signal Processing},
editor = {Roads, C. and Pope, S. and Picialli, A. and {De Poli}, G.},
file = {::},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
pages = {91--122},
publisher = {Swets {\&} Zeitlinger},
title = {{Musical Sound Modeling with Sinusoids plus Noise}},
year = {1997}
}
@article{Flanagan:1966:PhaseVocoder:BSTJ,
author = {Flanagan, James L.. and Golden, Roger M.},
file = {::},
journal = {The Bell System Technical Journal},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {1493--1509},
title = {{Phase Vocoder}},
volume = {45},
year = {1966}
}
@inproceedings{Abesser:2015:IntonationModulation:ISMIR,
address = {M{\'{a}}laga, Spain},
author = {Abe{\ss}er, Jakob and Cano, Estefan{\'{i}}a and Frieler, Klaus and Pfleiderer, Martin and Zaddach, Wolf-Georg},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - Unknown - SCORE-INFORMED ANALYSIS OF INTONATION AND PITCH MODULATION IN JAZZ SOLOS(2).pdf:pdf},
keywords = {melody{\_}contour{\_}analysis},
mendeley-tags = {melody{\_}contour{\_}analysis},
pages = {823--829},
title = {{Score-informed analysis of intonation and pitch modulation in jazz solos}},
year = {2015}
}
@inproceedings{Pantelli:2017:F0Contours:ICASSP,
author = {Panteli, Maria and Bittner, Rachel and Bello, Juan Pablo and Dixon, Simon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2017.7952233},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panteli et al. - Unknown - TOWARDS THE CHARACTERIZATION OF SINGING STYLES IN WORLD MUSIC Centre for Digital Music , Queen Mary Universit.pdf:pdf},
isbn = {978-1-5090-4117-6},
keywords = {melody{\_}contour{\_}analysis},
mendeley-tags = {melody{\_}contour{\_}analysis},
month = {mar},
pages = {636--640},
publisher = {IEEE},
title = {{Towards the characterization of singing styles in world music}},
xurl = {http://ieeexplore.ieee.org/document/7952233/},
year = {2017}
}
@article{Abesser:2017:BassGuitar:IEEE_TASLP,
author = {Abe{\ss}er, Jakob and Schuller, Gerald},
xdoi = {10.1109/TASLP.2017.2702384},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er, Schuller - 2017 - Instrument-Centered Music Transcription of Solo Bass Guitar Recordings.pdf:pdf},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
keywords = {acoustic signal processing,audio signal processing},
number = {9},
pages = {1741--1750},
title = {{Instrument-Centered Music Transcription of Solo Bass Guitar Recordings}},
volume = {25},
year = {2017}
}
@inproceedings{Abesser:2019:F0Contours:ICASSP,
address = {Brighton, UK},
author = {Abe{\ss}er, Jakob and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {idmt,melody{\_}contour{\_}analysis},
mendeley-tags = {idmt,melody{\_}contour{\_}analysis},
title = {{Fundamental Frequency Contour Classification: A Comparison Between Hand-Crafted and CNN-Based Features}},
year = {2019}
}
@book{Pfleiderer:2018:Jazzomat:BOOK,
editor = {Pfleiderer, Martin and Frieler, Klaus and Abe{\ss}er, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - Inside the Jazzomat - New Perspectives for Jazz Research.pdf:pdf},
publisher = {Schott Campus},
title = {{Inside the Jazzomat - New Perspectives for Jazz Research}},
year = {2018}
}
@article{Abesser:2017:ScoreInformedJazz:IEEE_TASLP,
author = {Abe{\ss}er, Jakob and Frieler, Klaus and Pfleiderer, Martin and Zaddach, Wolf-georg},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abe{\ss}er et al. - 2012 - Score-Informed Analysis of Tuning , Intonation , Pitch Modulation , and Dynamics in Jazz Solos(2).pdf:pdf},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {168--177},
title = {{Score-Informed Analysis of Tuning , Intonation , Pitch Modulation , and Dynamics in Jazz Solos}},
volume = {25},
year = {2017}
}
@inproceedings{Abesser:2010:BassPlayingStyles:ICASSP,
address = {Dallas, TX, USA},
author = {Abe{\ss}er, Jakob and Lukashevich, Hanna and Schuller, Gerald},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2010.5495945},
file = {::},
keywords = {bass{\_}transcription},
mendeley-tags = {bass{\_}transcription},
pages = {2290--2293},
title = {{Feature-Based Extraction of Plucking and Expression Styles of the Electric Bass Guitar}},
year = {2010}
}
@article{Lucic:2019:HighFidelityImageGeneration:ARXIV,
abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform state-of-the-art (SOTA) on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the art conditional model BigGAN on ImageNet using only 10{\%} of the labels and outperform it using 20{\%} of the labels.},
archivePrefix = {arXiv},
arxivId = {1903.02271},
author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
eprint = {1903.02271},
file = {::},
journal = {ArXiv pre-prints},
keywords = {generative{\_}adversarial{\_}network},
mendeley-tags = {generative{\_}adversarial{\_}network},
month = {mar},
title = {{High-Fidelity Image Generation With Fewer Labels}},
xurl = {http://arxiv.org/abs/1903.02271},
year = {2019}
}
@article{Purwins:2019:DeepLearningASP:IEEE_JSTSP,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.00078v1},
author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schl{\"{u}}ter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
xdoi = {10.1109/JSTSP.2019.2908700},
eprint = {arXiv:1905.00078v1},
file = {::},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {deep{\_}learning},
mendeley-tags = {deep{\_}learning},
number = {8},
pages = {1--14},
title = {{Deep Learning for Audio Signal Processing}},
xurl = {https://ieeexplore.ieee.org/document/8678825/},
volume = {14},
year = {2019}
}
@article{Abdoli:2009:1DConvEnvSounds:ARXIV,
abstract = {In this paper, we present an end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network (CNN) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal's fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the UrbanSound8k dataset and the experimental results have shown that it achieves 89{\%} of mean accuracy. Therefore, the propose approach outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input. Furthermore, the proposed approach has a small number of parameters compared to other architectures found in the literature, which reduces the amount of data required for training.},
archivePrefix = {arXiv},
arxivId = {1904.08990},
author = {Abdoli, Sajjad and Cardinal, Patrick and Koerich, Alessandro Lameiras},
eprint = {1904.08990},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdoli, Cardinal, Koerich - 2019 - End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network.pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {convolutional neural network,deep,environmental sound classification,gammatone filterbank,learning,machine{\_}listening},
mendeley-tags = {machine{\_}listening},
month = {apr},
pages = {1--24},
title = {{End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network}},
xurl = {http://arxiv.org/abs/1904.08990},
year = {2019}
}
@inproceedings{Dressler:2011:PitchEstimation:AES,
author = {Dressler, Karin},
booktitle = {Information Retrieval},
file = {:C$\backslash$:/Users/abr/Downloads/5c02980f4a4ae56ebc4eb227a20ac2055e31.pdf:pdf},
keywords = {pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
pages = {1--10},
title = {{Pitch Estimation by the Pair-Wise Evaluation of Spectral Peaks}},
year = {2011}
}
@article{Salamon:2012:Melodia:IEEE_TASLP,
author = {Salamon, Justin and G{\'{o}}mez, Emilia},
file = {:C$\backslash$:/Users/abr/Downloads/06155601.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {music transcription,pitch{\_}tracking},
mendeley-tags = {music transcription,pitch{\_}tracking},
number = {6},
pages = {1759--1770},
title = {{Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics}},
volume = {20},
year = {2012}
}
@inproceedings{Mauch:2014:PYINPitchTracker:ICASSP,
abstract = {We propose the Probabilistic YIN (PYIN) algorithm, a modification of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online1 as an open source C++ library for Vamp hosts.},
address = {Florence, Italy},
author = {Mauch, Matthias and Dixon, Simon},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2014.6853678},
file = {:C$\backslash$:/Users/abr/Downloads/06853678.pdf:pdf},
keywords = {Pitch estimation,YIN,pitch tracking,pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
number = {1},
pages = {659--663},
title = {{PYIN: A fundamental frequency estimator using probabilistic threshold distributions}},
year = {2014}
}
@article{Cheveigne:2002:YinPitchTracking:JASA,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
xdoi = {10.1121/1.1458024},
file = {:C$\backslash$:/Users/abr/Downloads/2002{\_}JASA{\_}YIN.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
month = {apr},
number = {4},
pages = {1917--1930},
title = {{YIN, a fundamental frequency estimator for speech and music}},
xurl = {http://asa.scitation.org/xdoi/10.1121/1.1458024},
volume = {111},
year = {2002}
}
@inproceedings{Kim:2018:PitchTracking:ICASSP,
abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
address = {New Orleans, USA},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2018.8461329},
file = {:C$\backslash$:/Users/abr/Downloads/08461329.pdf:pdf},
keywords = {Convolutional neural network,Pitch estimation,pitch{\_}tracking},
mendeley-tags = {pitch{\_}tracking},
pages = {161--165},
title = {{Crepe: A Convolutional Representation for Pitch Estimation}},
year = {2018}
}
@book{Mueller:2015:MusicProcessing:BOOK,
author = {M{\"{u}}ller, Meinard},
isbn = {978-3-319-21944-8},
publisher = {Springer},
title = {{Fundamentals of Music Processing}},
year = {2015}
}
@article{Flanagan:1980:SpeechParametricCoding:ASOC,
abstract = {We suggest that a class of speech coders can be designed from criteria that are perception-specific. Such a class represents a middle ground between "waveform" coders and speech-specific "source" coders. We argue that, within perceptually acceptable limits, a bandpass portion of a speech signal can be represented by its short-time spectral amplitude [S• [and its phase derivative q•. Both parameters are evaluated at the center frequency to. of the passband whose spectral width is W. rad/s. We show that the short-time spectral amplitude is identical to the Hilbert envelope A. of the passband waveform. As a by-product, we show that the bandwidth occupied by both A. 2 and A. 2 q•. is the same as that for their parent bandpass signal, but their spectra are translated to the range 0 to W.. No comparable statements can be adduced for A. and q•., but on practical grounds, we conjecture that useful low-pass limits of less than W./2 can be applied to both. Finally, we outline techniques to utilize the passband parameters A., q•., A. • and A. • q•n in digital voice coding},
author = {Flanagan, James L.},
xdoi = {10.1121/1.384752},
file = {::},
journal = {The Journal of the Acoustical Society of America},
keywords = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
mendeley-tags = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
month = {oct},
number = {2},
pages = {412--419},
publisher = {Acoustical Society of America (ASA)},
title = {{Parametric coding of speech spectra}},
volume = {68},
year = {1980}
}
@inproceedings{Atal:1982:LPCspeech:ICASSP,
abstract = {The excitation for LPC speech synthesis usually consists of two separate signals - a delta-function pulse once every pitch penod for voiced speech and white noise for unvoiced speech. This manner of representing excitation requires that speech segments be classified accurately into voiced and unvoiced categories and the pitch period of voiced segments be known. It is now well recognised that such a rigid idealization of the vocal excitation is often responsible for the unnatural qualizy a.ssock{\#}ed with synthesized speech. This paper describes a new approach to the excitation problem that does not require a priori knowledge of either the voiced-unvoiced decision or the pitch period. All classes of sounds are generated by exciting the LPC filter with a sequence of pulses; the amplitudes and locations of the pulses are determined using a non-iterative analysis-by-synthesis procedure. This procedure minimizes a perceptual-distance metric representing subjectively-important differences between the waveforms of the original and the synthetic speech signals. The disiance metric takes account of the finite-frequency resolution as well as the ditTerential sensitivity of the human ear to errors in the formant and inter-formant regions of the speech spectrum.},
author = {Atal, Bishnu S. and Remde, Joel R.},
booktitle = {International Conference on Acoustics, Speech and Signal Processing},
xdoi = {10.1109/icassp.1982.1171649},
file = {::},
keywords = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
mendeley-tags = {partial{\_}tracking,speech{\_}analysis{\_}synthesis},
month = {mar},
pages = {614--617},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{A new model of LPC excitation for producing natural-sounding speech at low bit rates}},
year = {1982}
}
@article{Robel:2006:AdapAddModelingContTraj:IEEE_TASLP,
author = {Robel, Axel},
xdoi = {10.1109/tsa.2005.858529},
file = {::},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
month = {jun},
number = {4},
pages = {1440--1453},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Adaptive Additive Modeling with Continuous Parameter Trajectories}},
volume = {14},
year = {2006}
}
@inproceedings{Bartkowiak:2011:NonTimeProgressiveSM:AES,
abstract = {In this paper we propose a new sinusoidal model tracking algorithm that implements a non-time-progressive way of data processing. Sinusoidal partial parameters are estimated in the consecutive frames; however, the order of establishing individual connections between partials is determined by a greedy rule within the whole signal or within a specific time window. In this way, the strongest connections may be determined early, and subsequent predictions of each trajectory evolution are based on a more reliable partial evolution history, compared to a traditional progressive scheme. As a consequence, the proposed non-progressive tracking algorithm offers a statistically significant improvement of obtained trajectories in terms of better classic pattern recognition measures.},
address = {New York, NY, USA},
author = {Bartkowiak, Maciej and {\.{Z}}ernicki, Tomasz},
booktitle = {Proceedings of the 131st Audio Engineering Society Convention},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {1--8},
title = {{A Non-Time-Progressive Partial Tracking Algorithm for Sinusoidal Modeling}},
year = {2011}
}
@article{Serra:1990:SpectralModelingSynthesis:CMJ,
author = {{Serra Xavier}, Julius Smith},
file = {::},
journal = {Computer Music Journal},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
number = {4},
pages = {12--24},
title = {{Spectral Modeling Synthesis: A Sound Analysis/Synthesis Based on a Deterministic plus Stochastic Decomposition}},
volume = {14},
year = {1990}
}
@inproceedings{Nadar:2019:ChordRecognitionCNN:SMC,
address = {M{\'{a}}laga, Spain},
author = {Nadar, Christon-Ragavan and Abe{\ss}er, Jakob and Grollmisch, Sascha},
booktitle = {Proceedings of the Sound {\&} Music Computing Conference (SMC)},
file = {:C$\backslash$:/Users/abr/Downloads/SMC{\_}2019{\_}Chord{\_}Recognition (3).pdf:pdf},
keywords = {chord{\_}recognition,idmt,m2d},
mendeley-tags = {chord{\_}recognition,idmt,m2d},
title = {{Towards CNN-based Acoustic Modeling of Seventh Chords for Recognition Chord Recognition}},
year = {2019}
}
@article{Marolt:2004:ConnectionistTranscription:IEEE_TOM,
abstract = {In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.},
author = {Marolt, Matija},
xdoi = {10.1109/TMM.2004.827507},
file = {::},
journal = {IEEE Transactions on Multimedia},
keywords = {Adaptive oscillators,Music transcription,Neural networks},
month = {jun},
number = {3},
pages = {439--449},
title = {{A Connectionist Approach to Automatic Transcription of Polyphonic Piano Music}},
volume = {6},
year = {2004}
}
@inproceedings{Lagrange:2005:TrackingSMPolyphonic:ICASSP,
abstract = {This paper proposes to further improve the tracking of partials in a polyphonic context. Spectral characteristics of the controlling parameters (amplitude and frequency) are taken into account to ensure that these parameters evolve slowly with time. The resulting algorithm better tracks closely-spaced sinusoids and is able to avoid most of the spectral data belonging to noise. As a consequence , the proposed algorithm extracts a more meaningful sinu-soidal representation from polyphonic recordings.},
address = {Philadelphia},
author = {Lagrange, Mathieu and Marchand, Sylvain and Rault, Jean-Bernard},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagrange et al. - 2005 - TRACKING PARTIALS FOR THE SINUSOIDAL MODELING OF POLYPHONIC SOUNDS.pdf:pdf},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {229--232, vol.3},
title = {{Tracking Partials for the Sinusoidal Modeling of Polyphonic Sounds}},
year = {2005}
}
@inproceedings{Lagrange:2003:PartialTrackingLinearPred:DAFX,
abstract = {In this paper, we introduce a new partial tracking method suitable for the sinusoidal modeling of mixtures of instrumental sounds with pseudo-stationary frequencies. This method, based on the linear prediction of the frequency evolutions of the partials, enables us to track these partials more accurately at the analysis stage, even in complex sound mixtures. This allows our spectral model to better handle polyphonic sounds.},
address = {London, UK},
author = {Lagrange, Mathieu and Marchand, Sylvain and Raspaud, Martin and Rault, Jean-Bernard},
booktitle = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {141--146},
title = {{Enhanced Partial Tracking Using Linear Prediction}},
xurl = {https://hal.archives-ouvertes.fr/hal-00308184},
year = {2003}
}
@inproceedings{Depalle:1993:PartialTrackingHMM:ICASSP,
address = {Minneapolis, MN},
author = {Depalle, Philippe and Garcia, Guillermo and Rodet, Xavier},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {225--228 vol.1},
title = {{Tracking of partials for additive sound synthesis using hidden Markov models}},
year = {1993}
}
@inproceedings{Fitz:1995:BandwidthEnhancedSM:ICMC,
abstract = {We present a system for sound modeling and synthesis that preserves the elegance and malleability of a sinusoidal model, while accommodating sounds with noisy (non-sinusoidal) components. We use an enhanced McAulay-Quatieri (MQ) style analysis that extracts bandwidth information in addition to the sinusoidal parameters for each partial.},
address = {Banff, Canada},
author = {Fitz, Kelly and Haken, Lippold},
booktitle = {Proceedings of the International Computer Music Conference},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {154--157},
title = {{Bandwidth Enhanced Sinusoidal Modeling in Lemur}},
year = {1995}
}
@inproceedings{Fitz:1992:MQLimitedOscillators:ICMC,
abstract = {The McAulay-Quatieri (MQ) analysis is a robust, general sinusoidal analysis technique. Unlike many other analysis techniques, it can be used to analyze sounds without a stable harmonic structure (i. e., polyphonic or non-harmonic sounds and instrument tones with extreme vibrato). The MQ technique can provide the time-varying spectral information needed to control a real-time additive synthesis engine. Unfortunately, the original MQ technique generates an arbitrary number of sinusoidal tracks, while a real-time system has a limited number of oscillators. This paper presents some improvements to the basic MQ analysis technique which, in addition to improving the quality of the ensuing syntheses, and making the sinusoidal model more robust, make the MQ analysis data more suitable for controlling a real-time sinusoidal synthesis engine with a fixed number of oscillators. 1. The Choice of the McAulay-Quatieri Model In our research, we have sought a model for sound that would accommodate synthesis with time scale modifications. The goal has been to find a model that would produce time-scaled syntheses of sampled audio signals that are otherwise perceptually equivalent to the original signals. We chose a sinusoidal model advanced by McAulay and Quatieri (McAulay and Quatieri 1985) as the basis for our research. The MQ sinusoidal model allows independent time-and frequency-scale modification. Many implementations of sinusoidal modeling derive sinusoidal components from the Short-Time Fourier Transform (STFT) and are of limited use for time-and frequency-scale modification because of artifacts caused by phase uncertainty and discotinuities. (A more detailed description of the STFT may be found in signal processing texts.) Pitch tracking analysis methods have been used in the past to solve some of these problems, but their use is restricted to the class of monophonic, strongly harmonic signals (Grey 1975, Haken 1989). McAulay and Quatieri (McAulay and Quatieri 1985) propose a sinusoidal analysis technique for speech processing. The premise of the MQ technique is that a sound can be represented by a collection of sinusoidal components (called tracks), each with time-varying amplitude and frequency. To construct these tracks, STFT's are performed on a signal at regular intervals, called frames. Amplitude peaks in the resulting frequency spectra are identified, and parabolic interpolation is used to obtain a close approximation of the exact spectral peak frequencies. These peaks are the most prominent frequencies in the sound at that instant. The peaks in adjacent frames are compared and peaks of similar frequencies are matched. A continuous chain of these matched peaks is a track. A peak that is not matched represents the birth or death of a track. MQ synthesis uses cubic phase interpolation to reduce phase uncertainty and eliminate phase discontinuities. 2. Lemur Extensions Lemur is a Macintosh implementation of the MQ technique with some extensions. It is based on the program MQAN, written by Rob Maher and James Beauchamp at the Computer Music Project at the University of Illinois (Maher 1989), which implemented the original MQ analysis/synthesis technique on a UNIX system. Lemur provides some extensions to the basic MQ technique.},
address = {San Jose, USA},
author = {Fitz, Kelly and Walker, William and Haken, Lippold},
booktitle = {Proceedings of the International Computer Music Conference},
file = {::},
keywords = {isad,partial{\_}tracking},
mendeley-tags = {isad,partial{\_}tracking},
pages = {381--382},
title = {{Extending the McAulay-Quatieri Analysis for Synthesis with a Limited Number of Oscillators}},
year = {1992}
}
@article{Barbedo:2011:InstrumentRecognition:IEEE_TASLP,
author = {Barbedo, Arnal Garcia Jayme and Tzanetakis, George},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcia, Barbedo, Tzanetakis - 2011 - Musical Instrument Classification Using Individual Partials.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {instrument recognition,partial{\_}tracking},
mendeley-tags = {instrument recognition,partial{\_}tracking},
number = {1},
pages = {111--122},
title = {{Musical Instrument Classification Using Individual Partials}},
volume = {19},
year = {2011}
}
@article{Grasis:2014:MultipleExpertInstrumentRecognition:LNCS,
author = {Grasis, Mikus and Abe{\ss}er, Jakob and Dittmar, Christian and Lukashevich, Hanna},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grasis et al. - 2014 - A Multiple-Expert Framework for Instrument Recognition.pdf:pdf},
isbn = {978-3-319-12976-1},
journal = {Sound, music, and motion. 10th International Symposium, CMMR 2013 : Marseille, France, October 15 - 18, 2013; Revised selected papers, Lecture Notes in Computer Science},
keywords = {Classifier ensemble,Decision fusion,Instrument recognition,Overtones,Partial tracking,Partial-wise features,instrument{\_}recognition},
mendeley-tags = {instrument{\_}recognition},
pages = {619--634},
title = {{A Multiple-Expert Framework for Instrument Recognition}},
volume = {8905},
year = {2014}
}
@article{Goto:2004:RealtimeMelodyBassTranscription:SP,
abstract = {In this paper, we describe the concept of music scene description and address the problem of detecting melody and bass lines in real-world audio signals containing the sounds of various instruments. Most previous pitch-estimation methods have had difficulty dealing with such complex music signals because these methods were designed to deal with mixtures of only a few sounds. To enable estimation of the fundamental frequency (F0) of the melody and bass lines, we propose a predominant-F0 estimation method called PreFEst that does not rely on the unreliable fundamental compo- nent and obtains the most predominant F0 supported by harmonics within an intentionally limited frequency range. This method estimates the relative dominance of every possible F0 (represented as a probability density function of the F0) by using MAP (maximum a posteriori probability) estimation and considers the F0!s temporal continuity by using a multiple-agent architecture. Experimental results with a set of ten music excerpts from compact-disc recordings showed that a real-time system implementing this method was able to detect melody and bass lines about 80{\%} of the time these existed. !},
author = {Goto, Masataka},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goto - 2004 - A real-time music scene description system Predominant-F0 estimation for detecting melody and bass lines in real-world aud.pdf:pdf},
journal = {Speech Communication},
keywords = {bass{\_}transcription,melody{\_}transcription},
mendeley-tags = {bass{\_}transcription,melody{\_}transcription},
pages = {311--329},
title = {{A real-time music scene description system: predominant-F0 estimation for detecting melody and bass lines in real-world audio signals}},
volume = {43},
year = {2004}
}
@article{Esterer:2019:LinearProgrammingParTrack:ARXIV,
abstract = {A new approach to the tracking of sinusoidal chirps using linear programming is proposed. It is demonstrated that the classical algorithm of McAulay and Quatieri is greedy and exhibits exponential complexity for long searches, while approaches based on the Viterbi algorithm exhibit factorial complexity. A linear programming (LP) formulation to find the best {\$}L{\$} paths in a lattice is described and its complexity is shown to be less than previous approaches. Finally it is demonstrated that the new LP formulation outperforms the classical algorithm in the tracking of sinusoidal chirps in high levels of noise.},
archivePrefix = {arXiv},
arxivId = {1901.05044},
author = {Esterer, Nicholas and Depalle, Philippe},
eprint = {1901.05044},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Esterer, Depalle - 2019 - A linear programming approach to the tracking of partials(2).pdf:pdf},
journal = {ArXiv pre-prints},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {jan},
title = {{A Linear Programming Approach to the Tracking of Partials}},
xurl = {http://arxiv.org/abs/1901.05044},
year = {2019}
}
@article{Marolt:2004:AdaptiveOscillators:JNMR,
abstract = {In this paper, we present a technique for tracking partials in musical signals, based on networks of adaptive oscillators. We show how synchronization of adaptive oscillators can be utilized to detect periodic patterns in outputs of a human auditory model and thus track stable frequency components (partials) in musical signals. The model is further extended to track groups of harmonically related partials by grouping oscillators into networks. We present the integration of the partial tracking model into a system for transcription of polyphonic piano music. The transcription system is based on a connectionist architecture that employs networks of adaptive oscillators for partial tracking and feed forward neural networks for associating partial groups with notes. We provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings.},
author = {Marolt, Matija},
xdoi = {10.1076/jnmr.33.1.49.35391},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {mar},
number = {1},
pages = {49--59},
title = {{Networks of Adaptive Oscillators for Partial Tracking and Transcription of Music Recordings}},
volume = {33},
year = {2004}
}
@inproceedings{Neri:2018:PartialTrackingLinearProgramming:DAFX,
abstract = {This paper proposes a new partial tracking method, based on linear programming, that can run in real-time, is simple to implement , and performs well in difficult tracking situations by considering spurious peaks, crossing partials, and a non-stationary short-term sinusoidal model. Complex constant parameters of a generalized short-term signal model are explicitly estimated to inform peak matching decisions. Peak matching is formulated as a variation of the linear assignment problem. Combinatorially optimal peak-to-peak assignments are found in polynomial time using the Hungarian algorithm. Results show that the proposed method creates high-quality representations of monophonic and polyphonic sounds.},
address = {Aveiro, Portugal},
author = {Neri, Julian and Depalle, Philippe},
booktitle = {Proceedings of the 21st International Conference on Digital Audio Effects (DAFx-18)},
file = {::},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
title = {{Fast Partial Tracking of Audio with Real-Time Capability Through Linear Programming}},
year = {2018}
}
@article{McAulay:1986:SpeechAnalysisSynthesis:IEEE_TASSP,
abstract = {A sinusoidal model for the speech waveform is used to develop a new analysis/synthesis technique that is characterized by the amplitudes, frequencies, and phases of the component sine waves. These parameters are estimated from the short-time Fourier ...},
author = {McAulay, Robert J. and Quatieri, Thomas F.},
xdoi = {10.1109/TASSP.1986.1164910},
file = {::},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
number = {4},
pages = {744--754},
title = {{Speech Analysis/Synthesis Based on a Sinusoidal Representation}},
volume = {34},
year = {1986}
}
@inproceedings{Satar:2005:PartialTrackingMusic:DAFX,
abstract = {In this paper we propose a novel approach for tracking of partials in music signals based on a robust Kalman filter. Our tracker is based on a regularized least-squares approach that is designed to minimize the worst-possible regularized residual norm over the class of admissible uncertainties at each iteration. We introduce a set of state-space models for our signals based on the evolution of frequency and amplitude in different classes of musical instruments. These prior models are used to estimate future values of partial tracks in successive time frames of our spectral data. Here, the parameters of evolution models are treated as bounded uncertainties and our tracker can robustly track partials in all frequency regions. Unlike the conventional Kalman tracker, performance of this tracker is not influenced by the magnified track variations in higher frequencies. This tracker promises an improved performance over conventional Kalman tracker while preserving its good properties and superiority over existing methodologies.},
address = {Madrid, Spain},
author = {Satar-Boroujeni, Hamid and Shafai, Bahram},
booktitle = {Proceedings of the 8th International Conference on Digital Audio Effects (DAFX-05)},
file = {::},
keywords = {partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
title = {{A Robust Algorithm for Partial Tracking of Music Signals}},
year = {2005}
}
@inproceedings{Sterian:1998:PartialTracking:SPIE,
abstract = {We present a new method for musical partial tracking in the context of musical transcription using a time-frequency Kalman filter structure. The filter is based upon a model for the evolution of a partial in amplitude and frequency. The parameters of this model are determined from a statistical analysis of partial behavior across a wide range of pitch from four brass instruments. Statistics are computed independently for the partial attributes of frequency and log-power first differences. We present observed power spectral density shapes, total powers, and histograms, as well as least-squares approximations to these. We demonstrate that a Kalman filter tracker using this partial model is capable of tracking partials in music. We discuss how the filter structure naturally provides quality-of-fit information about the data for use in further processing and how this information can be used to perform partial track initiation and termination within a common framework. We propose that a model-based approach to partial tracking is preferable to existing approaches which generally use heuristic rules or birth/death notions over a small time neighborhood. The advantages include better performance in the presence of cluttered data (e.g., multi-voice material) and simplified tracking over missed observations.},
address = {San Diego, California},
annote = {abr:
- use Kalman filter},
author = {Sterian, Andrew and Wakefield, Gregory H.},
booktitle = {Proceedings of the Annual Meeting and International Symposium on Optical Science, Engineering and Instrumentation (SPIE)},
xdoi = {10.1117/12.325677},
file = {::},
keywords = {kalman filter,music analysis,musical transcription,partial{\_}tracking},
mendeley-tags = {partial{\_}tracking},
month = {oct},
title = {{Model-based approach to partial tracking for musical transcription}},
year = {1998}
}
@inproceedings{Dorfer:2018:NoisyLabelsSelfVerification:DCASE,
address = {Surrey, UK},
author = {Dorfer, Matthias and Widmer, Gerhard},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/Downloads/DCASE2018{\_}Dorfer{\_}999.pdf:pdf},
keywords = {machine{\_}listening},
mendeley-tags = {machine{\_}listening},
title = {{Training General-Purpose Audio Tagging Networks with Noisy Labels and Iterative Self-Verification}},
year = {2018}
}
@inproceedings{Jeong:2018:AudioTagging:DCASE,
abstract = {In this paper; we describe the techniques and models applied to our submission for DCASE 2018 task 2: General-purpose audio tagging ofFreesound content with AudioSet labels. We mainly fo- cus on how to train deep learning models efficiently against strong augmentation and label noise. First; we conducted a single-block DenseNet architecture and multi-head softmax classifier for effi- cient learning with mixup augmentation. For the label noise; we applied the batch-wise loss masking to eliminate the loss of out- liers in a mini-batch. We also tried an ensemble of various models; trained by using different sampling rate or audio representation.},
address = {Surrey, UK},
author = {Jeong, Il-Young and Lim, Hyungui},
booktitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/Downloads/audio-tagging-system.pdf:pdf},
keywords = {audio{\_}tagging,machine{\_}listening},
mendeley-tags = {audio{\_}tagging,machine{\_}listening},
title = {{Audio Tagging System Using Densely Connected Convolutional Networks}},
year = {2018}
}
@inproceedings{Xu:2018:WeaklyLabeledEventDetection:ICASSP,
abstract = {In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classification, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labeled with one or a few audio tags but without timestamps of the audio events, which is called as weakly labeled data. Two sub-tasks are defined in this challenge including audio tagging and sound event detection using this weakly labeled data. A convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram is proposed. In addition, a temporal attention method is proposed along the frames to predicate the locations of each audio event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6$\backslash${\%} and Equal error 0.73, respectively.},
address = {Calgary, AB, Canada},
annote = {? how long are the input audio frames (10s?)},
archivePrefix = {arXiv},
arxivId = {1710.00343},
author = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
xdoi = {10.1109/ICASSP.2018.8461975},
eprint = {1710.00343},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {Attention,Audio tagging,DCASE2017 challenge,Gated linear unit,Weakly supervised sound event detection,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
mendeley-tags = {acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,machine{\_}listening,weakly{\_}labeled},
pages = {121--125},
title = {{Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network}},
year = {2018}
}
@book{Tag2019,
author = {Tag, King},
keywords = {acmus,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,audio{\_}matching,audio{\_}tagging,bass{\_}transcription,beat{\_}tracking,chord{\_}recognition,deep{\_}learning,few{\_}shot{\_}learning,generative{\_}adversarial{\_}network,hashing,idmt,instrument{\_}recognition,isad,m2d,machine{\_}learning,machine{\_}listening,melody{\_}contour{\_}analysis,melody{\_}transcription,meta{\_}learning,model{\_}investigation,multipitch{\_}estimation,music{\_}generation,music{\_}transcription,partial{\_}tracking,pitch{\_}tracking,rhythm{\_}analysis,semi{\_}supervised{\_}learning,siamese{\_}networks,source{\_}separation,speech{\_}analysis{\_}synthesis,speech{\_}recognition,tempo{\_}estimation,time{\_}series{\_}analysis,transfer{\_}learning,unsupervised{\_}learning,weakly{\_}labeled},
mendeley-tags = {acmus,acoustic{\_}event{\_}detection,acoustic{\_}scene{\_}classification,audio{\_}matching,audio{\_}tagging,bass{\_}transcription,beat{\_}tracking,chord{\_}recognition,deep{\_}learning,few{\_}shot{\_}learning,generative{\_}adversarial{\_}network,hashing,idmt,instrument{\_}recognition,isad,m2d,machine{\_}learning,machine{\_}listening,melody{\_}contour{\_}analysis,melody{\_}transcription,meta{\_}learning,model{\_}investigation,multipitch{\_}estimation,music{\_}generation,music{\_}transcription,partial{\_}tracking,pitch{\_}tracking,rhythm{\_}analysis,semi{\_}supervised{\_}learning,siamese{\_}networks,source{\_}separation,speech{\_}analysis{\_}synthesis,speech{\_}recognition,tempo{\_}estimation,time{\_}series{\_}analysis,transfer{\_}learning,unsupervised{\_}learning,weakly{\_}labeled},
title = {{THE BOOK OF TAGS}},
year = {2019}
}
@article{Fonseca:2020:FSD50K:ARXIV,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.00475v1},
author = {Fonseca, Eduardo and Member, Student and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
eprint = {arXiv:2010.00475v1},
file = {:C$\backslash$:/Users/abr/Downloads/2010.00475.pdf:pdf},
number = {8},
pages = {1--21},
title = {{FSD50K : an Open Dataset of Human-labeled Sound Events}},
volume = {14},
year = {2020}
}
@article{Ding:2020:AdaMD:IEEE_TASLP,
abstract = {The goal of acoustic (or sound) events detection (AED or SED) is to predict the temporal position of target events in given audio segments. This task plays a significant role in safety monitoring, acoustic early warning and other scenarios. However, the deficiency of data and diversity of acoustic event sources make the AED task a tough issue, especially for prevalent data-driven methods. In this article, we start from analyzing acoustic events according to their time-frequency domain properties, showing that different acoustic events have different time-frequency scale characteristics. Inspired by the analysis, we propose an adaptive multi-scale detection (AdaMD) method. By taking advantage of hourglass neural network and gated recurrent unit (GRU) module, our AdaMD produces multiple predictions at different temporal and frequency resolutions. An adaptive training algorithm is subsequently adopted to combine multi-scale predictions to enhance the overall capability. Experimental results on Detection and Classification of Acoustic Scenes and Events 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3 demonstrate that the AdaMD outperforms published state-of-the-art competitors in terms of the metrics of event error rate (ER) and F1-score. The verification experiment on our collected factory mechanical dataset also proves the noise-resistant capability of the AdaMD, providing the possibility for it to be deployed in the complex environment.},
archivePrefix = {arXiv},
arxivId = {1911.06878},
author = {Ding, Wenhao and He, Liang},
xdoi = {10.1109/TASLP.2019.2953350},
eprint = {1911.06878},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, He - 2020 - Adaptive multi-scale detection of acoustic events.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Rare acoustic event detection,adaptive multi-scale,hourglass network},
number = {X},
pages = {294--306},
title = {{Adaptive multi-scale detection of acoustic events}},
volume = {28},
year = {2020}
}
@inproceedings{Mars:2019:BinauralASC:DCASE,
address = {New York, NY, USA},
author = {Mars, Rohith and Pratik, Pranay and Nagisetty, Srikanth and Lim, Chongsoon},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
xdoi = {10.33682/6c9z-gd15},
file = {:S$\backslash$:/Meine Bibliotheken/2019{\_}xchange{\_}idmt/2019{\_}12{\_}EUSIPCO{\_}ASC{\_}SOTA/DCASE2019Workshop{\_}Mars{\_}73.pdf:pdf},
keywords = {acoustic{\_}scene{\_}classification,machine{\_}listening},
mendeley-tags = {acoustic{\_}scene{\_}classification,machine{\_}listening},
pages = {149--153},
title = {{Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks}},
year = {2019}
}
@article{Naranjo-Alcazar2020a,
abstract = {Sound Event Localization and Detection (SELD) is a problem related to the field of machine listening whose objective is to recognize individual sound events, detect their temporal activity, and estimate their spatial location. Thanks to the emergence of more hard-labeled audio datasets, Deep Learning techniques have become state-of-the-art solutions. The most common ones are those that implement a convolutional recurrent network (CRNN) having previously transformed the audio signal into multichannel 2D representation. The squeeze-excitation technique can be considered as a convolution enhancement that aims to learn spatial and channel feature maps independently rather than together as standard convolutions do. This is usually achieved by combining some global clustering operators, linear operators and a final calibration between the block input and its learned relationships. This work aims to improve the accuracy results of the baseline CRNN presented in DCASE 2020 Task 3 by adding residual squeeze-excitation (SE) blocks in the convolutional part of the CRNN. The followed procedure involves a grid search of the parameter ratio (used in the linear relationships) of the residual SE block, whereas the hyperparameters of the network remain the same as in the baseline. Experiments show that by simply introducing the residual SE blocks, the results obtained clearly exceed the baseline.},
archivePrefix = {arXiv},
arxivId = {2006.14436},
author = {Naranjo-Alcazar, Javier and Perez-Castanos, Sergi and Ferrandis, Jose and Zuccarello, Pedro and Cobos, Maximo},
eprint = {2006.14436},
file = {::},
title = {{Sound Event Localization and Detection using Squeeze-Excitation Residual CNNs}},
xurl = {http://arxiv.org/abs/2006.14436},
year = {2020}
}
@inproceedings{Draghici:2020:LanguageID:AM,
author = {Draghici, Alexandra and Abe{\ss}er, Jakob and Lukashevich, Hanna},
booktitle = {Proceedings of the Audio Mostly Conference},
file = {:C$\backslash$:/Users/abr/Downloads/Language{\_}Identification{\_}using{\_}CNN{\_}and{\_}CRNN (5).pdf:pdf},
isbn = {9781450375634},
keywords = {abt-md,acm reference format,convolu-,convolutional recurrent neural networks,speech recognition,spoken language identification,tional neural networks},
mendeley-tags = {abt-md},
title = {{A Study on Spoken Language Identification using Deep Neural Networks}},
year = {2020}
}
@article{Drossos:2020:SED:ARXIV,
abstract = {State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85{\%} and average training time per epoch by 78{\%}, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6{\%} and 3.8{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {2002.00476},
author = {Drossos, Konstantinos and Mimilakis, Stylianos I. and Gharib, Shayan and Li, Yanxiong and Virtanen, Tuomas},
eprint = {2002.00476},
file = {:X$\backslash$:/knowhow/publica{\_}exports/new{\_}publications/mis{\_}ijcnn{\_}2020.pdf:pdf},
keywords = {abt-md},
mendeley-tags = {abt-md},
title = {{Sound Event Detection with Depthwise Separable and Dilated Convolutions}},
xurl = {http://arxiv.org/abs/2002.00476},
year = {2020}
}
@article{Won2020,
abstract = {A traffic monitoring system is an integral part of Intelligent Transportation Systems (ITS). It is one of the critical transportation infrastructures that transportation agencies invest a huge amount of money to collect and analyze the traffic data to better utilize the roadway systems, improve the safety of transportation, and establish future transportation plans. With recent advances in MEMS, machine learning, and wireless communication technologies, numerous innovative traffic monitoring systems have been developed. In this article, we present a review of state-of-the-art traffic monitoring systems focusing on the major functionality-vehicle classification. We organize various vehicle classification systems, examine research issues and technical challenges, and discuss hardware/software design, deployment experience, and system performance of vehicle classification systems. Finally, we discuss a number of critical open problems and future research directions in an aim to provide valuable resources to academia, industry, and government agencies for selecting appropriate technologies for their traffic monitoring applications.},
archivePrefix = {arXiv},
arxivId = {1910.04656},
author = {Won, Myounggyu},
xdoi = {10.1109/ACCESS.2020.2987634},
eprint = {1910.04656},
file = {:C$\backslash$:/Users/abr/Desktop/2020{\_}08{\_}17{\_}Traffic{\_}Monitoring{\_}Literature/audio{\_}traffic{\_}monitoring{\_}literature/1910.04656.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Intelligent transportation systems,traffic monitoring systems,vehicle classification},
pages = {73340--73358},
title = {{Intelligent Traffic Monitoring Systems for Vehicle Classification: A Survey}},
volume = {8},
year = {2020}
}
@inproceedings{Pankajakshan:2019:SED:WASPAA,
abstract = {Polyphonic Sound Event Detection (SED) in real-world recordings is a challenging task because of the dynamic polyphony level, intensity, and duration of sound events. Current polyphonic SED systems fail to model the temporal structure of sound events explicitly and instead attempt to look at which sound events are present at each audio frame. Consequently, the event-wise detection performance is much lower than the segment-wise detection performance. In this work, we propose a joint model approach to improve the temporal localization of sound events using a multi-task learning setup. The first task predicts which sound events are present at each time frame; we call this branch 'Sound Event Detection (SED) model', while the second task predicts if a sound event is present or not at each frame; we call this branch 'Sound Activity Detection (SAD) model'. We verify the proposed joint model by comparing it with a separate implementation of both tasks aggregated together from individual task predictions. Our experiments on the URBAN-SED dataset show that the proposed joint model can alleviate False Positive (FP) and False Negative (FN) errors and improve both the segment-wise and the event-wise metrics.},
address = {New Paltz, NY, USA},
archivePrefix = {arXiv},
arxivId = {1907.05122},
author = {Pankajakshan, Arjun and Bear, Helen L. and Benetos, Emmanouil},
booktitle = {Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
xdoi = {10.1109/WASPAA.2019.8937193},
eprint = {1907.05122},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pankajakshan, Bear, Benetos - 2019 - Polyphonic sound event and sound activity detection A multi-task approach.pdf:pdf},
isbn = {9781728111230},
issn = {19471629},
keywords = {Polyphonic sound event detection,multi-task learning,sound activity detection},
pages = {323--327},
title = {{Polyphonic sound event and sound activity detection: A multi-task approach}},
year = {2019}
}
@inproceedings{Wang2020b,
abstract = {Since the audio recapture can be used to assist audio splicing, it is important to identify whether a suspected audio recording is recaptured or not. However, few works on such detection have been reported. In this paper, we propose an method to detect the recaptured audio based on deep learning and we investigate two deep learning techniques, i.e., neural network with dropout method and stack auto-encoders (SAE). The waveform samples of audio frame is directly used as the input for the deep neural network. The experimental results show that error rate around 7.5{\%} can be achieved, which indicates that our proposed method can successfully discriminate recaptured audio and original audio.},
author = {Wang, Yu and Salamon, Justin and Bryan, Nicholas J. and {Pablo Bello}, Juan},
xdoi = {10.1109/icassp40776.2020.9054708},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Few-Shot Sound Event Detection.pdf:pdf},
pages = {81--85},
title = {{Few-Shot Sound Event Detection}},
year = {2020}
}
@article{Greco2020,
author = {Greco, Antonio and Petkov, Nicolai and Saggese, Alessia and Vento, Mario},
xdoi = {10.1109/tifs.2020.2994740},
file = {:C$\backslash$:/Users/abr/Downloads/09093814.pdf:pdf},
issn = {1556-6013},
journal = {IEEE Transactions on Information Forensics and Security},
pages = {1--1},
title = {{AReN: A Deep Learning Approach for Sound Event Recognition using a Brain inspired Representation}},
volume = {15},
year = {2020}
}
@article{Martin-Morato2020,
author = {Mart{\'{i}}n-Morat{\'{o}}, Irene and Cobos, Maximo and Ferri, Francesc J.},
file = {:C$\backslash$:/Users/abr/Downloads/09115233.pdf:pdf},
pages = {1925--1935},
title = {{Adaptive Distance-Based Pooling in Convolutional Neural Networks for Audio Event Classificatio}},
volume = {28},
year = {2020}
}
@inproceedings{Mcdonnell:2019:AcousticScenes:DCASE,
abstract = {This technical report describes our approach to Tasks 1a, 1b and 1c in the 2019 DCASE acoustic scene classification challenge. Our focus was on developing strong single models, without use of any supplementary data. We investigated the use of a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We designed the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we used two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output.},
address = {New York, NY, USA},
author = {Mcdonnell, Mark D and Gao, Wei},
booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE)},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcdonnell, Gao - 2019 - Acoustic Scene Classification Using Deep Residual Networks With Late Fusion of Separated High and Low Frequency.pdf:pdf},
keywords = {acou,machine{\_}listening},
mendeley-tags = {acou,machine{\_}listening},
title = {{Acoustic Scene Classification Using Deep Residual Networks With Late Fusion of Separated High and Low Frequency Paths}},
xurl = {https://github.com/McDonnell-Lab/DCASE2019-Task1},
year = {2019}
}
@techreport{Stepien:2020:Sounds:REPORT,
author = {{Adrian St{\c{e}}pie{\'{n}}}},
file = {::},
institution = {Audio Analytics},
title = {{Why Real Sounds Matter for Machine Learning}}
}
@inproceedings{Arora:2017:TransferLearning:IEEE_WMSP,
abstract = {In this work, we address the limited availability of large annotated databases for real-life audio event detection by utilizing the concept of transfer learning. This technique aims to transfer knowledge from a source domain to a target domain, even if source and target have different feature distributions and label sets. We hypothesize that all acoustic events share the same inventory of basic acoustic building blocks and differ only in the temporal order of these acoustic units. We then construct a deep neural network with convolutional layers for extracting the acoustic units and a recurrent layer for capturing the temporal order. Under the above hypothesis, transfer learning from a source to a target domain with a different acoustic event inventory is realized by transferring the convolutional layers from the source to the target domain. The recurrent layer is, however, learnt directly from the target domain. Experiments on the transfer from a synthetic source database to the reallife target database of DCASE 2016 demonstrate that transfer learning leads to improved detection performance on average. However, the successful transfer to detect events which are very different from what was seen in the source domain, could not be verified. {\textcopyright} 2017 IEEE.},
address = {Luton, UK},
annote = {abr: test},
author = {Arora, Prerna and Haeb-Umbach, Reinhold},
booktitle = {Proceedings of the IEEE International Workshop on Multimedia Signal Processing (MMSP)},
xdoi = {10.1109/MMSP.2017.8122258},
file = {:C$\backslash$:/Users/abr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Haeb-Umbach - 2017 - A study on transfer learning for acoustic event detection in a real life scenario(4).pdf:pdf},
isbn = {978-1-5090-3649-3},
keywords = {machine{\_}listening,transfer{\_}learning},
mendeley-tags = {machine{\_}listening,transfer{\_}learning},
pages = {1--6},
title = {{A Study on Transfer Learning for Acoustic Event Detection in a Real Life Scenario}},
xurl = {http://ieeexplore.ieee.org/document/8122258/},
year = {2017}
}
@article{Koutini2019,
abstract = {Convolutional Neural Networks (CNNs) have had great success in many machine vision as well as machine audition tasks. Many image recognition network architectures have consequently been adapted for audio processing tasks. However, despite some successes, the performance of many of these did not translate from the image to the audio domain. For example, very deep architectures such as ResNet [1] and DenseNet [2], which significantly outperform VGG [3] in image recognition, do not perform better in audio processing tasks such as Acoustic Scene Classification (ASC). In this paper, we investigate the reasons why such powerful architectures perform worse in ASC compared to simpler models (e.g., VGG). To this end, we analyse the receptive field (RF) of these CNNs and demonstrate the importance of the RF to the generalization capability of the models. Using our receptive field analysis, we adapt both ResNet and DenseNet, achieving state-of-the-art performance and eventually outperforming the VGG-based models. We introduce systematic ways of adapting the RF in CNNs, and present results on three data sets that show how changing the RF over the time and frequency dimensions affects a model's performance. Our experimental results show that very small or very large RFs can cause performance degradation, but deep models can be made to generalize well by carefully choosing an appropriate RF size within a certain range.},
archivePrefix = {arXiv},
arxivId = {1907.01803},
author = {Koutini, Khaled and Eghbal-Zadeh, Hamid and Dorfer, Matthias and Widmer, Gerhard},
xdoi = {10.23919/EUSIPCO.2019.8902732},
eprint = {1907.01803},
file = {:C$\backslash$:/Users/abr/Downloads/1907.01803.pdf:pdf},
isbn = {9789082797039},
issn = {22195491},
journal = {European Signal Processing Conference},
keywords = {Acoustic scene classification,CNN,Deep learning,Machine learning},
title = {{The receptive field as a regularizer in deep convolutional neural networks for acoustic scene classification}},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{Zhang2019,
author = {Zhang, Hongyi and Dauphin, Yann N and Brain, Google},
booktitle = {ICLR},
file = {:C$\backslash$:/Users/abr/Downloads/fixup{\_}initialization{\_}residual{\_}learning{\_}without{\_}normalization (1).pdf:pdf},
pages = {1--16},
title = {{Fixup Initialization: Residual Learning without Normalization}},
year = {2019}
}
@article{Hernandez-Olivan:2020:Boundary:ARXIV,
abstract = {The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and $\backslash$textit{\{}end-to-end{\}} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and $\backslash$textit{\{}end-to-end{\}} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy {\$}F{\_}1{\$} of 0.411 that outperforms the current one obtained under the same conditions.},
archivePrefix = {arXiv},
arxivId = {2008.07527},
author = {Hernandez-Olivan, Carlos and Beltran, Jose R. and Diaz-Guerra, David},
eprint = {2008.07527},
file = {:C$\backslash$:/Users/abr/Downloads/2008.07527.pdf:pdf},
pages = {1--11},
title = {{Music Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features}},
xurl = {http://arxiv.org/abs/2008.07527},
year = {2020}
}
@inproceedings{Grill:2015:Segmentation:EUSIPCO,
address = {Nice, France},
author = {Grill, Thomas and Schl{\"{u}}ter, Jan},
booktitle = {Proceedings of the European Signal Processing Conference (EUSIPCO)},
file = {:C$\backslash$:/Users/abr/Downloads/2015{\_}eusipco.pdf:pdf},
isbn = {9780992862633},
pages = {1306--1310},
title = {{Music Boundary Detection using Neural Networks on Spectrograms and Self-Similarity Lag Matrices}},
year = {2015}
}


