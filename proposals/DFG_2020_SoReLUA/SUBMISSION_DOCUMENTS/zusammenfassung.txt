Die Entwicklung computergestützter Methoden für die Analyse auditiver Szenen wurde in den letzten Jahrzehnten aktiv erforscht. Eine der anspruchsvollsten Erkennungsaufgaben ist hierbei die präzise Erkennung und Klassifizierung von Klangereignissen, die in einer Aufnahme einer akustischen Szene zu hören sind. Der rasche Fortschritt auf dem Gebiet der künstlichen Intelligenz (KI), besonders des Deep Learnings, hat in den letzten Jahren zur Entwicklung einer Vielzahl neuartiger datengesteuerter Algorithmen auf dem Gebiet der Klangerkennung geführt. Die angewandten KI-Verfahren wurden dabei oft aus Forschungsgebieten wie der Bildverarbeitung oder Spracherkennung übernommen und adaptiert. Während das menschliche Gehör komplexe akustische Szenen leicht in Einzelklänge zerlegen kann, um die wichtigsten Schallquellen zu identifizieren, stehen entsprechende KI-basierte Algorithmen vor verschiedenen Herausforderungen. Beispiele hierfür sind  die große akustische Variabilität realer Schallereignisse, die erforderliche Robustheit gegenüber wechselnder akustischer Aufnahmebedingungen, die durch die Aufnahmehardware und die Raumakustik verursacht werden, sowie die notwendige Adaptivität gegenüber neuen und bis dahin ungehörten Klangereignissen. Das Hauptziel dieses Projektes ist die Entwicklung von Methoden zum tieferen Verständnis und zur Modellierung komplexer Umwelt- oder Industrieschallszenen. Es besteht ein hohes Transferpotential der entwickelten Methoden hin zu Bereichen wie Hörgeräten und Sprachassistenzsystemen, akustischer Überwachung von Verkehrsinfrastuktur oder akustischer Zustandsüberwachung in industriellen Fertigungsketten. Im ersten Schritt soll die zeitlichen Struktur einer gegebenen Tonaufnahme beschrieben werden. Hierzu werden zum einen klanglich homogene Zeitsegmente identifiziert und zum anderen der lokale Polyphoniegrad ermittelt, welcher beschreibt, wie viele Schallereignisse zu einem bestimmten Zeitpunkt gleichzeitig hörbar sind. Im zweiten Schritt erfolgt die Erkennung und Klassifikation einzelner Klangereignisse um ein detaillierteres Verständnis einer akustischen Szene abzuleiten. Außerdem erfolgt eine Modellierung der typischen Auftretensreihenfolge und charakteristischer Wiederholungsmuster bestimmter Klänge. Um die Robustheit der Klangklassifikation gegenüber Änderungen der akustischen Aufnahmebedingungen zu verbessern, werden sogenannte Domain Adaptation Verfahren untersucht, welche z.B. die klangliche Variabilität der Trainingsdaten eines tiefen neuronalen Netzwerkes erhöhen können. Aufgrund der Komplexität der Aufgabenstellung werden Verfahren aus dem Bereich Explainable AI eingesetzt um Klassifizierungsentscheidungen insbesondere für komplexe polyphone Klangszenen besser bewerten zu können. Abschließend werden im dritten Teilschritt Methoden entwickelt, um bis dahin ungehörte Geräusche (akustische Anomalien) zu erkennen und mittels inkrementellen Lernstrategien in das zugrundeliegende Klassifikationsmodell zu integrieren.

(noch 4 Zeichen)