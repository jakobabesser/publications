The development of computational methods for auditory scene analysis and machine listening has been actively researched over the last decades. One of the most challenging recognition tasks is sound event detection, which involves a precise detection and classification of audio events that can be heard within a recorded acoustic scene. The rapid progress in the area of deep learning has led to the development of a multitude of novel data-driven algorithms in the field of sound event detection in the last years. The applied deep learning techniques are often inspired from related research areas such as computer vision, natural language processing, and speech processing. While the human auditory system can easily decompose complex acoustic scenes to identify and focus on the most salient sound sources, algorithms driven by artificial intelligence face various challenges such as the large acoustic variability of real-life sound events, the required robustness towards changing acoustic conditions caused by recording hardware and the surrounding room acoustics, as well as adaptivity towards previously unheard sound classes. In this project, we aim to develop methods to understand and model complex environmental or industrial sound scenes in three partial steps. In the first step, we want to derive a coarse-level description of the temporal structure of a given audio recording. This involves the identification of segments with homogeneous timbre characteristics and the estimation of the local degree of polyphony, which measures how many sound events are simultaneously audible at a certain point in time. In the second step, we aim to derive a more detailed understanding of an acoustic scene by detecting and classifying particular sound events. Furthermore, we will model the typical order of appearance as well as characteristic repetition patterns of different sound event types. In order to improve the robustness against changes in the acoustic recording conditions, we will investigate methods for domain adaptation, which can be integrated into the sound event detection algorithms. Due to the complexity of the task, we will use explainable AI techniques to evaluate the meaningfulness of classification decisions, particularly for complex polyphonic sound event detection scenarios. Finally, in the third step, we will develop methods to recognize previously unheard sounds (acoustic anomalies) using different timbral and temporal novelty measures. Continuous learning techniques will then be used to integrate such sounds into the underlying sound classification model in order to mimic the humanâ€™s capability to continuously expand its known vocabulary of sounds.